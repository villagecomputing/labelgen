{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Wikipedia\n",
    "\n",
    "[View on Github](https://github.com/villagecomputing/superpipe/tree/main/docs/examples/web_scraping/web_scraping.ipynb)\n",
    "\n",
    "We'll use Superpipe to build a pipeline that receives a famous person's name and figures out their birthday, whether they're still alive and if not, their cause of death.\n",
    "\n",
    "This pipeline will work in 4 steps -\n",
    "\n",
    "1. Do a google search with the person's name\n",
    "2. Use an LLM to fetch the URL of their wikipedia page from the search results\n",
    "3. Fetch the contents of the wikipedia page and convert them to markdown\n",
    "4. Use an LLM to extract the birthdate and living or dead from the wikipedia contents\n",
    "\n",
    "We'll build the pipeline, evaluate it on some data, and optimize it to maximize accuracy while reducing cost and latency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Building the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from superpipe.steps import LLMStructuredStep, CustomStep, SERPEnrichmentStep\n",
    "from superpipe import models\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Step 1: use Superpipe's built-in SERP enrichment step to search for the persons wikipedia page\n",
    "# Include a unique \"name\" for the step that will used to reference this step's output in future steps\n",
    "\n",
    "search_step = SERPEnrichmentStep(\n",
    "  prompt= lambda row: f\"{row['name']} wikipedia\",\n",
    "  name=\"search\"\n",
    ")\n",
    "\n",
    "# Step 2: Use an LLM to extract the wikipedia URL from the search results\n",
    "# First, define a Pydantic model that specifies the structured output we want from the LLM\n",
    "\n",
    "class ParseSearchResult(BaseModel):\n",
    "  wikipedia_url: str = Field(description=\"The URL of the Wikipedia page for the person\")\n",
    "\n",
    "# Then we use the built-in LLMStructuredStep and specify a model and a prompt\n",
    "# The prompt is a function that has access to all the fields in the input as well as the outputs of previous steps\n",
    "\n",
    "parse_search_step = LLMStructuredStep(\n",
    "  model=models.gpt35,\n",
    "  prompt= lambda row: f\"Extract the Wikipedia URL for {row['name']} from the following search results: \\n\\n {row['search']}\",\n",
    "  out_schema=ParseSearchResult,\n",
    "  name=\"parse_search\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from superpipe.pipeline import Pipeline\n",
    "import requests\n",
    "import html2text\n",
    "import json\n",
    "\n",
    "h = html2text.HTML2Text()\n",
    "h.ignore_links = True\n",
    "\n",
    "# Step 3: we create a CustomStep that can execute any arbitrary function (transform)\n",
    "# The function fetches the contents of the wikipedia url and converts them to markdown\n",
    "\n",
    "fetch_wikipedia_step = CustomStep(\n",
    "  transform=lambda row: h.handle(requests.get(row['wikipedia_url']).text),\n",
    "  name=\"wikipedia\"\n",
    ")\n",
    "\n",
    "# Step 4: we extract the date of birth, living/dead status and cause of death from the wikipedia contents\n",
    "\n",
    "class ExtractedData(BaseModel):\n",
    "    date_of_birth: str = Field(description=\"The date of birth of the person in the format YYYY-MM-DD\")\n",
    "    alive: bool = Field(description=\"Whether the person is still alive\")\n",
    "    cause_of_death: str = Field(description=\"The cause of death of the person. If the person is alive, return 'N/A'\")\n",
    "\n",
    "extract_step = LLMStructuredStep(\n",
    "  model=models.gpt4,\n",
    "  prompt= lambda row: f\"\"\"Extract the date of birth for {row['name']}, whether they're still alive \\\n",
    "  and if not, their cause of death from the following Wikipedia content: \\n\\n {row['wikipedia']}\"\"\",\n",
    "  out_schema=ExtractedData,\n",
    "  name=\"extract_data\"\n",
    ")\n",
    "\n",
    "# Finally we define and run the pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "  search_step,\n",
    "  parse_search_step,\n",
    "  fetch_wikipedia_step,\n",
    "  extract_step\n",
    "])\n",
    "\n",
    "output = pipeline.run({\"name\": \"Jean-Paul Sartre\"})\n",
    "print(json.dumps(output, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Evaluating the pipeline\n",
    "\n",
    "Now, we'll evaluate the pipeline on a dataset. Think of this as unit tests for your code. You wouldn't ship code to production without testing it, you shouldn't ship LLM pipelines to production without evaluating them.\n",
    "\n",
    "To do this, we need:\n",
    "\n",
    "1. **A dataset with labels** - In this case we need a list of famous people and the true date of birth, living status and cause of death of each person\n",
    "2. **Evaluation function** - a function that defines what \"correct\" is. We'll use simple comparison for date of birth and living status, and an LLM call to evaluate the correctness of cause of death."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying step search: 100%|██████████| 10/10 [00:08<00:00,  1.16it/s]\n",
      "Applying step parse_search: 100%|██████████| 10/10 [00:10<00:00,  1.02s/it]\n",
      "Applying step wikipedia: 100%|██████████| 10/10 [00:04<00:00,  2.27it/s]\n",
      "Applying step extract_data: 100%|██████████| 10/10 [01:26<00:00,  8.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  1.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>dob_label</th>\n",
       "      <th>alive_label</th>\n",
       "      <th>cause_label</th>\n",
       "      <th>search</th>\n",
       "      <th>__parse_search__</th>\n",
       "      <th>wikipedia_url</th>\n",
       "      <th>wikipedia</th>\n",
       "      <th>__extract_data__</th>\n",
       "      <th>date_of_birth</th>\n",
       "      <th>alive</th>\n",
       "      <th>cause_of_death</th>\n",
       "      <th>__eval_fn__</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ruth Bader Ginsburg</td>\n",
       "      <td>1933-03-15</td>\n",
       "      <td>False</td>\n",
       "      <td>Pancreatic cancer</td>\n",
       "      <td>{\"searchParameters\":{\"q\":\"Ruth Bader Ginsburg ...</td>\n",
       "      <td>{'input_tokens': 1922, 'output_tokens': 23, 'i...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Ruth_Bader_Ginsburg</td>\n",
       "      <td>Jump to content\\n\\nMain menu\\n\\nMain menu\\n\\nm...</td>\n",
       "      <td>{'input_tokens': 46522, 'output_tokens': 37, '...</td>\n",
       "      <td>1933-03-15</td>\n",
       "      <td>False</td>\n",
       "      <td>complications of metastatic pancreatic cancer</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bill Gates</td>\n",
       "      <td>1955-10-28</td>\n",
       "      <td>True</td>\n",
       "      <td>N/A</td>\n",
       "      <td>{\"searchParameters\":{\"q\":\"Bill Gates wikipedia...</td>\n",
       "      <td>{'input_tokens': 1809, 'output_tokens': 20, 'i...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Bill_Gates</td>\n",
       "      <td>Jump to content\\n\\nMain menu\\n\\nMain menu\\n\\nm...</td>\n",
       "      <td>{'input_tokens': 46613, 'output_tokens': 32, '...</td>\n",
       "      <td>1955-10-28</td>\n",
       "      <td>True</td>\n",
       "      <td>N/A</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Steph Curry</td>\n",
       "      <td>1988-03-14</td>\n",
       "      <td>True</td>\n",
       "      <td>N/A</td>\n",
       "      <td>{\"searchParameters\":{\"q\":\"Steph Curry wikipedi...</td>\n",
       "      <td>{'input_tokens': 1339, 'output_tokens': 20, 'i...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Stephen_Curry</td>\n",
       "      <td>Jump to content\\n\\nMain menu\\n\\nMain menu\\n\\nm...</td>\n",
       "      <td>{'input_tokens': 64861, 'output_tokens': 32, '...</td>\n",
       "      <td>1988-03-14</td>\n",
       "      <td>True</td>\n",
       "      <td>N/A</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Scott Belsky</td>\n",
       "      <td>1980-04-18</td>\n",
       "      <td>True</td>\n",
       "      <td>N/A</td>\n",
       "      <td>{\"searchParameters\":{\"q\":\"Scott Belsky wikiped...</td>\n",
       "      <td>{'input_tokens': 1566, 'output_tokens': 21, 'i...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Scott_Belsky</td>\n",
       "      <td>Jump to content\\n\\nMain menu\\n\\nMain menu\\n\\nm...</td>\n",
       "      <td>{'input_tokens': 2227, 'output_tokens': 32, 'i...</td>\n",
       "      <td>1980-04-18</td>\n",
       "      <td>True</td>\n",
       "      <td>N/A</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Steve Jobs</td>\n",
       "      <td>1955-02-24</td>\n",
       "      <td>False</td>\n",
       "      <td>Pancreatic tumor/cancer</td>\n",
       "      <td>{\"searchParameters\":{\"q\":\"Steve Jobs wikipedia...</td>\n",
       "      <td>{'input_tokens': 1625, 'output_tokens': 20, 'i...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Steve_Jobs</td>\n",
       "      <td>Jump to content\\n\\nMain menu\\n\\nMain menu\\n\\nm...</td>\n",
       "      <td>{'input_tokens': 47086, 'output_tokens': 42, '...</td>\n",
       "      <td>1955-02-24</td>\n",
       "      <td>False</td>\n",
       "      <td>respiratory arrest related to a pancreatic neu...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Paris Hilton</td>\n",
       "      <td>1981-02-17</td>\n",
       "      <td>True</td>\n",
       "      <td>N/A</td>\n",
       "      <td>{\"searchParameters\":{\"q\":\"Paris Hilton wikiped...</td>\n",
       "      <td>{'input_tokens': 1322, 'output_tokens': 20, 'i...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Paris_Hilton</td>\n",
       "      <td>Jump to content\\n\\nMain menu\\n\\nMain menu\\n\\nm...</td>\n",
       "      <td>{'input_tokens': 49288, 'output_tokens': 32, '...</td>\n",
       "      <td>1981-02-17</td>\n",
       "      <td>True</td>\n",
       "      <td>N/A</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Kurt Vonnegut</td>\n",
       "      <td>1922-11-11</td>\n",
       "      <td>False</td>\n",
       "      <td>Brain injuries</td>\n",
       "      <td>{\"searchParameters\":{\"q\":\"Kurt Vonnegut wikipe...</td>\n",
       "      <td>{'input_tokens': 1369, 'output_tokens': 22, 'i...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Kurt_Vonnegut</td>\n",
       "      <td>Jump to content\\n\\nMain menu\\n\\nMain menu\\n\\nm...</td>\n",
       "      <td>{'input_tokens': 29700, 'output_tokens': 45, '...</td>\n",
       "      <td>1922-11-11</td>\n",
       "      <td>False</td>\n",
       "      <td>brain injuries incurred several weeks prior, f...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Snoop Dogg</td>\n",
       "      <td>1971-10-20</td>\n",
       "      <td>True</td>\n",
       "      <td>N/A</td>\n",
       "      <td>{\"searchParameters\":{\"q\":\"Snoop Dogg wikipedia...</td>\n",
       "      <td>{'input_tokens': 1702, 'output_tokens': 20, 'i...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Snoop_Dogg</td>\n",
       "      <td>Jump to content\\n\\nMain menu\\n\\nMain menu\\n\\nm...</td>\n",
       "      <td>{'input_tokens': 40901, 'output_tokens': 32, '...</td>\n",
       "      <td>1971-10-20</td>\n",
       "      <td>True</td>\n",
       "      <td>N/A</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Kobe Bryant</td>\n",
       "      <td>1978-08-23</td>\n",
       "      <td>False</td>\n",
       "      <td>Helicopter crash</td>\n",
       "      <td>{\"searchParameters\":{\"q\":\"Kobe Bryant wikipedi...</td>\n",
       "      <td>{'input_tokens': 1355, 'output_tokens': 21, 'i...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Kobe_Bryant</td>\n",
       "      <td>Jump to content\\n\\nMain menu\\n\\nMain menu\\n\\nm...</td>\n",
       "      <td>{'input_tokens': 74108, 'output_tokens': 33, '...</td>\n",
       "      <td>1978-08-23</td>\n",
       "      <td>False</td>\n",
       "      <td>helicopter crash</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Aaron Swartz</td>\n",
       "      <td>1986-11-08</td>\n",
       "      <td>False</td>\n",
       "      <td>Suicide</td>\n",
       "      <td>{\"searchParameters\":{\"q\":\"Aaron Swartz wikiped...</td>\n",
       "      <td>{'input_tokens': 1329, 'output_tokens': 21, 'i...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Aaron_Swartz</td>\n",
       "      <td>Jump to content\\n\\nMain menu\\n\\nMain menu\\n\\nm...</td>\n",
       "      <td>{'input_tokens': 37532, 'output_tokens': 34, '...</td>\n",
       "      <td>1986-11-08</td>\n",
       "      <td>False</td>\n",
       "      <td>Suicide by hanging</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name   dob_label  alive_label              cause_label  \\\n",
       "0  Ruth Bader Ginsburg  1933-03-15        False        Pancreatic cancer   \n",
       "1           Bill Gates  1955-10-28         True                      N/A   \n",
       "2          Steph Curry  1988-03-14         True                      N/A   \n",
       "3         Scott Belsky  1980-04-18         True                      N/A   \n",
       "4           Steve Jobs  1955-02-24        False  Pancreatic tumor/cancer   \n",
       "5         Paris Hilton  1981-02-17         True                      N/A   \n",
       "6        Kurt Vonnegut  1922-11-11        False           Brain injuries   \n",
       "7           Snoop Dogg  1971-10-20         True                      N/A   \n",
       "8          Kobe Bryant  1978-08-23        False         Helicopter crash   \n",
       "9         Aaron Swartz  1986-11-08        False                  Suicide   \n",
       "\n",
       "                                              search  \\\n",
       "0  {\"searchParameters\":{\"q\":\"Ruth Bader Ginsburg ...   \n",
       "1  {\"searchParameters\":{\"q\":\"Bill Gates wikipedia...   \n",
       "2  {\"searchParameters\":{\"q\":\"Steph Curry wikipedi...   \n",
       "3  {\"searchParameters\":{\"q\":\"Scott Belsky wikiped...   \n",
       "4  {\"searchParameters\":{\"q\":\"Steve Jobs wikipedia...   \n",
       "5  {\"searchParameters\":{\"q\":\"Paris Hilton wikiped...   \n",
       "6  {\"searchParameters\":{\"q\":\"Kurt Vonnegut wikipe...   \n",
       "7  {\"searchParameters\":{\"q\":\"Snoop Dogg wikipedia...   \n",
       "8  {\"searchParameters\":{\"q\":\"Kobe Bryant wikipedi...   \n",
       "9  {\"searchParameters\":{\"q\":\"Aaron Swartz wikiped...   \n",
       "\n",
       "                                    __parse_search__  \\\n",
       "0  {'input_tokens': 1922, 'output_tokens': 23, 'i...   \n",
       "1  {'input_tokens': 1809, 'output_tokens': 20, 'i...   \n",
       "2  {'input_tokens': 1339, 'output_tokens': 20, 'i...   \n",
       "3  {'input_tokens': 1566, 'output_tokens': 21, 'i...   \n",
       "4  {'input_tokens': 1625, 'output_tokens': 20, 'i...   \n",
       "5  {'input_tokens': 1322, 'output_tokens': 20, 'i...   \n",
       "6  {'input_tokens': 1369, 'output_tokens': 22, 'i...   \n",
       "7  {'input_tokens': 1702, 'output_tokens': 20, 'i...   \n",
       "8  {'input_tokens': 1355, 'output_tokens': 21, 'i...   \n",
       "9  {'input_tokens': 1329, 'output_tokens': 21, 'i...   \n",
       "\n",
       "                                       wikipedia_url  \\\n",
       "0  https://en.wikipedia.org/wiki/Ruth_Bader_Ginsburg   \n",
       "1           https://en.wikipedia.org/wiki/Bill_Gates   \n",
       "2        https://en.wikipedia.org/wiki/Stephen_Curry   \n",
       "3         https://en.wikipedia.org/wiki/Scott_Belsky   \n",
       "4           https://en.wikipedia.org/wiki/Steve_Jobs   \n",
       "5         https://en.wikipedia.org/wiki/Paris_Hilton   \n",
       "6        https://en.wikipedia.org/wiki/Kurt_Vonnegut   \n",
       "7           https://en.wikipedia.org/wiki/Snoop_Dogg   \n",
       "8          https://en.wikipedia.org/wiki/Kobe_Bryant   \n",
       "9         https://en.wikipedia.org/wiki/Aaron_Swartz   \n",
       "\n",
       "                                           wikipedia  \\\n",
       "0  Jump to content\\n\\nMain menu\\n\\nMain menu\\n\\nm...   \n",
       "1  Jump to content\\n\\nMain menu\\n\\nMain menu\\n\\nm...   \n",
       "2  Jump to content\\n\\nMain menu\\n\\nMain menu\\n\\nm...   \n",
       "3  Jump to content\\n\\nMain menu\\n\\nMain menu\\n\\nm...   \n",
       "4  Jump to content\\n\\nMain menu\\n\\nMain menu\\n\\nm...   \n",
       "5  Jump to content\\n\\nMain menu\\n\\nMain menu\\n\\nm...   \n",
       "6  Jump to content\\n\\nMain menu\\n\\nMain menu\\n\\nm...   \n",
       "7  Jump to content\\n\\nMain menu\\n\\nMain menu\\n\\nm...   \n",
       "8  Jump to content\\n\\nMain menu\\n\\nMain menu\\n\\nm...   \n",
       "9  Jump to content\\n\\nMain menu\\n\\nMain menu\\n\\nm...   \n",
       "\n",
       "                                    __extract_data__ date_of_birth  alive  \\\n",
       "0  {'input_tokens': 46522, 'output_tokens': 37, '...    1933-03-15  False   \n",
       "1  {'input_tokens': 46613, 'output_tokens': 32, '...    1955-10-28   True   \n",
       "2  {'input_tokens': 64861, 'output_tokens': 32, '...    1988-03-14   True   \n",
       "3  {'input_tokens': 2227, 'output_tokens': 32, 'i...    1980-04-18   True   \n",
       "4  {'input_tokens': 47086, 'output_tokens': 42, '...    1955-02-24  False   \n",
       "5  {'input_tokens': 49288, 'output_tokens': 32, '...    1981-02-17   True   \n",
       "6  {'input_tokens': 29700, 'output_tokens': 45, '...    1922-11-11  False   \n",
       "7  {'input_tokens': 40901, 'output_tokens': 32, '...    1971-10-20   True   \n",
       "8  {'input_tokens': 74108, 'output_tokens': 33, '...    1978-08-23  False   \n",
       "9  {'input_tokens': 37532, 'output_tokens': 34, '...    1986-11-08  False   \n",
       "\n",
       "                                      cause_of_death  __eval_fn__  \n",
       "0      complications of metastatic pancreatic cancer          1.0  \n",
       "1                                                N/A          1.0  \n",
       "2                                                N/A          1.0  \n",
       "3                                                N/A          1.0  \n",
       "4  respiratory arrest related to a pancreatic neu...          1.0  \n",
       "5                                                N/A          1.0  \n",
       "6  brain injuries incurred several weeks prior, f...          1.0  \n",
       "7                                                N/A          1.0  \n",
       "8                                   helicopter crash          1.0  \n",
       "9                                 Suicide by hanging          1.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = [\n",
    "  (\"Ruth Bader Ginsburg\", \"1933-03-15\", False, \"Pancreatic cancer\"),\n",
    "  (\"Bill Gates\", \"1955-10-28\", True, \"N/A\"),\n",
    "  (\"Steph Curry\", \"1988-03-14\", True, \"N/A\"),\n",
    "  (\"Scott Belsky\", \"1980-04-18\", True, \"N/A\"),\n",
    "  (\"Steve Jobs\", \"1955-02-24\", False, \"Pancreatic tumor/cancer\"),\n",
    "  (\"Paris Hilton\", \"1981-02-17\", True, \"N/A\"),\n",
    "  (\"Kurt Vonnegut\", \"1922-11-11\", False, \"Brain injuries\"),\n",
    "  (\"Snoop Dogg\", \"1971-10-20\", True, \"N/A\"),\n",
    "  (\"Kobe Bryant\", \"1978-08-23\", False, \"Helicopter crash\"),\n",
    "  (\"Aaron Swartz\", \"1986-11-08\", False, \"Suicide\")\n",
    "]\n",
    "df = pd.DataFrame([{\"name\": d[0], \"dob_label\": d[1], \"alive_label\": d[2], \"cause_label\": d[3]} for d in data])\n",
    "\n",
    "class EvalResult(BaseModel):\n",
    "  result: bool = Field(description=\"Is the answer correct or not?\")\n",
    "\n",
    "cause_evaluator = LLMStructuredStep(\n",
    "  model=models.gpt4,\n",
    "  prompt=lambda row: f\"This is the correct cause of death: {row['cause_label']}. Is this provided cause of death accurate? The phrasing might be slightly different. Use your judgement: \\n{row['cause_of_death']}\",\n",
    "  out_schema=EvalResult,\n",
    "  name=\"cause_evaluator\")\n",
    "\n",
    "def eval_fn(row):\n",
    "  score = 0\n",
    "  if row['date_of_birth'] == row['dob_label']:\n",
    "    score += 0.25\n",
    "  if row['alive'] == row['alive_label']:\n",
    "    score += 0.25\n",
    "  if row['cause_label'] == \"N/A\":\n",
    "    if row['cause_of_death'] == \"N/A\":\n",
    "      score += 0.5\n",
    "  elif cause_evaluator.run(row)['result']:\n",
    "    score += 0.5  \n",
    "  return score\n",
    "\n",
    "pipeline.run(df)\n",
    "print(\"Score: \", pipeline.evaluate(eval_fn))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Optimizing the pipeline\n",
    "\n",
    "This pipeline has an accuracy score of 100%, but perhaps there's room for improvement on cost and speed. First let's view the cost and latency of each step to figure out which one is the bottleneck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step search:\n",
      "- Latency: 12.000389575958252\n",
      "- Cost: 0.0\n",
      "Step parse_search:\n",
      "- Latency: 10.51110366685316\n",
      "- Cost: 0.008334\n",
      "Step wikipedia:\n",
      "- Latency: 4.235257387161255\n",
      "- Cost: 0.0\n",
      "Step extract_data:\n",
      "- Latency: 90.95815300196409\n",
      "- Cost: 4.7203800000000005\n"
     ]
    }
   ],
   "source": [
    "for step in pipeline.steps:\n",
    "  print(f\"Step {step.name}:\")\n",
    "  print(f\"- Latency: {step.statistics.total_latency}\")\n",
    "  print(f\"- Cost: {step.statistics.input_cost + step.statistics.output_cost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly the final step (`extract_data`) is the one responsible for the bulk of the cost and latency. This makes sense, because we're feeding in the entire wikipedia article to GPT-4, one of the most expensive models.\n",
    "\n",
    "Let's find out if we can get away with a cheaper/faster model. Most models cannot handle the number of tokens needed to ingest a whole wikipedia article, so we'll turn to the two that can that are also cheaper than GPT4: Claude 3 Sonnet and Claude 3 Haiku."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying step search: 100%|██████████| 10/10 [00:08<00:00,  1.20it/s]\n",
      "Applying step parse_search: 100%|██████████| 10/10 [00:10<00:00,  1.06s/it]\n",
      "Applying step wikipedia: 100%|██████████| 10/10 [00:03<00:00,  2.56it/s]\n",
      "Applying step extract_data_new: 100%|██████████| 10/10 [01:26<00:00,  8.63s/it]\n",
      "Applying step search: 100%|██████████| 10/10 [00:08<00:00,  1.18it/s]\n",
      "Applying step parse_search: 100%|██████████| 10/10 [00:10<00:00,  1.03s/it]\n",
      "Applying step wikipedia: 100%|██████████| 10/10 [00:03<00:00,  2.57it/s]\n",
      "Applying step extract_data_new: 100%|██████████| 10/10 [05:17<00:00, 31.73s/it]\n",
      "/Users/amandhesi/llm/superpipe/superpipe/util.py:44: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.\n",
      "  styler = styler.applymap(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_a82f5_row0_col1, #T_a82f5_row0_col2, #T_a82f5_row0_col3, #T_a82f5_row0_col4 {\n",
       "  background-color: rgb(0,255,0);\n",
       "  color: black;\n",
       "}\n",
       "#T_a82f5_row1_col1, #T_a82f5_row1_col2, #T_a82f5_row1_col3, #T_a82f5_row1_col4 {\n",
       "  background-color: rgb(255,0,0);\n",
       "  color: black;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_a82f5\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_a82f5_level0_col0\" class=\"col_heading level0 col0\" >extract_data_new__model</th>\n",
       "      <th id=\"T_a82f5_level0_col1\" class=\"col_heading level0 col1\" >score</th>\n",
       "      <th id=\"T_a82f5_level0_col2\" class=\"col_heading level0 col2\" >input_cost</th>\n",
       "      <th id=\"T_a82f5_level0_col3\" class=\"col_heading level0 col3\" >output_cost</th>\n",
       "      <th id=\"T_a82f5_level0_col4\" class=\"col_heading level0 col4\" >total_latency</th>\n",
       "      <th id=\"T_a82f5_level0_col5\" class=\"col_heading level0 col5\" >input_tokens</th>\n",
       "      <th id=\"T_a82f5_level0_col6\" class=\"col_heading level0 col6\" >output_tokens</th>\n",
       "      <th id=\"T_a82f5_level0_col7\" class=\"col_heading level0 col7\" >num_success</th>\n",
       "      <th id=\"T_a82f5_level0_col8\" class=\"col_heading level0 col8\" >num_failure</th>\n",
       "      <th id=\"T_a82f5_level0_col9\" class=\"col_heading level0 col9\" >index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_a82f5_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_a82f5_row0_col0\" class=\"data row0 col0\" >claude-3-haiku-20240307</td>\n",
       "      <td id=\"T_a82f5_row0_col1\" class=\"data row0 col1\" >1.000000</td>\n",
       "      <td id=\"T_a82f5_row0_col2\" class=\"data row0 col2\" >0.129856</td>\n",
       "      <td id=\"T_a82f5_row0_col3\" class=\"data row0 col3\" >0.001945</td>\n",
       "      <td id=\"T_a82f5_row0_col4\" class=\"data row0 col4\" >109.038948</td>\n",
       "      <td id=\"T_a82f5_row0_col5\" class=\"data row0 col5\" >defaultdict(<class 'int'>, {'gpt-3.5-turbo-0125': 15056, 'claude-3-haiku-20240307': 487402})</td>\n",
       "      <td id=\"T_a82f5_row0_col6\" class=\"data row0 col6\" >defaultdict(<class 'int'>, {'gpt-3.5-turbo-0125': 208, 'claude-3-haiku-20240307': 1218})</td>\n",
       "      <td id=\"T_a82f5_row0_col7\" class=\"data row0 col7\" >10</td>\n",
       "      <td id=\"T_a82f5_row0_col8\" class=\"data row0 col8\" >0</td>\n",
       "      <td id=\"T_a82f5_row0_col9\" class=\"data row0 col9\" >4643861466949536679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a82f5_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_a82f5_row1_col0\" class=\"data row1 col0\" >claude-3-sonnet-20240229</td>\n",
       "      <td id=\"T_a82f5_row1_col1\" class=\"data row1 col1\" >0.450000</td>\n",
       "      <td id=\"T_a82f5_row1_col2\" class=\"data row1 col2\" >1.465117</td>\n",
       "      <td id=\"T_a82f5_row1_col3\" class=\"data row1 col3\" >0.022944</td>\n",
       "      <td id=\"T_a82f5_row1_col4\" class=\"data row1 col4\" >339.825781</td>\n",
       "      <td id=\"T_a82f5_row1_col5\" class=\"data row1 col5\" >defaultdict(<class 'int'>, {'gpt-3.5-turbo-0125': 14733, 'claude-3-sonnet-20240229': 488036})</td>\n",
       "      <td id=\"T_a82f5_row1_col6\" class=\"data row1 col6\" >defaultdict(<class 'int'>, {'gpt-3.5-turbo-0125': 208, 'claude-3-sonnet-20240229': 1786})</td>\n",
       "      <td id=\"T_a82f5_row1_col7\" class=\"data row1 col7\" >10</td>\n",
       "      <td id=\"T_a82f5_row1_col8\" class=\"data row1 col8\" >0</td>\n",
       "      <td id=\"T_a82f5_row1_col9\" class=\"data row1 col9\" >3722756468172814577</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fa9301f7df0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from superpipe.grid_search import GridSearch\n",
    "from superpipe.models import claude3_haiku, claude3_sonnet\n",
    "from superpipe.steps import LLMStructuredCompositeStep\n",
    "\n",
    "# we need to use LLMStructuredCompositeStep which uses GPT3.5 for structured JSON extraction\n",
    "# because Claude does not support JSON mode or function calling out of the box\n",
    "new_extract_step = LLMStructuredCompositeStep(\n",
    "  model=models.claude3_haiku,\n",
    "  prompt=extract_step.prompt,\n",
    "  out_schema=ExtractedData,\n",
    "  name=\"extract_data_new\"\n",
    ")\n",
    "\n",
    "new_pipeline = Pipeline([\n",
    "  search_step,\n",
    "  parse_search_step,\n",
    "  fetch_wikipedia_step,\n",
    "  new_extract_step\n",
    "], evaluation_fn=eval_fn)\n",
    "\n",
    "param_grid = {\n",
    "  new_extract_step.name:{\n",
    "    \"model\": [claude3_haiku, claude3_sonnet]}\n",
    "}\n",
    "grid_search = GridSearch(new_pipeline, param_grid)\n",
    "grid_search.run(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strangely, Claude 3 Haiku is both more accurate (100% v/s 45%) as well as cheaper and faster. This is suprising, but useful information that we wouldn't have found out unless we built and evaluated pipelines on _our specific data_ rather than benchmark data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying step search: 100%|██████████| 10/10 [00:08<00:00,  1.14it/s]\n",
      "Applying step parse_search: 100%|██████████| 10/10 [00:11<00:00,  1.15s/it]\n",
      "Applying step wikipedia: 100%|██████████| 10/10 [00:03<00:00,  2.52it/s]\n",
      "Applying step extract_data_new: 100%|██████████| 10/10 [01:27<00:00,  8.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  1.0\n",
      "Step search:\n",
      "- Latency: 8.75270938873291\n",
      "- Cost: 0.0\n",
      "Step parse_search:\n",
      "- Latency: 11.506851500831544\n",
      "- Cost: 0.007930999999999999\n",
      "Step wikipedia:\n",
      "- Latency: 3.9602952003479004\n",
      "- Cost: 0.0\n",
      "Step extract_data_new:\n",
      "- Latency: 87.57113150181249\n",
      "- Cost: 0.12396325000000001\n"
     ]
    }
   ],
   "source": [
    "best_params = grid_search.best_params\n",
    "new_pipeline.update_params(best_params)\n",
    "new_pipeline.run(df)\n",
    "print(\"Score: \", new_pipeline.score)\n",
    "for step in new_pipeline.steps:\n",
    "  print(f\"Step {step.name}:\")\n",
    "  print(f\"- Latency: {step.statistics.total_latency}\")\n",
    "  print(f\"- Cost: {step.statistics.input_cost + step.statistics.output_cost}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snappy-L-l7h_3B-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
