{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Superpipe - optimized LLM pipelines for structured data","text":"<p>A lightweight framework to build, evaluate and optimize LLM pipelines for structured outputs: data labeling, extraction, classification, and tagging. Evaluate pipelines on your own data and optimize models, prompts and other parameters for the best accuracy, cost, and speed.</p> <p></p>"},{"location":"#installation","title":"Installation","text":"<p>Make sure you have Python 3.10+ installed, then run</p> <pre><code>pip install superpipe-py\n</code></pre>"},{"location":"#build-evaluate-optimize","title":"Build, evaluate, optimize","text":"<p>There are three stages of using Superpipe.</p> <ol> <li>Build \u2014 use your favorite LLM library (langchain, LlamaIndex) and combine with Superpipe's building blocks.</li> <li>Evaluate \u2014 your pipeline needs to be evaluated on your data. Your data and use case are unique, so benchmarks are insufficient.</li> <li>Optimize \u2014 build once, experiment many times. Easily try different models, prompts, and parameters to optimize end-to-end.</li> </ol> <p>To see a toy example, keep reading. For more details go to Step 1: Build</p>"},{"location":"#build","title":"Build","text":"<p>In this toy example, we'll use Superpipe to classify someone's work history into job departments. A superpipe pipeline consists of one or more steps. Each step takes in an input dataframe or dictionary and returns a new dataframe or dictionary with the outputs of the step appended.</p> <p>Below, we use a built-in Superpipe step: <code>LLMStructuredStep</code> which extracts structured data using an LLM call. The expected structure is specified by a Pydantic model.</p> <pre><code>from superpipe.steps import LLMStructuredStep\nfrom superpipe.models import gpt35\nfrom pydantic import BaseModel, Field\n\nwork_history = \"Software engineer at Tech Innovations, project manager at Creative Solutions, CTO at Startup Dreams.\"\ninput = {\"work_history\": work_history}\n\ndef current_job_prompt(row):\n  return f\"\"\"Given an employees work history, classify them into one of the following departments:\n  HR, Legal, Finance, Sales, Product, Founder, Engineering\n  {row['work_history']}\"\"\"\n\nclass Department(BaseModel):\n    job_department: str = Field(description=\"Job department\")\n\njob_department_step = LLMStructuredStep(\n  model=gpt35,\n  prompt=current_job_prompt,\n  out_schema=Department,\n  name=\"job_department\")\n\njob_department_step.run(input)\n</code></pre> Show output <p>In addition to the input (<code>work_history</code>) and result (<code>job_department</code>), the output also contains some step metadata for the <code>job_department</code> step including token usage, cost, and latency.</p> <pre><code>{\n  \"work_history\": \"Software engineer at Tech Innovations, project manager at Creative Solutions, CTO at Startup Dreams.\",\n  \"__job_department__\": {\n    \"input_tokens\": 97,\n    \"output_tokens\": 10,\n    \"input_cost\": 0.0000485,\n    \"output_cost\": 0.000015,\n    \"success\": true,\n    \"error\": null,\n    \"latency\": 0.9502187501639128,\n    \"content\": {\n      \"job_department\": \"Engineering\"\n    }\n  },\n  \"job_department\": \"Engineering\"\n}\n</code></pre>"},{"location":"#evaluate","title":"Evaluate","text":"<p>Once you've built your pipeline it's time to see how well it works. Think of this as unit tests for your code. You wouldn't ship code to production without testing it, you shouldn't ship LLM pipelines to production without evaluating them.</p> <p>This requires:</p> <ul> <li>A dataset with labels - the correct label for each row in your data. You can use an early version of your pipeline to generate candidate labels and manually inspect and correct to generate your ground truth.</li> <li>Evaluation function - a function that defines what \"correct\" is. In this example we use a simple string comparison evaluation function, but in general it could be any arbitrary function, including a call to an LLM to do more advanced evals.</li> </ul> <pre><code>from superpipe.pipeline import Pipeline\nimport pandas as pd\n\nwork_histories = [\n  \"Software engineer at Tech Innovations, project manager at Creative Solutions, CTO at Startup Dreams.\",\n  \"Journalist for The Daily News, senior writer at Insight Magazine, currently Investor at VC Global.\",\n  \"Sales associate at Retail Giant, sales manager at Boutique Chain, now regional sales director at Luxury Brands Inc.\"\n]\nlabels = [\n  \"Engineering\",\n  \"Finance\",\n  \"Sales\"\n]\ninput = pd.DataFrame([{\"work_history\": work_histories[i], \"label\": labels[i]} for i in range(3)])\nevaluate = lambda row: row[\"job_department\"] == row[\"label\"]\n\ncategorizer = Pipeline(\n  steps=[job_department_step],\n  evaluation_fn=evaluate)\ncategorizer.run(input)\n\nprint(categorizer.statistics)\n</code></pre> Show output <p>The <code>score</code> field is calculated by applying the evaluate function on each row. In this case we were able to correctly classify each row so the score is 1 (i.e. 100%). We can also see the total cost and latency.</p> <pre><code>+---------------+------------------------------+\n|     score     |             1.0              |\n+---------------+------------------------------+\n|  input_tokens | {'gpt-3.5-turbo-0125': 1252} |\n+---------------+------------------------------+\n| output_tokens | {'gpt-3.5-turbo-0125': 130}  |\n+---------------+------------------------------+\n|   input_cost  |    $0.0006259999999999999    |\n+---------------+------------------------------+\n|  output_cost  |   $0.00019500000000000005    |\n+---------------+------------------------------+\n|  num_success  |              3               |\n+---------------+------------------------------+\n|  num_failure  |              0               |\n+---------------+------------------------------+\n| total_latency |      9.609524499624968       |\n+---------------+------------------------------+\n</code></pre>"},{"location":"#optimize","title":"Optimize","text":"<p>The last step in using Superpipe is trying out many combinations of parameters to optimize your pipeline along cost, accuracy, and speed. In this example, we'll try two different models and two prompts (4 combinations). Superpipe's grid search makes it easy to try all combinations - build once, experiment many times.</p> <pre><code>from superpipe.grid_search import GridSearch\nfrom superpipe.models import gpt35, gpt4\n\ndef short_job_prompt(row):\n  return f\"\"\"Classify into:  HR, Legal, Finance, Sales, Product, Founder, Engineering\n  {row['work_history']}\"\"\"\n\nparams_grid = {\n    job_department_step.name: {\n        \"model\": [gpt35, gpt4],\n        \"prompt\": [current_job_prompt, short_job_prompt]\n    },\n}\n\ngrid_search = GridSearch(categorizer, params_grid)\ngrid_search.run(input)\n</code></pre> Show output <p>The results of the grid search show that:</p> <ol> <li>The longer prompt is more accurate even though it costs more and is slower</li> <li>There's no advantage in using gpt4 instead of gpt3.5</li> </ol> <p><p></p></p>"},{"location":"#next-steps","title":"Next Steps","text":"<p>Step 1: Build \u2014 to get started building with Superpipe.</p> <p>Concepts \u2014 to understand the core concepts behind Superpipe.</p> <p>Why Superpipe? \u2014 to understand whether Superpipe is right for you.</p> <p>Examples \u2014 for more advanced examples and usage.</p>"},{"location":"build/","title":"Step 1: Build","text":"<p>We'll use Superpipe to build a pipeline that receives a famous person's name and figures out their birthday, whether they're still alive, and if not, their cause of death.</p> <p>This pipeline will work in 4 steps -</p> <ol> <li>Do a google search with the person's name</li> <li>Use an LLM to fetch the URL of their wikipedia page from the search results</li> <li>Fetch the contents of the wikipedia page and convert them to markdown</li> <li>Use an LLM to extract the birthdate and living or dead from the wikipedia contents</li> </ol> <p>We'll build the pipeline, evaluate it on some data, and optimize it to maximize accuracy while reducing cost and latency.</p> <p>View notebook on Github</p> <pre><code>from superpipe.steps import LLMStructuredStep, CustomStep, SERPEnrichmentStep\nfrom superpipe import models\nfrom pydantic import BaseModel, Field\n\n# Step 1: use Superpipe's built-in SERP enrichment step to search for the persons wikipedia page\n# Include a unique \"name\" for the step that will used to reference this step's output in future steps\n\nsearch_step = SERPEnrichmentStep(\n  prompt= lambda row: f\"{row['name']} wikipedia\",\n  name=\"search\"\n)\n\n# Step 2: Use an LLM to extract the wikipedia URL from the search results\n# First, define a Pydantic model that specifies the structured output we want from the LLM\n\nclass ParseSearchResult(BaseModel):\n  wikipedia_url: str = Field(description=\"The URL of the Wikipedia page for the person\")\n\n# Then we use the built-in LLMStructuredStep and specify a model and a prompt\n# The prompt is a function that has access to all the fields in the input as well as the outputs of previous steps\n\nparse_search_step = LLMStructuredStep(\n  model=models.gpt35,\n  prompt= lambda row: f\"Extract the Wikipedia URL for {row['name']} from the following search results: \\n\\n {row['search']}\",\n  out_schema=ParseSearchResult,\n  name=\"parse_search\"\n)\n</code></pre> <p>Next, we'll create the final 2 steps of the pipeline and then the pipeline itself.</p> <pre><code>from superpipe.pipeline import Pipeline\nimport requests\nimport html2text\n\nh = html2text.HTML2Text()\nh.ignore_links = True\n\n# Step 3: we create a CustomStep that can execute any arbitrary function (transform)\n# The function fetches the contents of the wikipedia url and converts them to markdown\n\nfetch_wikipedia_step = CustomStep(\n  transform=lambda row: h.handle(requests.get(row['wikipedia_url']).text),\n  name=\"wikipedia\"\n)\n\n# Step 4: we extract the date of birth, living/dead status and cause of death from the wikipedia contents\n\nclass ExtractedData(BaseModel):\n    date_of_birth: str = Field(description=\"The date of birth of the person in the format YYYY-MM-DD\")\n    alive: bool = Field(description=\"Whether the person is still alive\")\n    cause_of_death: str = Field(description=\"The cause of death of the person. If the person is alive, return 'N/A'\")\n\nextract_step = LLMStructuredStep(\n  model=models.gpt4,\n  prompt= lambda row: f\"\"\"Extract the date of birth for {row['name']}, whether they're still alive \\\n  and if not, their cause of death from the following Wikipedia content: \\n\\n {row['wikipedia']}\"\"\",\n  out_schema=ExtractedData,\n  name=\"extract_data\"\n)\n\n# Finally we define and run the pipeline\n\npipeline = Pipeline([\n  search_step,\n  parse_search_step,\n  fetch_wikipedia_step,\n  extract_step\n])\n\npipeline.run({\"name\": \"Jean-Paul Sartre\"})\n</code></pre>"},{"location":"build/#output","title":"Output","text":"<p>When we run the pipeline, it returns an object that contains:</p> <ul> <li>the original input</li> <li>the outputs of each step in the pipeline</li> <li>some metadata associated with each LLM step</li> </ul> <pre><code>{\n  \"name\": \"Jean-Paul Sartre\",\n  \"search\": \"{\\\"searchParameters\\\":{\\\"q\\\":\\\"Jean-Paul Sartre wikipedia\\\",\\\"type\\\":\\\"search\\\",\\\"engine\\\":\\\"google\\\"},...}\",\n  \"__parse_search__\": {\n    \"input_tokens\": 1704,\n    \"output_tokens\": 23,\n    \"input_cost\": 0.000852,\n    \"output_cost\": 3.45e-5,\n    \"success\": true,\n    \"error\": null,\n    \"latency\": 0.9851684169843793,\n    \"content\": {\n      \"wikipedia_url\": \"https://en.wikipedia.org/wiki/Jean-Paul_Sartre\"\n    }\n  },\n  \"wikipedia_url\": \"https://en.wikipedia.org/wiki/Jean-Paul_Sartre\",\n  \"wikipedia\": \"Jump to content\\n\\nMain menu\\n\\nMain menu\\n\\nmove to sidebar hide\\n\\nNavigation\\n\\n  *...\",\n  \"__extract_data__\": {\n    \"input_tokens\": 32542,\n    \"output_tokens\": 35,\n    \"input_cost\": 0.32542,\n    \"output_cost\": 0.00105,\n    \"success\": true,\n    \"error\": null,\n    \"latency\": 8.941851082723588,\n    \"content\": {\n      \"date_of_birth\": \"1905-06-21\",\n      \"alive\": false,\n      \"cause_of_death\": \"pulmonary edema\"\n    }\n  },\n  \"date_of_birth\": \"1905-06-21\",\n  \"alive\": false,\n  \"cause_of_death\": \"pulmonary edema\"\n}\n</code></pre> <p>Inspecting the last 3 fields of the output, we see that its results were correct.</p> <p>Now, let's see how to evaluate this pipeline on a labeled dataset.</p>"},{"location":"build/#next-steps","title":"Next Steps","text":"<p>Step 2: Evaluate \u2014 to learn about evaluating your pipelines.</p> <p>Concepts \u2014 to understand the core concepts behind Superpipe.</p> <p>Examples \u2014 for more advanced examples and usage.</p>"},{"location":"consulting/","title":"Work with us","text":"<p>We're currently taking a limited number of consulting projects. In our past consulting work we've found we can accelerate your AI projects and build production-ready AI software in days or weeks, not months or years. We can also help you understand where AI can have the most impact in your business.</p>"},{"location":"consulting/#contact-us","title":"Contact us","text":"<p>Get in touch by filling out this short form.</p>"},{"location":"consulting/#past-work","title":"Past work","text":"<p>We've worked with a number of companies in e-commerce, insurance, consumer tech, and B2B SaaS helping with RAG pipelines, data extraction and classification tasks.</p> <p>Reach out to us if you'd like to see case studies.</p>"},{"location":"consulting/#who-we-are","title":"Who we are","text":"<p>We're Ben and Aman. We both have backgrounds in Machine Learning/AI and have spent many years applying AI at Google, Facebook, Square and DoorDash.</p>"},{"location":"evaluate/","title":"Step 2: Evaluate","text":"<p>In Step 1 we built a pipeline that receives a famous person's name and figures out their birthday, whether they're still alive, and if not, their cause of death. Now, we'll evaluate it on a dataset. Think of this as unit tests for your code. You wouldn't ship code to production without testing it, you shouldn't ship LLM pipelines to production without evaluating them.</p> <p>To do this, we need:</p> <ul> <li> <p>A dataset with labels - In this case we need a list of famous people and the true date of birth, living status and cause of death of each person. We will assume we already have these, although in the real world we'd use our pipeline to create candidate labels and manually correct them where wrong.</p> </li> <li> <p>Evaluation function - a function that defines what \"correct\" is. We'll use simple comparison for date of birth and living status, and an LLM call to evaluate the correctness of cause of death.</p> </li> </ul> <p>View notebook on Github</p> <pre><code>import pandas as pd\n\ndata = [\n  (\"Ruth Bader Ginsburg\", \"1933-03-15\", False, \"Pancreatic cancer\"),\n  (\"Bill Gates\", \"1955-10-28\", True, \"N/A\"),\n  (\"Steph Curry\", \"1988-03-14\", True, \"N/A\"),\n  (\"Scott Belsky\", \"1980-04-18\", True, \"N/A\"),\n  (\"Steve Jobs\", \"1955-02-24\", False, \"Pancreatic tumor/cancer\"),\n  (\"Paris Hilton\", \"1981-02-17\", True, \"N/A\"),\n  (\"Kurt Vonnegut\", \"1922-11-11\", False, \"Brain injuries\"),\n  (\"Snoop Dogg\", \"1971-10-20\", True, \"N/A\"),\n  (\"Kobe Bryant\", \"1978-08-23\", False, \"Helicopter crash\"),\n  (\"Aaron Swartz\", \"1986-11-08\", False, \"Suicide\")\n]\ndf = pd.DataFrame([{\"name\": d[0], \"dob_label\": d[1], \"alive_label\": d[2], \"cause_label\": d[3]} for d in data])\n\nclass EvalResult(BaseModel):\n  result: bool = Field(description=\"Is the answer correct or not?\")\n\ncause_evaluator = LLMStructuredStep(\n  model=models.gpt4,\n  prompt=lambda row: f\"This is the correct cause of death: {row['cause_label']}. Is this provided cause of death accurate? The phrasing might be slightly different. Use your judgement: \\n{row['cause_of_death']}\",\n  out_schema=EvalResult,\n  name=\"cause_evaluator\")\n\ndef eval_fn(row):\n  score = 0\n  if row['date_of_birth'] == row['dob_label']:\n    score += 0.25\n  if row['alive'] == row['alive_label']:\n    score += 0.25\n  if row['cause_label'] == \"N/A\":\n    if row['cause_of_death'] == \"N/A\":\n      score += 0.5\n  elif cause_evaluator.run(row)['result']:\n    score += 0.5\n  return score\n\npipeline.run(df)\nprint(\"Score: \", pipeline.evaluate(eval_fn))\ndf\n</code></pre>"},{"location":"evaluate/#output","title":"Output","text":"<pre><code>Score: 1.0\n</code></pre> <p>Our pipeline gets an accuracy score of 100% on this dataset! This is great news. But before we deploy this in production, we'll look at the cost and speed of the pipeline and try to optimize it.</p>"},{"location":"evaluate/#a-note-on-ground-truth-labels","title":"A note on ground truth labels","text":"<p>So far we've assumed we somehow had ground truth labels already. Usually this is not the case. Since our pipeline had 100% accuracy, we could've used it to generate ground truth labels if we didn't already have them. We'd of course have to manually inspect and fix any incorrect ones, which is a slow and tedious process. We're building Superpipe Studio to make this process easier.</p>"},{"location":"evaluate/#next-steps","title":"Next Steps","text":"<p>Optimize \u2014 to learn about optimizing your pipelines for accuracy, cost &amp; speed.</p> <p>Concepts \u2014 to understand the core concepts behind Superpipe.</p> <p>Examples \u2014 for more advanced examples and usage.</p>"},{"location":"optimize/","title":"Step 3: Optimize","text":"<p>In Step 1 we built a pipeline that receives a famous person's name and figures out their birthday, whether they're still alive, and if not, their cause of death. In Step 2 we evaluated our pipeline on a dataset and found it was 100% accurate. Now we'll optimize it to reduce cost and speed while hopefully maintaining accuracy.</p> <p>In the previous step the pipeline had an accuracy score of 100%, but perhaps there's room for improvement on cost and speed. First let's view the cost and latency of each step to figure out which one is the bottleneck.</p> <p>View notebook on Github</p> <pre><code>for step in pipeline.steps:\n  print(f\"Step {step.name}:\")\n  print(f\"- Latency: {step.statistics.total_latency}\")\n  print(f\"- Cost: {step.statistics.input_cost + step.statistics.output_cost}\")\n</code></pre> <p>Output:</p> <pre><code>Step search:\n- Latency: 10.888118982315063\n- Cost: 0.0\nStep parse_search:\n- Latency: 13.858203417155892\n- Cost: 0.0085\nStep wikipedia:\n- Latency: 5.329825401306152\n- Cost: 0.0\nStep extract_data:\n- Latency: 105.81304024951532\n- Cost: 4.72056\n</code></pre> <p>Clearly the final step (<code>extract_data</code>) is the one responsible for the bulk of the cost and latency. This makes sense, because we're feeding in the entire wikipedia article to GPT-4, one of the most expensive models.</p> <p>Let's find out if we can get away with a cheaper/faster model. Most models cannot handle the number of tokens needed to ingest a whole wikipedia article, so we'll turn to the two that can that are also cheaper than GPT4: Claude 3 Sonnet and Claude 3 Haiku.</p> <pre><code>from superpipe.grid_search import GridSearch\nfrom superpipe.models import claude3_haiku, claude3_sonnet\nfrom superpipe.steps import LLMStructuredCompositeStep\n\n# we need to use LLMStructuredCompositeStep which uses GPT3.5 for structured JSON extraction\n# because Claude does not support JSON mode or function calling out of the box\nnew_extract_step = LLMStructuredCompositeStep(\n  model=models.claude3_haiku,\n  prompt=extract_step.prompt,\n  out_schema=ExtractedData,\n  name=\"extract_data_new\"\n)\n\nnew_pipeline = Pipeline([\n  search_step,\n  parse_search_step,\n  fetch_wikipedia_step,\n  new_extract_step\n], evaluation_fn=eval_fn)\n\nparam_grid = {\n  new_extract_step.name:{\n    \"model\": [claude3_haiku, claude3_sonnet]}\n}\ngrid_search = GridSearch(new_pipeline, param_grid)\ngrid_search.run(df)\n</code></pre> <p>Output:</p> <p></p> <p>Strangely, Claude 3 Haiku is both more accurate (100% v/s 45%) as well as cheaper and faster. This is suprising, but useful information that we wouldn't have found out unless we built and evaluated pipelines on our specific data rather than benchmark data.</p> <p>Finally we'll re-run the pipeline with the best params and print out the cost and latency of each step.</p> <pre><code>best_params = grid_search.best_params\nnew_pipeline.update_params(best_params)\nnew_pipeline.run(df)\nprint(\"Score: \", new_pipeline.score)\nfor step in new_pipeline.steps:\n  print(f\"Step {step.name}:\")\n  print(f\"- Latency: {step.statistics.total_latency}\")\n  print(f\"- Cost: {step.statistics.input_cost + step.statistics.output_cost}\")\n</code></pre> <p>Output:</p> <pre><code>Score:  1.0\nStep search:\n- Latency: 8.75270938873291\n- Cost: 0.0\nStep parse_search:\n- Latency: 11.506851500831544\n- Cost: 0.007930999999999999\nStep wikipedia:\n- Latency: 3.9602952003479004\n- Cost: 0.0\nStep extract_data_new:\n- Latency: 87.57113150181249\n- Cost: 0.12396325000000001\n</code></pre> <p>Incredibly, we were able to get the same score (100%) as GPT4 but with the final step being 20% faster and 38x cheaper!!</p> <p>This is why the optimization step is so important.</p>"},{"location":"optimize/#next-steps","title":"Next Steps","text":"<p>Concepts \u2014 to understand the core concepts behind Superpipe in more depth.</p> <p>Why Superpipe? \u2014 to understand whether Superpipe is right for you.</p> <p>Examples \u2014 for more advanced examples and usage.</p>"},{"location":"principles/","title":"Principles","text":"<p>Principle 1: Your LLM pipeline is only as good as your data. \u2014 This is why Superpipe is designed from the ground up to work with datasets, not just single inputs.</p> <p>Principle 2: Abstract the boilerplate and the glue (chaining, evaluating, inspecting), NOT the logic. \u2014 Superpipe abstracts away tedious things like calculating cost, speed, and accuracy. It lets you glue together existing building blocks from other libraries and your own logic.</p> <p>Principle 3: You can't optimize what you can't evaluate. You can't evaluate without ground truth.</p> <p>Principle 4: You can't optimize what you can't inspect. \u2014 Losses can happen at any step, you need to quickly understand which step caused the loss.</p> <p>Principle 5: Build once, experiment many times.</p> <p>Principle 6: Spend your time learning new ML techniques, not new Python abstractions.</p> <p>Principle 7: Don't f* with my prompts.</p>"},{"location":"roadmap/","title":"Roadmap","text":"<ol> <li>Superpipe Studio</li> <li>Copilot</li> <li>Pipeline Hosting</li> <li>Synthetic Data</li> </ol>"},{"location":"use_cases/","title":"Use Cases","text":"<p>Superpipe is useful for any data labeling, extraction, classification, or tagging task where the output is structured and the structure is known.</p>"},{"location":"use_cases/#extraction","title":"Extraction","text":"<ul> <li> <p>Document extraction \u2014 Extract entities and facts from PDFs, emails, websites, etc.</p> </li> <li> <p>Product Catalog tagging \u2014 Enrich your product catalog with AI-generated tags to power search, filtering and recommendations.</p> </li> <li> <p>Query analysis \u2014 Extract filter arguments from natural language search queries.</p> </li> </ul>"},{"location":"use_cases/#classification","title":"Classification","text":"<ul> <li> <p>Product Categorization \u2014 Categorize your product catalog into your custom taxonomy to power search, filtering and merchandising.</p> </li> <li> <p>Sentiment analysis \u2014 Analyze sentiment of customer reviews, customer support interactions and flag important themes.</p> </li> <li> <p>Customer &amp; Business classification \u2014 Classify your customers or businesses into government classification codes (NAICS/SIC) or your custom internal categories.</p> </li> <li> <p>Content moderation \u2014 Detect harmful or policy-violating content.</p> </li> </ul>"},{"location":"use_cases/#next-steps","title":"Next Steps","text":"<p>Workflow \u2014 to understand the full workflow we suggest for building pipelines.</p> <p>Examples \u2014 for more advanced examples and usage.</p>"},{"location":"why/","title":"Why Superpipe?","text":"<p>There are several great tools that help you chain together LLM calls or build agent workflows that make for great prototypes or demos. Superpipe is different for two reasons:</p> <p>Focuses exclusively on unstructured -&gt; structured (eg. extraction, classification) problems. Though it can be used for generative use cases, that is not where it shines.</p> <p>Focuses on evaluating and optimizing across accuracy, speed and cost. Accuracy, speed and cost don't matter for prototypes and demos, but they do when you're trying to solve real problems for real customers at scale.</p> <p>Superpipe does not pretend to do everything, it does a few things and does them well.</p>"},{"location":"why/#when-to-use-superpipe","title":"When to use Superpipe","text":"<p>Superpipe is a good choice if your use case has some or most of these properties</p> <ol> <li>You primarily care about structured output, not generative output.</li> <li>You care about evaluating your pipeline on a ground truth dataset (you may or may not already have labels).</li> <li>You care about finding the best combination of accuracy, cost and speed for your use case.</li> <li>You need to monitor your pipeline in production to prevent model drift and data drift.</li> <li>You want to continuously increase accuracy and decrease cost/speed as new models and techniques come out.</li> <li>You want to use your production data to create fine-tuned or use-case specific models.</li> </ol>"},{"location":"why/#when-not-to-use-superpipe","title":"When NOT to use Superpipe","text":"<p>Superpipe is not a good choice if</p> <ol> <li>You're working on generative, chat-like, or agentic use cases.</li> <li>You're building a demo or prototype.</li> <li>You just need to build something good enough and don't need to optimize across accuracy, cost and speed.</li> </ol>"},{"location":"workflow/","title":"Workflow","text":"<p>It's easier than ever to build an LLM prototype. When building LLM pipelines that are accurate, cheap and fast in production \u2014 engineers need to think like scientists. Superpipe makes this easy.</p>"},{"location":"workflow/#the-problem","title":"The Problem","text":"<p>Problem 1: You have no idea how your pipeline will perform on real data</p> <p>To properly evaluate your extraction/classification pipelines, you need high-quality labeled data. It's not sufficient to evaluate on public benchmark data, you need to evaluate on your own data to reflect true accuracy.</p> <p>Problem 2: You need to optimize your pipeline end-to-end across accuracy, cost and speed</p> <p>You can get pretty far by optimizing your prompts, trying smaller models, or changing various parameters of your pipeline. However, optimizing each piece in isolation isn't enough. Pipelines need to be optimized end-to-end.</p> <p>Problem 3: You need to monitor your pipeline in production and improve it over time</p> <p>Once your pipeline is serving production traffic, you still need to know how it's doing. Data and models drift over time. As cheaper, faster and better models come out, you need to easily try them and swap them out when it makes sense.</p>"},{"location":"workflow/#the-solution","title":"The Solution","text":"<p>Superpipe helps you solve these problems by simplifying the following workflow:</p> <p>Step 1: Build your pipeline</p> <p>Superpipe makes it easy to build extraction and classification pipelines. You can also use langchain, LlamaIndex or your favorite LLM library. Superpipe acts as the glue between components and leaves you fully in control of the logic.</p> <p>One of our principles is to abstract the boilerplate, not the logic. Using superpipe to build your pipeline, however, makes the following steps easier.</p> <p>Step 2: Generate labeled data</p> <p>Use your superpipe pipeline to create candidate labels. If you use a powerful model combined with powerful techniques you can generate reasonably accurate labels. Then you can manually inspect these labels with Superpipe Studio and fix the wrong ones. This is an important and often overlooked step. Without labeled data you can't evaluate your pipeline and without evaluation you're flying blind.</p> <p>Step 3: Evaluate your pipeline</p> <p>Armed with labeled data, you can build and evaluate cheaper and faster pipelines. You may want to combine open-source models with more complex techniques (like multi-step prompting, chain-of-though, etc.). You may want to iterate between Steps 1 &amp; 3 by trying multiple approaches and comparing them on accuracy, cost and speed.</p> <p>Step 4: Optimize your pipeline</p> <p>A pipeline usually has many parameters - foundation models, prompts, structured outputs, k-values, etc. Superpipe lets you easily find the best combination of parameters by running a grid search over the parameter space. You can compare different approaches against each other and track your experiments with Studio.</p> <p>Step 5: Deploy, monitor and further optimize</p> <p>Once you've deployed the winning pipeline to production, Studio helps monitor the accuracy, cost and speed of the pipeline. It helps you run auto-evaluations on a sub-sample of production data to make sure model drift or data drift aren't hurting. It also lets you easily backtest the newest models and techniques and compare them to production.</p> <p></p> <p>The 5-step Superpipe workflow</p> <p></p> <p>While there are a number of general-purpose LLM libraries focused on different aspects of the above, Superpipe focuses on a specific problem (data extraction and classification) and the optimal workflow for the problem. It is designed with the following goals in mind:</p> <p>Superpipe design goals</p> <ul> <li>Simplicity: easy to get started because there few abstractions to learn.</li> <li>Unopinionated: acts as connective tissue and abstracts boilerplate but leaves you in control of logic.</li> <li>Works with datasets: works natively with <code>pandas</code> dataframes so you can evaluate and optimize over datasets.</li> <li>Parametric: every aspect of the pipeline is exposed as a parameter, you can easily try different models or run hyperparameter searches.</li> <li>Plays well with others: use your favorite LLM library or tool, including langchain, LlamaIndex, DSpy, etc.</li> </ul>"},{"location":"blog/","title":"Superpipe Blog","text":""},{"location":"blog/2024/03/26/introducing-superpipe/","title":"Introducing Superpipe","text":"<p>By Aman Dhesi and Ben Scharfstein</p> <p></p> <p>Superipe is a lightweight framework to build, evaluate and optimize LLM pipelines for structured outputs: data labeling, extraction, classification, and tagging. Evaluate pipelines on your own data and optimize models, prompts and other parameters for the best accuracy, cost, and speed.</p>"},{"location":"blog/2024/03/26/introducing-superpipe/#why-we-built-superpipe","title":"Why we built Superpipe","text":"<p>For the past few months we've been helping companies with structured data problems like product classification and document extraction.</p> <p>Through working on these projects, we noticed a few problems.</p> <ol> <li>Many companies were doing \u201cvibe-check\u201d engineering with LLMs and had no idea how accurate their prompts and pipelines were.</li> <li>Usually this was because they didn't have labeled data to evaluate on. Even when they did, their labels were often inaccurate.</li> <li>And they lacked tools to rigorously compare different approaches, prompts, models and parameters. This resulted in a mess of experiments spread across notebooks.</li> </ol> <p>Despite these limitations, we were able to get very high quality results. First, we learned that LLMs are actually very good at classification and extraction when used correctly. Second, we came to the conclusion that multi-step techniques worked quite well on a wide variety of use cases and outperformed zero-shot prompting much of the time. And finally, we observed that there\u2019s usually a lot of cost and speed headroom without a loss in quality if you use cheaper models for \u201ceasy\u201d steps, and expensive models for \u201chard\u201d steps.</p> <p>After a few experiments and projects, our process became:</p> <ol> <li>Build and run a v1 pipeline with a powerful model like GPT-4 for each step.</li> <li>Evaluate our pipeline by manually labelling ground truth data.</li> <li>Optimize the pipeline over prompts, models, and other parameters.</li> </ol> <p>We built Superpipe to productize the the process of building, evaluating and optimizing the multi-step LLM pipelines we built. With Superpipe, we\u2019re able to build 10x cheaper pipelines, 10x faster.</p> <p>Today we\u2019re open-sourcing Superpipe under the MIT license so that you can build faster, better, and cheaper pipelines as well. You can view the source code here.</p>"},{"location":"blog/2024/03/26/introducing-superpipe/#superpipe-helps-engineers-think-like-scientists","title":"Superpipe helps engineers think like scientists","text":"<p>As ML engineers (even before LLMs) we knew the importance of high quality ground truth data and proper evaluation metrics. However, what we learned is that those without experience building probabilistic systems hadn\u2019t necessarily learned those lessons yet. Now that every engineer can use AI with a few lines of code, it\u2019s important that engineers start thinking more like scientists.</p> <p>To put it in traditional software engineering terms, Superpipe brings test driven development to LLM pipelines. You can think of each labeled data point as a unit test. You wouldn't ship traditional software without units tests and you shouldn't ship LLM software without evals.</p> <p>Tests will help you evaluate accuracy, but that\u2019s only half the equation. When building with LLMs, there\u2019s generally a tradeoff between cost/speed and accuracy. On the same pipeline and prompts, cheaper models are generally faster and but less accurate.</p> <p>However, pipelines aren\u2019t static. You can vary prompts, augment with retrieval, chain LLMs, enrich with public information and more. Superpipe will help you iterate faster and build cheaper and more accurate classification and extraction pipelines. In many cases, you can skip straight from v1\u2014&gt;v6.</p> <p></p>"},{"location":"blog/2024/03/26/introducing-superpipe/#how-it-works","title":"How it works","text":"<p>There are three steps to using Superpipe:</p> <ol> <li>Build - create a multistep Pipeline using Steps</li> <li>Evaluate - generate and label ground truth data. Evaluate your results on speed, cost and accuracy.</li> <li>Optimize - run a Grid Search over the parameters of your pipeline to understand the tradeoffs between models, prompts, and other parameters.</li> </ol> <p>The result of this process is a rigorous understanding of the cost, speed, and accuracy tradeoffs between different approaches, conveniently presented to you right in your notebook.</p> <p></p>"},{"location":"blog/2024/03/26/introducing-superpipe/#learn-more","title":"Learn more","text":"<p>The best way to learn more about Superpipe by reading our docs, checking out our Github, or asking questions on our Discord.</p>"},{"location":"blog/2024/03/28/comparing-gpt-4-and-claude-3-on-long-context-tasks/","title":"Comparing GPT-4 and Claude 3 on long-context tasks","text":"<p>Why you should take benchmarks with a grain of salt</p> <p>By Aman Dhesi</p> <p>Anthropic released the Claude 3 family of models last month, claiming it beats GPT-4 on all benchmarks. But others disagreed with these claims.</p> <p>So which one is it - is Claude 3 better than GPT-4 or not? And isn't the whole point of benchmarks to evaluate the models objectively and remove the guesswork?</p> <p>The answer is... it depends. Which model is better depends on what task you use the model for, and what data you use it on.</p> <p>Instead of relying on third-party benchmarks, as Hamel Husain suggests you should be evaluating models on your own, domain-specific data, with all its nuances and intricacies.</p> <p>In this short blogpost, we'll evaluate Claude 3 and GPT-4 on a specific long-context extraction task. We'll do this comparison using Superpipe which makes it easy to swap in different models and compare them on accuracy, cost and speed.</p> <p>All the code and data is available in this Colab notebook.</p>"},{"location":"blog/2024/03/28/comparing-gpt-4-and-claude-3-on-long-context-tasks/#the-task-long-context-extraction","title":"The task - long context extraction","text":"<p>For some types of tasks, we need LLMs with very long context windows. Currently the only LLMs with context windows longer than 100K are GPT-4 and the Claude 3 family of models.</p> <p>Conventional wisdom suggests that the bigger and more expensive a model is, the more accurate it is on all tasks. Let's evaluate whether this is true on a specific task - extracting information from Wikipedia pages of famous people.</p> <p>Given the wikipedia page of a (real) person, we'll use an LLM to extract their date of birth, whether or not they're still alive and if not, their cause of death.</p> <p></p> <p>We'll perform a single LLM call and pass in the entire contents of the Wikipedia page. Wikipedia pages of famous people can easily be more than 50k tokens in length, which is why only models with context windows longer than 100k are eligible for this task.</p>"},{"location":"blog/2024/03/28/comparing-gpt-4-and-claude-3-on-long-context-tasks/#the-data","title":"The data","text":"<p>Our dataset contains 49 data points, each containing 4 fields:</p> <ul> <li>a wikipedia url</li> <li>the person's true date of birth</li> <li>whether they're still alive</li> <li>if not alive, cause of death</li> </ul> <p></p> <p>The latter 3 fields are the labels, they're only used to evaluate the result of the LLM extraction. All the data can be found along with the code here.</p>"},{"location":"blog/2024/03/28/comparing-gpt-4-and-claude-3-on-long-context-tasks/#results","title":"Results","text":"<p>The results are in:</p> <ul> <li>The entire Claude 3 family outperforms GPT-4 on accuracy</li> <li>Haiku and Sonnet are both significantly cheaper than GPT-4 (34x and 3x, respectively)</li> <li>There's no accuracy benefit in using Opus over Sonnet.</li> </ul> <p>Based on these results, I would deploy Sonnet if I mainly cared about accuracy and Haiku if I was cost-sensitive.</p> <p></p>"},{"location":"blog/2024/03/28/comparing-gpt-4-and-claude-3-on-long-context-tasks/#using-superpipe-to-compare-models","title":"Using superpipe to compare models","text":"<p>Superpipe takes care of all the boilerplate when comparing models on tasks, including</p> <ul> <li>Defining an eval function, including using LLMs to perform evaluation</li> <li>Keeping track of token usage and latency</li> <li>Error handling to make sure a single error doesn't tank your whole experiment</li> </ul> <p>To learn more about how to use Superpipe, check out the docs</p>"},{"location":"concepts/","title":"Concepts","text":""},{"location":"concepts/#steps","title":"Steps","text":"<p>At a high level Superpipe works by taking input data and transforming it in steps to get the desired output. Each step takes in an input dataframe or Python dictionary and returns a new dataframe or dictionary with the outputs of the step appended. For example, here's how you'd invoke the built-in <code>LLMStep</code>.</p> <pre><code>from superpipe.steps import LLMStep\n\njoke_prompt = lambda row: f\"\"\"\nTell me a joke about {row['topic']}\n\"\"\"\n\njoke_step = LLMStep(\n  prompt=joke_prompt,\n  model=models.gpt35,\n  name=\"joke\"\n)\njoke_step.run(dataframe)\n</code></pre> Why steps? <p>When doing data transformation, extraction or classification using LLMs, it's crucial to think in steps.</p> <ul> <li> <p>Increased accuracy \u2014</p> </li> <li> <p>Separation of concerns \u2014</p> </li> <li> <p>Interpretability and debugging \u2014</p> </li> <li> <p>Modularity \u2014</p> </li> </ul> <p>Superpipe comes with a handful of built-in steps but it's easy (and recommended) to create your own steps by subclassing <code>CustomStep</code>. This allows you to do pretty much anything inside a step - call a third party api, lookup a DB, etc.</p> <p>For more details see the steps section.</p>"},{"location":"concepts/#pipelines","title":"Pipelines","text":"<p>Steps are chained together to create a pipeline. Similar to a step, a pipeline also takes an input dataframe or dictionary and returns a new dataframe or dictionary with all the outputs of all the steps appended.</p> <pre><code>from superpipe.pipeline import Pipeline\n\nserp_step = ...\ntop3_codes_step = ...\ntop1_code_step = ...\n\npipeline = Pipeline(\n  steps=[\n    serp_step,\n    top3_codes_step,\n    top1_code_step]\n)\n</code></pre> <p>For more details see the pipelines section.</p>"},{"location":"concepts/#evaluation","title":"Evaluation","text":"<p>Pipelines can and should be evaluated to ensure they work well. Without proper evaluation, there's no way to know if the pipeline will perform well in the wild. There are two ways to do this - you can pass in an evaluation function when initializing the pipeline or call the <code>evaluate</code> method after the pipeline has run.</p> <p>You can provide any arbitrary evaluation function, as long as it takes in a <code>row</code> argument and returns a boolean. The simplest type of evaluation is a string comparison, but you can use arbitrarily complex eval functions including LLM calls.</p> <pre><code>evaluator = lambda row: row['code'].lower() == row['code_groundtruth'].lower()\n\n# option 1\npipeline = Pipeline(\n  steps=[\n    serp_step,\n    top3_codes_step,\n    top1_code_step],\n  evaluation_fn=evaluator\n)\n\n# option 2\npipeline.evaluate(evaluator)\n</code></pre>"},{"location":"concepts/#parameters","title":"Parameters","text":"<p>One of the design goals of Superpipe is to make pipelines and steps fully parametric - meaning every aspect of both is exposed as a parameter and can be changed even after the pipeline has been defined and run.</p> <p>For example in the <code>joke_step</code> defined above, the <code>model</code> and <code>prompt</code> that were passed in during initialization are parameters that can be easily update afterwards using the <code>update_params</code> function.</p> <pre><code>from superpipe import models\n\njoke_step.update_params({\n  \"model\": models.gpt4\n})\n</code></pre> <p>The same applies to pipelines as well, but since pipelines themselves don't have parameters, only steps do, the <code>params</code> dict needs to contain the step parameters dict as the values and step names as the keys.</p> <pre><code>pipeline.update_params({\n  \"joke\": {\n    \"model\": models.gpt4\n  }\n})\n</code></pre>"},{"location":"concepts/#optimization","title":"Optimization","text":"<p>Building and evaluating your pipeline is a good start, but you rarely get the best accuracy-cost-speed tradeoff on the first attempt, and there's a lot of low-hanging fruit to optimize. There are two ways to optimize your solution:</p> <ol> <li>Tune the parameters of your pipeline</li> <li>Try a different technique (ie. build a different pipeline)</li> </ol> <p>For 1, Superpipe lets you run a hyperparameter grid search. This means you can try different models, values of K, prompts, etc. on the same pipeline and dataset. Then you can compare the results across accuracy, cost, speed and pick the one that's best for your situation.</p> <p>For 2, we're working on a AI-powered copilot that lets you experiment with different techniques.</p>"},{"location":"concepts/grid_search/","title":"Grid Search","text":"<p>The core of Superpipe is running a grid search to optimize your pipeline. A grid search allows you to perform a hyperparameter search over every permutation of your parameters and retrun a dataframe with cost, accuracy, and latency information.</p> <p><code>GridSearch</code> takes as inputs a pipeline and a dictionary of parameters to search over for each step.</p> <p>In our Comparing Pipelines example, we use a grid search to search over models and number of embedding results.</p> <pre><code>from superpipe import grid_search\n\nparams_grid = {\n    short_description_step.name: {\n        'model': [models.gpt35, models.gpt4],\n    },\n    embedding_search_step.name: {\n        'k': [3, 5, 7],\n    },\n    categorize_step.name: {\n        'model': [models.gpt35, models.gpt4],\n    },\n}\n\n\nsearch_embeddings = grid_search.GridSearch(categorizer, params_grid)\nsearch_embeddings.run(df)\n</code></pre>"},{"location":"concepts/grid_search/#output","title":"Output","text":"short_description__model embedding_search__k categorize__model score input_tokens output_tokens input_cost output_cost num_success num_failure total_latency index 0 gpt-3.5-turbo-0125 3 gpt-3.5-turbo-0125 0.833 {'gpt-3.5-turbo-0125': 11315} {'gpt-3.5-turbo-0125': 2108} 0.005657 0.003162 30 0 103.464159 -7791233023527820859 1 gpt-3.5-turbo-0125 3 gpt-4-turbo-preview 0.933 {'gpt-3.5-turbo-0125': 5852, 'gpt-4-turbo-preview': 5852} {'gpt-3.5-turbo-0125': 1837, 'gpt-4-turbo-preview': 1837} 0.057896 0.011756 30 0 82.123847 -1229872059569985205 2 gpt-3.5-turbo-0125 5 gpt-3.5-turbo-0125 0.9 {'gpt-3.5-turbo-0125': 11824} {'gpt-3.5-turbo-0125': 1998} 0.005912 0.002997 30 0 60.67743 -2156008638839003309 3 gpt-3.5-turbo-0125 5 gpt-4-turbo-preview 0.967 {'gpt-3.5-turbo-0125': 5852, 'gpt-4-turbo-preview': 5852} {'gpt-3.5-turbo-0125': 1792, 'gpt-4-turbo-preview': 1792} 0.063456 0.011688 30 0 85.082716 -373516568509500608 4 gpt-3.5-turbo-0125 7 gpt-3.5-turbo-0125 0.9 {'gpt-3.5-turbo-0125': 12575} {'gpt-3.5-turbo-0125': 2141} 0.006287 0.003211 30 0 149.574122 5513717612912975259 5 gpt-3.5-turbo-0125 7 gpt-4-turbo-preview 0.967 {'gpt-3.5-turbo-0125': 5852, 'gpt-4-turbo-preview': 5852} {'gpt-3.5-turbo-0125': 1733, 'gpt-4-turbo-preview': 1733} 0.069126 0.011599 30 0 78.444735 2766483574959374285 6 gpt-4-turbo-preview 3 gpt-3.5-turbo-0125 0.867 {'gpt-4-turbo-preview': 5852, 'gpt-3.5-turbo-0125': 5852} {'gpt-4-turbo-preview': 1836, 'gpt-3.5-turbo-0125': 1836} 0.061260 0.055532 30 0 138.30416 7602228094953899657 7 gpt-4-turbo-preview 3 gpt-4-turbo-preview 0.867 {'gpt-4-turbo-preview': 11298} {'gpt-4-turbo-preview': 2095} 0.112980 0.062850 30 0 164.999652 -6892174709507839108 8 gpt-4-turbo-preview 5 gpt-3.5-turbo-0125 0.867 {'gpt-4-turbo-preview': 5852, 'gpt-3.5-turbo-0125': 5852} {'gpt-4-turbo-preview': 1803, 'gpt-3.5-turbo-0125': 1803} 0.061548 0.054541 30 0 140.513508 -8924542522527535100 9 gpt-4-turbo-preview 5 gpt-4-turbo-preview 0.967 {'gpt-4-turbo-preview': 11977} {'gpt-4-turbo-preview': 2158} 0.119770 0.064740 30 0 178.206688 -9078237607708088845 10 gpt-4-turbo-preview 7 gpt-3.5-turbo-0125 0.9 {'gpt-4-turbo-preview': 5852, 'gpt-3.5-turbo-0125': 5852} {'gpt-4-turbo-preview': 1806, 'gpt-3.5-turbo-0125': 1806} 0.061864 0.054631 30 0 141.250665 -1609701935912568703 11 gpt-4-turbo-preview 7 gpt-4-turbo-preview 0.967 {'gpt-4-turbo-preview': 12528} {'gpt-4-turbo-preview': 2090} 0.125280 0.062700 30 0 169.717205 -7994583890545252174"},{"location":"concepts/models/","title":"Models","text":"<p>Superpipe conveniently comes with pricing and shorthand for some commonly used models.</p> <pre><code>gpt4 = \"gpt-4-turbo-preview\"\ngpt35 = \"gpt-3.5-turbo-0125\"\nmixtral = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\nmistral = \"mistralai/Mistral-7B-Instruct-v0.1\"\ncodellama = \"togethercomputer/CodeLlama-34b-Instruct\"\nllama270b = \"meta-llama/Llama-2-70b-chat-hf\"\nstripedhyena = \"togethercomputer/StripedHyena-Nous-7B\"\n</code></pre>"},{"location":"concepts/models/#setting-or-changing-the-model-provider","title":"Setting or changing the model provider","text":"<p>You can use any model and model provider with Superpipe as long as it conforms to the OpenAI spec. </p> <p>For example, Together, Anyscale, and OpenRouter all provide Superpipe compatible endpoints for open source models. </p> <p>For non OpenAI you need to provide a <code>baseURL</code> when using the model. You can do this by calling the <code>set_client_for_model</code> function.</p> <p><code>set_client_for_model(model_name, api_key, base_url, pricing)</code></p> <p>The <code>pricing</code> dictionary takes a model name as the key and a tuple for the input and output price per million tokens as values.</p> <pre><code>openai.set_client_for_model(\n    model_name='google/gemma-7b-it:free', \n    api_key=\"OPEN_ROUTER_API_KEY\", \n    base_url=\"https://openrouter.ai/api/v1\", \n    pricing={\n        'google/gemma-7b-it:free': (0, 0)\n    }\n)\n</code></pre>"},{"location":"concepts/pipelines/","title":"Pipelines","text":"<p>Pipelines are the engines that make Superpipe run. A pipeline is a series of steps chained together that acts on a dataframe. A pipeline takes an optional evaluation function that can run arbitrary Python code. Evaluation functions need to return booleans.</p>"},{"location":"concepts/pipelines/#pipeline-statistics","title":"Pipeline statistics","text":"<p>A pipeline object has associated pipeline statistics.</p> Stat Description score Accuracy score of the pipeline as defined by the evaluation function. input_tokens Total number of input tokens used by the pipeline split out by model. output_tokens Total number of output tokens used by the pipeline split out by model. input_cost Total input cost of the pipeline split out by model. output_cost Total output cost of the pipeline split out by model. num_success Number of successful rows. num_failure Number of unsuccessful rows. total_latency Total latency of the pipeline."},{"location":"concepts/pipelines/#pipeline-methods","title":"Pipeline methods","text":""},{"location":"concepts/pipelines/#update_param","title":"update_param()","text":"<p><code>pipeline.update_params()</code> takes a parameters dictionary of steps and parameters. For example, to update the <code>categorize</code> pipeline to use GPT-4, we can call <code>update_param</code> and pass in the step name as the key, with a sub dictionary with model as the key.</p> <pre><code>categorizer.update_params({\n  \"categorize\": {\n    \"model\": models.gpt4\n  }\n})\n</code></pre>"},{"location":"concepts/pipelines/#example","title":"Example","text":"<p>You can find the full code for this example in the comparing pipelines example. This is just the pipeline definition.</p> <pre><code>evaluate = lambda row: row['predicted_category'].lower() == row['category_new'].lower()\n\ncategorizer = pipeline.Pipeline([\n  short_description_step,\n  embedding_search_step,\n  categorize_step,\n  select_category_step\n], evaluation_fn=evaluate)\n\ncategorizer.run(test_df)\n</code></pre>"},{"location":"concepts/steps/","title":"Steps","text":"<p>At a high level Superpipe works by taking input data and transforming it in steps to get the desired output. Each step takes in an input dataframe or Python dictionary and returns a new dataframe or dictionary with the outputs of the step appended.</p>"},{"location":"concepts/steps/#built-in-steps","title":"Built in steps","text":"<p>Superpipe comes with a handful of built-in steps that you can use to build your pipelines.</p> <ul> <li>LLM Step - Standard LLM calls with a single output.</li> <li>Structured LLM Step - LLM calls with structured output.</li> <li>SERP Step - Enrich data with Google Search.</li> <li>Embedding Search Step - Embed strings and search over them.</li> </ul>"},{"location":"concepts/steps/#custom-steps","title":"Custom Steps","text":"<p>It's easy (and recommended) to create your own steps using Custom Step. This allows you to do pretty much anything inside a step - call a third party api, lookup a DB, etc.</p>"},{"location":"concepts/steps/CustomStep/","title":"Custom step","text":"<p>Custom steps allow you to implement arbitrary python in your pipeline.</p> <p>Custom steps require a function, called a <code>transform</code> as well as pydandic model that defines the output schema. Optionally you can provide a name.</p> <p>The logic really can be arbitrary, including API calls, Langchain chains, local generative models or simple data cleaning.</p>"},{"location":"concepts/steps/CustomStep/#example","title":"Example","text":"<p>This is an extremely simple custom step, implemented merely as a lambda function.</p> <pre><code>select_category_step = steps.CustomStep(\n  transform=lambda row: row[f'category{row[\"category_index\"]}'],\n  name=\"select_category\"\n)\n</code></pre>"},{"location":"concepts/steps/CustomStep/#api-call-as-a-custom-step","title":"API call as a custom step","text":"<p>In this example we wrap an API call in a custom step. The result will be three new columns to our dataframe as defined by the pydantic model.</p> <pre><code>import json\nfrom datetime import datetime\nimport requests\nfrom pydantic import BaseModel, Field\n\napi_key = 'API_KEY'\n\ndef get_linkedin_data(profile, api_key=api_key):\n\n    headers = {'Authorization': 'Bearer ' + api_key}\n    api_endpoint = 'https://nubela.co/proxycurl/api/v2/linkedin'\n    params = {\n        'linkedin_profile_url': 'https://www.linkedin.com/in/{profile}/',\n        'extra': 'include',\n        'github_profile_id': 'include',\n        'facebook_profile_id': 'include',\n        'twitter_profile_id': 'include',\n        'personal_contact_number': 'include',\n        'personal_email': 'include',\n        'inferred_salary': 'include',\n        'skills': 'include',\n        'use_cache': 'if-present',\n        'fallback_to_cache': 'on-error',\n    }\n    response = requests.get(api_endpoint,\n                            params=params,\n                            headers=headers)\n\n    # JSON string provided\n    json_str = response.content\n\n    # Load the JSON string into a Python dictionary\n    profile = json.loads(json_str)\n\n    # Extract occupation\n    headline = profile.get('headline', 'Not provided')\n\n    # Extract the most recent job\n    experiences = profile.get('experiences', [])\n    most_recent_job = experiences[0] if experiences else 'Not provided'\n\n    # Calculate years of work experience since college\n    education = profile.get('education', [])\n    college_graduation_year = None\n    for edu in education:\n        if 'Bachelor' in edu.get('degree_name', ''):\n            college_graduation_year = edu.get('ends_at', {}).get('year')\n            break\n\n    years_of_experience = datetime.now().year - college_graduation_year if college_graduation_year else 'Not provided'\n\n    return({'headline': headline, 'most_recent_job': most_recent_job.get('title'), 'years_of_experience': years_of_experience})\n\nlinkedin_step = steps.CustomStep(\n    transform=get_linkedin_data,\n    name='linkedin'\n)\n</code></pre>"},{"location":"concepts/steps/EmbeddingSearchStep/","title":"Embedding Classification Step","text":"<p>The <code>EmbeddingSearchStep</code> is used to embed a list of strings and search over them.</p> <p>The list of strings can either be fixed and provided upon initialization in the <code>candidates</code> argument, or can be generated dynamically from each input row by passing in the <code>candidates_fn</code> argument which returns a list of strings.</p> <p>In either case, the candidates will be embedded and saved to an in memory vector store using <code>faiss</code>. You can provide whatever embedding function you want in the <code>embed_fn</code> argument, for example Cohere, OpenAI, or custom embeddings.</p> <p>The <code>k</code> value determines the number of embeddings returned and is often a parameter you may want to permute in a grid search.</p> <p>For best results, it is best to provide a <code>search_prompt</code> that is dense with identifying information about the category.</p>"},{"location":"concepts/steps/EmbeddingSearchStep/#example","title":"Example","text":"<p>In this example we use Cohere to embed our categories and search over them with a \"short description\" that we generated with a previous step.</p> <pre><code>COHERE_API_KEY = os.environ.get('COHERE_API_KEY')\nco = cohere.Client(COHERE_API_KEY)\n\ndef embed(texts: List[str]):\n  embeddings = co.embed(\n    model=\"embed-english-v3.0\",\n    texts=texts,\n    input_type='classification'\n  ).embeddings\n  return np.array(embeddings).astype('float32')\n\nembedding_search_prompt = lambda row: row[\"short_description\"]\n\nembedding_search_step = steps.EmbeddingSearchStep(\n  search_prompt= embedding_search_prompt,\n  embed=embed,\n  k=5,\n  candidates=taxonomy,\n  name=\"embedding_search\"\n)\n</code></pre> <p>See this example for a full walkthrough.</p>"},{"location":"concepts/steps/LLMStep/","title":"LLM Step","text":"<p>The <code>LLMStep</code> is used to create generative responses.</p> <p>The output of <code>LLMStep</code> is added to the dataframe as a column that can be referenced by the step name. </p>"},{"location":"concepts/steps/LLMStep/#statistics","title":"Statistics","text":"<p><code>LLMStep</code> returns useful statistics about the LLM call for each row. </p> Stat name Description input_tokens Number of input token used. output_tokens Number of output tokens used. input_cost Input cost of running the LLM call. output_cost Output cost of running the LLM call. num_success Number of succesful calls. num_failure Number of unsuccesful calls. total_latency Latency for the LLM call."},{"location":"concepts/steps/LLMStep/#example","title":"Example","text":"<p>In this example, we provide information about a business and three potential codes to choose from and we expect two structured fields in return, <code>reasoning</code> and <code>code</code>.</p> <pre><code>joke_prompt = lambda row: f\"\"\"\nTell me a joke about {row['topic']}\n\"\"\"\n\nJokesStep = steps.LLMStep(\n  prompt=joke_prompt,\n  model=models.gpt35,\n  name=\"joke\"\n)\n</code></pre> <p>When used in a pipeline, this creates a column called \"joke\". </p>"},{"location":"concepts/steps/LLMStructuredStep/","title":"Structured LLM Step","text":"<p>The <code>LLMStructuredStep</code> is used to create structured responses.</p> <p>An <code>LLMStructuredStep</code> instance takes a Pydantic model and a prompt generator function as arguments. The pydantic model specifies the output structure. The prompt generator function defines how to generate a prompt from the input data. You can optionally provide a model and step name. </p>"},{"location":"concepts/steps/LLMStructuredStep/#statistics","title":"Statistics","text":"<p><code>LLMStructuredStep</code> returns useful statistics about the LLM call for each row.  </p> Stat name Description input_tokens Number of input tokens used. output_tokens Number of output tokens used. input_cost Input cost of running the LLM call. output_cost Output cost of running the LLM call. num_success Number of succesful calls. num_failure Number of unsuccesful calls. total_latency Latency for the LLM call."},{"location":"concepts/steps/LLMStructuredStep/#example","title":"Example","text":"<p>In this example, we provide information about a business and three potential codes to choose from and we expect two structured fields in return, <code>reasoning</code> and <code>code</code>.</p> <pre><code>from pydantic import BaseModel, Field\n\ndef business_code_prompt(row): return f\"\"\"\n    You are given a business name and a list of google search results about a company.\n    You are given 3 possible NAICS codes it could be -- pick the best one and explain your reasoning.\n\n    Company name: {row['name']}\n    NAICS options: {row['top3_codes']}\n    Search results:\n    {row['serp']}\n\"\"\"\n\n\nclass BusinessCode(BaseModel):\n    reasoning: str = Field(description=\"The thought process for why this is the best NAICS code\")\n    code: str = Field(description=\"The best NAICS code\")\n\nbusiness_code_step = steps.LLMStructuredStep(\n  model=models.gpt4,\n  prompt=business_code_prompt,\n  out_schema=BusinessCode,\n  name=\"business_code\")\n</code></pre>"},{"location":"concepts/steps/LLMStructuredStep/#supported-models","title":"Supported models","text":"<p><code>LLMStructuredStep</code> currently only works with models that support JSON mode. There may be other models not on this list that also work.</p> model provider gpt4 OpenAI gpt35 OpenAI mixtral Together, Anyscale mistral Together, Anyscale codellama Together <p>See the models page for information on the mapping of model names to models.</p>"},{"location":"concepts/steps/SERPStep/","title":"SERP Step","text":"<p>The <code>SERPStep</code> uses serper.dev to enrich data for your pipeline via Google search. </p> <p>The <code>SERPStep</code> takes a search prompt, name and an optional post processing function as inputs. </p> <p>The post processing function is useful if only some of the search content is relevant for your pipeline. For example, Serper will return \"Knowledge Graph\" content about entities Google knows about that can be very useful. </p>"},{"location":"concepts/steps/SERPStep/#example","title":"Example","text":"<pre><code>def shorten(x):\n    short = []\n    kgi = json.loads(x).get('knowledgeGraph')\n    if kgi is None:\n        short.append(kgi)\n    else:\n        kg = {'title': kgi.get('title'), 'type': kgi.get(\n            'type'), 'description': kgi.get('description')},\n        short.append(kg)\n    y = json.loads(x).get('organic')\n    if y is None:\n        return None\n    for o in y[:3]:\n        short.append({\n            'title': o['title'],\n            'snippet': o['snippet'],\n            'link': o['link']\n        })\n    return short\n\n\ndef serp_prompt(row):\n    name = row['name']\n    street = row['address']['street']\n    city = row['address']['city']\n    state = row['address']['state']\n    zip = row['address']['zip']\n    return f\"Review for {name} located at {street} {city} {state} {str(zip)}\"\n\n\nserp_step = steps.SERPEnrichmentStep(\n  prompt=serp_prompt,\n  postprocess=shorten,\n  name=\"serp\")\n</code></pre>"},{"location":"examples/","title":"Superpipe Examples","text":"<p>The best way to learn how to use Superpipe is to take a look at some of our examples. </p> <p>The examples are constructed to teach new concepts as well as give you ideas for how to solve different types of concepts.</p> <p>Product categorization</p> <p>Business code classification</p> <ul> <li>SERP enrichment</li> <li>Embedding classification</li> <li>Custom step</li> </ul> <p>Comparing different approaches</p> <ul> <li>Grid search</li> <li>Heiarchical classification</li> </ul> <p>Evaluating generative output</p> <ul> <li>Custom eval function</li> </ul>"},{"location":"examples/business_classification/business_classification/","title":"Business code classification","text":"<p>This notebook demonstrates how to use Superpipe to generate North American Industry Classification System (NAICS) codes for businesses based on name and address.</p> <p>We are provided with a list of business names and addresses, for example</p> <pre><code>{\n  \"name\": \"\",\n  \"street\": \"\",\n  \"city\": \"\",\n  \"state\": \"\",\n  \"zip\": {}\n}\n</code></pre> <p>The objective is to accurately assign a NAICS code to each business, which categorizes it into a specific industry. For example, the NAICS code for the above business might be</p> <p><code>311811 - Retail Bakeries</code></p> <p>The challenge is to correctly generate the NAICS code for each business, considering the vast array of industries covered by the NAICS system.</p> <p>Import libraries, and load the data and the taxonomy</p> In\u00a0[1]: Copied! <pre>import pandas as pd\nimport json\nfrom pydantic import BaseModel, Field\nfrom typing import List\nfrom superpipe import *\n\n# df = pd.read_csv('./data.csv')\n</pre> import pandas as pd import json from pydantic import BaseModel, Field from typing import List from superpipe import *  # df = pd.read_csv('./data.csv') <pre>/var/folders/bh/hwln9nhn6tb990j17wgs638r0000gn/T/ipykernel_42665/1849040241.py:1: DeprecationWarning: \nPyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\nbut was not found to be installed on your system.\nIf this would cause problems for you,\nplease provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n        \n  import pandas as pd\n</pre> <p>Now, let's define the first step of the pipeline which uses a Google SERP library to search for the <code>description</code> field on the input object.</p> <p>We do this by using SuperPipe's built-in <code>SERPEnrichmentStep</code>. You could also easily build your own using a <code>CustomStep</code> instead.</p> In\u00a0[2]: Copied! <pre>def shorten(x):\n    short = []\n    kgi = json.loads(x).get('knowledgeGraph')\n    if kgi is None:\n        short.append(kgi)\n    else:\n        kg = {'title': kgi.get('title'), 'type': kgi.get(\n            'type'), 'description': kgi.get('description')},\n        short.append(kg)\n    y = json.loads(x).get('organic')\n    if y is None:\n        return None\n    for o in y[:3]:\n        short.append({\n            'title': o['title'],\n            'snippet': o['snippet'],\n            'link': o['link']\n        })\n    return short\n\n\ndef serp_prompt(row):\n    name = row['name']\n    street = row['address']['street']\n    city = row['address']['city']\n    state = row['address']['state']\n    zip = row['address']['zip']\n    return f\"Review for {name} located at {street} {city} {state} {str(zip)}\"\n\n\nserp_step = steps.SERPEnrichmentStep(\n  prompt=serp_prompt,\n  postprocess=shorten,\n  name=\"serp\")\n</pre> def shorten(x):     short = []     kgi = json.loads(x).get('knowledgeGraph')     if kgi is None:         short.append(kgi)     else:         kg = {'title': kgi.get('title'), 'type': kgi.get(             'type'), 'description': kgi.get('description')},         short.append(kg)     y = json.loads(x).get('organic')     if y is None:         return None     for o in y[:3]:         short.append({             'title': o['title'],             'snippet': o['snippet'],             'link': o['link']         })     return short   def serp_prompt(row):     name = row['name']     street = row['address']['street']     city = row['address']['city']     state = row['address']['state']     zip = row['address']['zip']     return f\"Review for {name} located at {street} {city} {state} {str(zip)}\"   serp_step = steps.SERPEnrichmentStep(   prompt=serp_prompt,   postprocess=shorten,   name=\"serp\") <p>The second step of the pipeline takes the business name and the search results and feeds them into an LLM to get the 3 most likely NAICS codes. We create this step using <code>LLMStructuredStep</code>.</p> <p>An <code>LLMStructuredStep</code> instance takes a Pydantic model and a prompt generator function as arguments. The pydantic model specifies the output structure (remember every <code>LLMStructuredStep</code> creates structured output). The prompt generator function defines how to generate a prompt from the input data.</p> In\u00a0[3]: Copied! <pre>def top3_codes_prompt(row):\n    return f\"\"\"You are given a business name and a list of google search results about a company.\n    Return an array of the top 3 most like NAICS business codes this company falls into. Only use codes in the 2022 taxonomy.\n\n    Company name: {row['name']}\n    Search results:\n    {row['serp']}\n    \"\"\"\n\n\nclass Top3Codes(BaseModel):\n    top3_codes: List[int] = Field(\n        description=\"The top 3 most likely NAICS codes\")\n\n\ntop3_codes_step = steps.LLMStructuredStep(\n  model=models.gpt4,\n  prompt=top3_codes_prompt,\n  out_schema=Top3Codes,\n  name=\"top3_codes\")\n</pre> def top3_codes_prompt(row):     return f\"\"\"You are given a business name and a list of google search results about a company.     Return an array of the top 3 most like NAICS business codes this company falls into. Only use codes in the 2022 taxonomy.      Company name: {row['name']}     Search results:     {row['serp']}     \"\"\"   class Top3Codes(BaseModel):     top3_codes: List[int] = Field(         description=\"The top 3 most likely NAICS codes\")   top3_codes_step = steps.LLMStructuredStep(   model=models.gpt4,   prompt=top3_codes_prompt,   out_schema=Top3Codes,   name=\"top3_codes\") <p>The third step the business name, search results and 3 most likely NAICS codes into an LLM to get the most likely NAICS code and the thinking behind it. Again, we create this step using <code>LLMStructuredStep</code>.</p> In\u00a0[4]: Copied! <pre>def top_code_prompt(row): return f\"\"\"\nYou are given a business name and a list of google search results about a company.\nYou are given 3 possible NAICS codes it could be -- pick the best one and explain your thinking.\n\nCompany name: {row['name']}\nNAICS options: {row['top3_codes']}\nSearch results:\n{row['serp']}\n\"\"\"\n\n\nclass TopCode(BaseModel):\n    result: int = Field(description=\"The best NAICS code\")\n    thinking: str = Field(\n        description=\"The thought process for why this is the best NAICS code\")\n\n\ntop1_code_step = steps.LLMStructuredStep(\n  model=models.gpt4,\n  prompt=top_code_prompt,\n  out_schema=TopCode,\n  name=\"top1_code\")\n</pre> def top_code_prompt(row): return f\"\"\" You are given a business name and a list of google search results about a company. You are given 3 possible NAICS codes it could be -- pick the best one and explain your thinking.  Company name: {row['name']} NAICS options: {row['top3_codes']} Search results: {row['serp']} \"\"\"   class TopCode(BaseModel):     result: int = Field(description=\"The best NAICS code\")     thinking: str = Field(         description=\"The thought process for why this is the best NAICS code\")   top1_code_step = steps.LLMStructuredStep(   model=models.gpt4,   prompt=top_code_prompt,   out_schema=TopCode,   name=\"top1_code\") <p>We're done defining the steps. Finally, we define an evaluation function - a simple string comparison against the ground truth column which was present in the dataset. Then we define a <code>Pipeline</code> and run it.</p> In\u00a0[\u00a0]: Copied! <pre>evaluate = lambda row: row['result'] == row['NAICS']\n\nnaics_coder = pipeline.Pipeline(\n  steps=[\n    serp_step,\n    top3_codes_step,\n    top1_code_step], \n  evaluation_fn=evaluate)\n\nnaics_coder.run(df)\n</pre> evaluate = lambda row: row['result'] == row['NAICS']  naics_coder = pipeline.Pipeline(   steps=[     serp_step,     top3_codes_step,     top1_code_step],    evaluation_fn=evaluate)  naics_coder.run(df)"},{"location":"examples/business_classification/business_classification/#business-code-classification","title":"Business code classification\u00b6","text":"<p>View on Github</p>"},{"location":"examples/business_classification/business_classification/#approach","title":"Approach\u00b6","text":"<p>We'll implement the following multi-step approach:</p> <ol> <li><p>Do a google search with the company's name and address</p> </li> <li><p>Feed the name and the search results from Step 1 into an LLM and ask it for the 3 most likely NAICS codes</p> </li> <li><p>Feed the name, search results and the 3 most likely NAICS codes from Step 2 into an LLM and ask it for the most likely NAICS code along with its reasoning.</p> </li> </ol>"},{"location":"examples/business_classification/business_classification/#defining-the-pipeline-using-superpipe","title":"Defining the Pipeline using SuperPipe\u00b6","text":""},{"location":"examples/comparing_pipelines/furniture/","title":"Comparing different approaches","text":"In\u00a0[1]: Copied! <pre>from dotenv import load_dotenv\nload_dotenv()\n\nimport os\nOPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\nCOHERE_API_KEY = os.getenv('COHERE_API_KEY')\n</pre> from dotenv import load_dotenv load_dotenv()  import os OPENAI_API_KEY = os.getenv('OPENAI_API_KEY') COHERE_API_KEY = os.getenv('COHERE_API_KEY') In\u00a0[2]: Copied! <pre># %pip install cohere\n\nimport pandas as pd\nfrom superpipe import *\nfrom pydantic import BaseModel, Field\nimport cohere\nimport os\nimport numpy as np\nfrom typing import List\n</pre> # %pip install cohere  import pandas as pd from superpipe import * from pydantic import BaseModel, Field import cohere import os import numpy as np from typing import List In\u00a0[3]: Copied! <pre>df = pd.read_csv('./furniture_clean.csv')\n</pre> df = pd.read_csv('./furniture_clean.csv') In\u00a0[4]: Copied! <pre># Remove the 'Furniture &gt; ' from each string in the 'category' column since they all start with Furniture.\ndf['category_new'] = df['category'].str.replace('Furniture &gt; ', '')\n</pre> # Remove the 'Furniture &gt; ' from each string in the 'category' column since they all start with Furniture. df['category_new'] = df['category'].str.replace('Furniture &gt; ', '') <p>For our embeddings approach we want the taxonomy to be a single string. We'll create the taxonomy from the ground truth data.</p> In\u00a0[5]: Copied! <pre>taxonomy = list(set(df['category_new']))\ntaxonomy[0:5]\n</pre> taxonomy = list(set(df['category_new'])) taxonomy[0:5]  Out[5]: <pre>['Outdoor Tables &gt; Outdoor Coffee Tables',\n 'Chairs &gt; Dining Chairs',\n 'Tables &amp; Desks &gt; Bar Carts',\n 'Chairs &gt; Accent Chairs',\n 'Chairs &gt; Desk Chairs']</pre> <p>However, for our heiarchical approach we need to understand the taxonomy a little more so we'll create a lookup table between first and second level categories.</p> In\u00a0[6]: Copied! <pre># Create a lookup table with first level taxonomy as keys and second level as values\nlookup_table = df['category_new'].str.split(' &gt; ', expand=True).groupby(0)[1].apply(list).apply(set)\nlookup_table['Chairs']\n</pre> # Create a lookup table with first level taxonomy as keys and second level as values lookup_table = df['category_new'].str.split(' &gt; ', expand=True).groupby(0)[1].apply(list).apply(set) lookup_table['Chairs'] Out[6]: <pre>{'Accent Chairs', 'Desk Chairs', 'Dining Chairs', 'Recliners'}</pre> In\u00a0[7]: Copied! <pre>short_description_prompt = lambda row: f\"\"\"\nYou are given a product name and description for a piece of furniture.\nReturn a single sentence decribing the product.\nProduct name: {row['name']}\nProduct description: {row['description']}\n\"\"\"\n\nclass ShortDescription(BaseModel):\n  short_description: str = Field(description=\"A single sentence describing the product\")\n  \nshort_description_step = steps.LLMStructuredStep(\n  prompt=short_description_prompt,\n  model=models.gpt35,\n  out_schema=ShortDescription,\n  name=\"short_description\"\n)\n</pre> short_description_prompt = lambda row: f\"\"\" You are given a product name and description for a piece of furniture. Return a single sentence decribing the product. Product name: {row['name']} Product description: {row['description']} \"\"\"  class ShortDescription(BaseModel):   short_description: str = Field(description=\"A single sentence describing the product\")    short_description_step = steps.LLMStructuredStep(   prompt=short_description_prompt,   model=models.gpt35,   out_schema=ShortDescription,   name=\"short_description\" ) <p>We are using Cohere to embed both our description and the taxonomy but you can substitute in any embeddings provider with the <code>EmbeddingSearchStep</code>. Unlike LLMs that are good at ignoring irrelevant information, we've learned from experience that short, simple descriptions work better in embedding space than trying to include too much. This is something you can and should experiment with.</p> In\u00a0[8]: Copied! <pre># set your cohere api key as an env var or set it directly here\nCOHERE_API_KEY = os.environ.get('COHERE_API_KEY')\nco = cohere.Client(COHERE_API_KEY)\n\ndef embed_fn(texts: List[str]):\n  embeddings = co.embed(\n    model=\"embed-english-v3.0\",\n    texts=texts,\n    input_type='classification'\n  ).embeddings\n  return np.array(embeddings).astype('float32')\n\nembedding_search_prompt = lambda row: row[\"short_description\"]\n\nembedding_search_step = steps.EmbeddingSearchStep(\n  search_prompt= embedding_search_prompt,\n  embed_fn=embed_fn,\n  k=5,    \n  candidates=taxonomy,\n  name=\"embedding_search\"\n)\n</pre> # set your cohere api key as an env var or set it directly here COHERE_API_KEY = os.environ.get('COHERE_API_KEY') co = cohere.Client(COHERE_API_KEY)  def embed_fn(texts: List[str]):   embeddings = co.embed(     model=\"embed-english-v3.0\",     texts=texts,     input_type='classification'   ).embeddings   return np.array(embeddings).astype('float32')  embedding_search_prompt = lambda row: row[\"short_description\"]  embedding_search_step = steps.EmbeddingSearchStep(   search_prompt= embedding_search_prompt,   embed_fn=embed_fn,   k=5,       candidates=taxonomy,   name=\"embedding_search\" ) <p>We now take the result of the embeddings and ask the LLM to pick the best response. It's important that our embedding search is optimized for recall because if the correct answer doesn't exist in the response our categorize step will have no chance of succeeding.</p> In\u00a0[9]: Copied! <pre>def categorize_prompt(row):\n    categories = \"\"\n    i = 1\n    while f\"candidate{i}\" in row:\n        categories += f'{i}. {row[\"embedding_search\"][f\"candidate{i}\"]}\\n'\n        i += 1\n\n    return f\"\"\"\n    You are given a product description and {i-1} options for the product's category.\n    Pick the index of the most accurate category.\n    The index must be between 1 and {i-1}.\n    Product description: {row['short_description']}\n    Categories:\n    {categories}\n    \"\"\"\n    \nclass CategoryIndex(BaseModel):\n    category_index: int = Field(description=\"The index of the most accurate category\")\n    \ncategorize_step = steps.LLMStructuredStep(\n  prompt=categorize_prompt,\n  model=models.gpt35,\n  out_schema=CategoryIndex,\n  name=\"categorize\"\n)\n</pre> def categorize_prompt(row):     categories = \"\"     i = 1     while f\"candidate{i}\" in row:         categories += f'{i}. {row[\"embedding_search\"][f\"candidate{i}\"]}\\n'         i += 1      return f\"\"\"     You are given a product description and {i-1} options for the product's category.     Pick the index of the most accurate category.     The index must be between 1 and {i-1}.     Product description: {row['short_description']}     Categories:     {categories}     \"\"\"      class CategoryIndex(BaseModel):     category_index: int = Field(description=\"The index of the most accurate category\")      categorize_step = steps.LLMStructuredStep(   prompt=categorize_prompt,   model=models.gpt35,   out_schema=CategoryIndex,   name=\"categorize\" ) <p>By returning just the index we can ensure that the actual string we use is in the taxonomy since LLMs sometimes hallucinate characters. Additionally, we don't need to waste response tokens on printing the entire string.</p> In\u00a0[10]: Copied! <pre>predicated_category_step = steps.CustomStep(\n  transform=lambda row: row[\"embedding_search\"][f'candidate{row[\"category_index\"]}'],\n  name=\"predicted_category\"\n)\n</pre> predicated_category_step = steps.CustomStep(   transform=lambda row: row[\"embedding_search\"][f'candidate{row[\"category_index\"]}'],   name=\"predicted_category\" ) <p>We'd like to test our end to end pipeline to make sure it works before we go any further. We'll make a copy of the first five rows of the dataframe and run the pipeline to make sure it works</p> In\u00a0[11]: Copied! <pre>test_df = df.head(5).copy()\n</pre> test_df = df.head(5).copy() In\u00a0[12]: Copied! <pre>evaluate = lambda row: row['predicted_category'].lower() == row['category_new'].lower()\n\ncategorizer = pipeline.Pipeline([\n  short_description_step, \n  embedding_search_step, \n  categorize_step,\n  predicated_category_step\n], evaluation_fn=evaluate)\n\ncategorizer.run(test_df)\n</pre> evaluate = lambda row: row['predicted_category'].lower() == row['category_new'].lower()  categorizer = pipeline.Pipeline([   short_description_step,    embedding_search_step,    categorize_step,   predicated_category_step ], evaluation_fn=evaluate)  categorizer.run(test_df) <pre>Applying step short_description:   0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>Applying step short_description: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:08&lt;00:00,  1.60s/it]\nApplying step embedding_search: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00&lt;00:00,  7.53it/s]\nApplying step categorize: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:02&lt;00:00,  1.72it/s]\nApplying step predicted_category: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00&lt;00:00, 9271.23it/s]\n</pre> Out[12]: name description category brand.name category_new __short_description__ short_description category1 category2 category3 category4 category5 __categorize__ category_index predicted_category 0 EnGauge Deluxe Bedframe Introducing the Engauge Deluxe Bedframe - the ... Furniture &gt; Beds &amp; Headboards &gt; Bedframes NaN Beds &amp; Headboards &gt; Bedframes {'input_tokens': 313, 'output_tokens': 62, 'in... Introducing the EnGauge Deluxe Bedframe - a st... Beds &amp; Headboards &gt; Bedframes Beds &amp; Headboards &gt; Beds Beds &amp; Headboards &gt; Headboards Mattresses &amp; Box Springs &gt; Mattresses Mattresses &amp; Box Springs &gt; Box Springs &amp; Found... {'input_tokens': 208, 'output_tokens': 10, 'in... 1 Beds &amp; Headboards &gt; Bedframes 1 Sparrow &amp; Wren Sullivan King Channel-Stitched ... 85\"L x 83\"W x 56\"H | Total weight: 150 lbs. | ... Furniture &gt; Beds &amp; Headboards &gt; Beds Sparrow &amp; Wren Beds &amp; Headboards &gt; Beds {'input_tokens': 169, 'output_tokens': 68, 'in... Handcrafted Sparrow &amp; Wren Sullivan King Chann... Beds &amp; Headboards &gt; Headboards Beds &amp; Headboards &gt; Beds Beds &amp; Headboards &gt; Bedframes Kids Beds &amp; Headboards &gt; Kid's Beds Mattresses &amp; Box Springs &gt; Mattresses {'input_tokens': 213, 'output_tokens': 10, 'in... 2 Beds &amp; Headboards &gt; Beds 2 Queen Bed With Frame Dimensions:Head Board -49H x 63.75W x 1.5DFoot... Furniture &gt; Beds &amp; Headboards &gt; Beds Hillsdale Beds &amp; Headboards &gt; Beds {'input_tokens': 124, 'output_tokens': 60, 'in... The Queen Bed With Frame features a stylish de... Beds &amp; Headboards &gt; Bedframes Beds &amp; Headboards &gt; Headboards Beds &amp; Headboards &gt; Beds Kids Beds &amp; Headboards &gt; Kid's Beds Sets &gt; Bedroom Furniture Sets {'input_tokens': 202, 'output_tokens': 10, 'in... 1 Beds &amp; Headboards &gt; Bedframes 3 Dylan Queen Bed Add a touch of a modern farmhouse to your bedr... Furniture &gt; Beds &amp; Headboards &gt; Beds NaN Beds &amp; Headboards &gt; Beds {'input_tokens': 140, 'output_tokens': 49, 'in... Add a touch of modern farmhouse to your bedroo... Beds &amp; Headboards &gt; Headboards Beds &amp; Headboards &gt; Beds Beds &amp; Headboards &gt; Bedframes Sets &gt; Bedroom Furniture Sets Kids Beds &amp; Headboards &gt; Kid's Beds {'input_tokens': 191, 'output_tokens': 10, 'in... 2 Beds &amp; Headboards &gt; Beds 4 Sparrow &amp; Wren Mara Full Diamond-Tufted Bed 78\"L x 56\"W x 51\"H | Total weight: 130 lbs. | ... Furniture &gt; Beds &amp; Headboards &gt; Beds Sparrow &amp; Wren Beds &amp; Headboards &gt; Beds {'input_tokens': 168, 'output_tokens': 97, 'in... The Sparrow &amp; Wren Mara Full Diamond-Tufted Be... Beds &amp; Headboards &gt; Headboards Beds &amp; Headboards &gt; Beds Beds &amp; Headboards &gt; Bedframes Kids Beds &amp; Headboards &gt; Kid's Beds Sets &gt; Bedroom Furniture Sets {'input_tokens': 236, 'output_tokens': 10, 'in... 2 Beds &amp; Headboards &gt; Beds <p>Let's print our pipeline statistics and see how it's doing</p> In\u00a0[13]: Copied! <pre>print(categorizer.statistics)\n</pre> print(categorizer.statistics) <pre>+---------------+------------------------------+\n|     score     |             0.8              |\n+---------------+------------------------------+\n|  input_tokens | {'gpt-3.5-turbo-0125': 1964} |\n+---------------+------------------------------+\n| output_tokens | {'gpt-3.5-turbo-0125': 386}  |\n+---------------+------------------------------+\n|   input_cost  |    $0.0009819999999999998    |\n+---------------+------------------------------+\n|  output_cost  |          $0.000579           |\n+---------------+------------------------------+\n|  num_success  |              5               |\n+---------------+------------------------------+\n|  num_failure  |              0               |\n+---------------+------------------------------+\n| total_latency |      10.87322429305641       |\n+---------------+------------------------------+\n</pre> <p>Our pipeline is doing well but that's only on 5 data points. Let's try it on a few more.</p> In\u00a0[14]: Copied! <pre>test_df100 = df.head(100).copy()\ncategorizer.run(test_df100)\n</pre> test_df100 = df.head(100).copy() categorizer.run(test_df100) <pre>Applying step short_description: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [02:10&lt;00:00,  1.31s/it]\nApplying step embedding_search: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:14&lt;00:00,  7.09it/s]\nApplying step categorize: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:50&lt;00:00,  2.00it/s]\nApplying step predicted_category: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:00&lt;00:00, 25426.19it/s]\n</pre> Out[14]: name description category brand.name category_new __short_description__ short_description category1 category2 category3 category4 category5 __categorize__ category_index predicted_category 0 EnGauge Deluxe Bedframe Introducing the Engauge Deluxe Bedframe - the ... Furniture &gt; Beds &amp; Headboards &gt; Bedframes NaN Beds &amp; Headboards &gt; Bedframes {'input_tokens': 313, 'output_tokens': 52, 'in... Introducing the EnGauge Deluxe Bedframe, a stu... Beds &amp; Headboards &gt; Bedframes Beds &amp; Headboards &gt; Beds Beds &amp; Headboards &gt; Headboards Mattresses &amp; Box Springs &gt; Mattresses Sets &gt; Bedroom Furniture Sets {'input_tokens': 193, 'output_tokens': 10, 'in... 1 Beds &amp; Headboards &gt; Bedframes 1 Sparrow &amp; Wren Sullivan King Channel-Stitched ... 85\"L x 83\"W x 56\"H | Total weight: 150 lbs. | ... Furniture &gt; Beds &amp; Headboards &gt; Beds Sparrow &amp; Wren Beds &amp; Headboards &gt; Beds {'input_tokens': 169, 'output_tokens': 63, 'in... The Sparrow &amp; Wren Sullivan King Channel-Stitc... Beds &amp; Headboards &gt; Headboards Beds &amp; Headboards &gt; Beds Beds &amp; Headboards &gt; Bedframes Kids Beds &amp; Headboards &gt; Kid's Beds Mattresses &amp; Box Springs &gt; Mattresses {'input_tokens': 207, 'output_tokens': 10, 'in... 2 Beds &amp; Headboards &gt; Beds 2 Queen Bed With Frame Dimensions:Head Board -49H x 63.75W x 1.5DFoot... Furniture &gt; Beds &amp; Headboards &gt; Beds Hillsdale Beds &amp; Headboards &gt; Beds {'input_tokens': 124, 'output_tokens': 55, 'in... Queen Bed With Frame featuring a Head Board me... Beds &amp; Headboards &gt; Bedframes Beds &amp; Headboards &gt; Headboards Beds &amp; Headboards &gt; Beds Kids Beds &amp; Headboards &gt; Kid's Beds Sets &gt; Bedroom Furniture Sets {'input_tokens': 197, 'output_tokens': 10, 'in... 3 Beds &amp; Headboards &gt; Beds 3 Dylan Queen Bed Add a touch of a modern farmhouse to your bedr... Furniture &gt; Beds &amp; Headboards &gt; Beds NaN Beds &amp; Headboards &gt; Beds {'input_tokens': 140, 'output_tokens': 40, 'in... Add a touch of modern farmhouse charm to your ... Beds &amp; Headboards &gt; Headboards Beds &amp; Headboards &gt; Beds Beds &amp; Headboards &gt; Bedframes Sets &gt; Bedroom Furniture Sets Kids Beds &amp; Headboards &gt; Kid's Beds {'input_tokens': 182, 'output_tokens': 10, 'in... 2 Beds &amp; Headboards &gt; Beds 4 Sparrow &amp; Wren Mara Full Diamond-Tufted Bed 78\"L x 56\"W x 51\"H | Total weight: 130 lbs. | ... Furniture &gt; Beds &amp; Headboards &gt; Beds Sparrow &amp; Wren Beds &amp; Headboards &gt; Beds {'input_tokens': 168, 'output_tokens': 54, 'in... The Sparrow &amp; Wren Mara Full Diamond-Tufted Be... Beds &amp; Headboards &gt; Headboards Beds &amp; Headboards &gt; Beds Beds &amp; Headboards &gt; Bedframes Mattresses &amp; Box Springs &gt; Mattresses Sets &gt; Bedroom Furniture Sets {'input_tokens': 194, 'output_tokens': 10, 'in... 2 Beds &amp; Headboards &gt; Beds ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 95 Modway Melanie Tufted Button Upholstered Fabri... Twin | Clean lines, a straightforward profile,... Furniture &gt; Beds &amp; Headboards &gt; Beds Modway Beds &amp; Headboards &gt; Beds {'input_tokens': 225, 'output_tokens': 77, 'in... The Modway Melanie Tufted Button Upholstered F... Beds &amp; Headboards &gt; Headboards Beds &amp; Headboards &gt; Beds Mattresses &amp; Box Springs &gt; Mattresses Beds &amp; Headboards &gt; Bedframes Sets &gt; Bedroom Furniture Sets {'input_tokens': 218, 'output_tokens': 10, 'in... 2 Beds &amp; Headboards &gt; Beds 96 Concord Queen Panel Bed Looking for a new bed that has it all? Check o... Furniture &gt; Beds &amp; Headboards &gt; Beds Daniel's Amish Beds &amp; Headboards &gt; Beds {'input_tokens': 205, 'output_tokens': 55, 'in... The Concord Queen Panel Bed is a contemporary ... Beds &amp; Headboards &gt; Headboards Beds &amp; Headboards &gt; Beds Beds &amp; Headboards &gt; Bedframes Kids Beds &amp; Headboards &gt; Kid's Beds Sets &gt; Bedroom Furniture Sets {'input_tokens': 197, 'output_tokens': 11, 'in... 2 Beds &amp; Headboards &gt; Beds 97 Sparrow &amp; Wren Myers King Bed Dimensions: 85\"L x 82\"W x 56\"H | Headboard hei... Furniture &gt; Beds &amp; Headboards &gt; Beds Sparrow &amp; Wren Beds &amp; Headboards &gt; Beds {'input_tokens': 271, 'output_tokens': 64, 'in... The Sparrow &amp; Wren Myers King Bed is a luxurio... Beds &amp; Headboards &gt; Headboards Beds &amp; Headboards &gt; Beds Beds &amp; Headboards &gt; Bedframes Kids Beds &amp; Headboards &gt; Kid's Beds Mattresses &amp; Box Springs &gt; Mattresses {'input_tokens': 209, 'output_tokens': 10, 'in... 2 Beds &amp; Headboards &gt; Beds 98 Loden Beige 3 Pc Queen Upholstered Bed with 2 ... A classic design and sophisticated silhouette ... Furniture &gt; Beds &amp; Headboards &gt; Beds Rooms To Go Beds &amp; Headboards &gt; Beds {'input_tokens': 181, 'output_tokens': 56, 'in... The Loden Beige 3 Pc Queen Upholstered Bed wit... Beds &amp; Headboards &gt; Headboards Beds &amp; Headboards &gt; Beds Storage &gt; Dressers Storage &gt; Nightstands Beds &amp; Headboards &gt; Bedframes {'input_tokens': 192, 'output_tokens': 10, 'in... 2 Beds &amp; Headboards &gt; Beds 99 Hempstead Captain Bed in Graystone by A-America Hempstead Captain Bed Furniture &gt; Beds &amp; Headboards &gt; Beds A-America Beds &amp; Headboards &gt; Beds {'input_tokens': 97, 'output_tokens': 33, 'inp... The Hempstead Captain Bed in Graystone by A-Am... Beds &amp; Headboards &gt; Headboards Beds &amp; Headboards &gt; Beds Beds &amp; Headboards &gt; Bedframes Sets &gt; Bedroom Furniture Sets Kids Beds &amp; Headboards &gt; Kid's Beds {'input_tokens': 175, 'output_tokens': 10, 'in... 2 Beds &amp; Headboards &gt; Beds <p>100 rows \u00d7 15 columns</p> In\u00a0[15]: Copied! <pre>print(categorizer.statistics)\n</pre> print(categorizer.statistics) <pre>+---------------+-------------------------------+\n|     score     |              0.91             |\n+---------------+-------------------------------+\n|  input_tokens | {'gpt-3.5-turbo-0125': 39747} |\n+---------------+-------------------------------+\n| output_tokens |  {'gpt-3.5-turbo-0125': 6728} |\n+---------------+-------------------------------+\n|   input_cost  |     $0.019873500000000002     |\n+---------------+-------------------------------+\n|  output_cost  |     $0.010092000000000002     |\n+---------------+-------------------------------+\n|  num_success  |              100              |\n+---------------+-------------------------------+\n|  num_failure  |               0               |\n+---------------+-------------------------------+\n| total_latency |       191.24415755318478      |\n+---------------+-------------------------------+\n</pre> <p>At current gpt-3.5-turbo pricing this batch of 100 requests cost $0.030291 and took five minutes and a half minutes to run for 90% accuracy. Let's see how hierarchical prompting does.</p> <p>In our first step we're just asking the model to pick the right top level category. This is a relatively easy task if the categories are non-overlapping or can be very difficult if there are multiple correct answers. We'll only know by trying and inspecting our losses.</p> In\u00a0[16]: Copied! <pre>first_level_categories = list(lookup_table.keys())\n\ndef first_level_category_prompt(row):\n    i = len(first_level_categories)\n\n    return f\"\"\"\n    You are given a product name, description and {i} options for the product's top level category.\n    Pick the index of the most accurate category.\n    The index must be between 1 and {i}.\n    Product description: {row['description']}\n    Product name: {row['name']}\n    Categories:\n    {first_level_categories}\n    \"\"\"\n    \nclass FirstLevelCategoryIndex(BaseModel):\n    first_category_index: int = Field(description=\"The index of the most accurate first level category\")\n    \nfirst_level_category_step = steps.LLMStructuredStep(\n  prompt=first_level_category_prompt,\n  model=models.gpt35,\n  out_schema=FirstLevelCategoryIndex,\n  name=\"first_categorize\"\n)\n</pre> first_level_categories = list(lookup_table.keys())  def first_level_category_prompt(row):     i = len(first_level_categories)      return f\"\"\"     You are given a product name, description and {i} options for the product's top level category.     Pick the index of the most accurate category.     The index must be between 1 and {i}.     Product description: {row['description']}     Product name: {row['name']}     Categories:     {first_level_categories}     \"\"\"      class FirstLevelCategoryIndex(BaseModel):     first_category_index: int = Field(description=\"The index of the most accurate first level category\")      first_level_category_step = steps.LLMStructuredStep(   prompt=first_level_category_prompt,   model=models.gpt35,   out_schema=FirstLevelCategoryIndex,   name=\"first_categorize\" ) In\u00a0[17]: Copied! <pre>select_first_category_step = steps.CustomStep(\n  transform=lambda row: first_level_categories[row[\"first_category_index\"] - 1],\n  name=\"predicted_first_category\"\n)\n</pre> select_first_category_step = steps.CustomStep(   transform=lambda row: first_level_categories[row[\"first_category_index\"] - 1],   name=\"predicted_first_category\" ) <p>Next we'll give the second layer of the taxonomy to the model to classify. Just as before are trying to predict the index to make sure our final output is valid.</p> In\u00a0[18]: Copied! <pre>def second_level_category_prompt(row):\n    second_level_categories = list(lookup_table[row['predicted_first_category']])\n    i = len(second_level_categories)\n\n    return f\"\"\"\n    You are given a product name, description, first level category \n    and {i} options for the product's second level category.\n    Pick the index of the most accurate category.\n    The index must be between 1 and {i}.\n    Product description: {row['description']}\n    Product name: {row['name']}\n    First level category: {row['predicted_first_category']}\n    Categories:\n    {second_level_categories}\n    \"\"\"\n    \nclass SecondLevelCategoryIndex(BaseModel):\n    second_category_index: int = Field(description=\"The index of the most accurate second level category\")\n    \nsecond_level_category_step = steps.LLMStructuredStep(\n  prompt=second_level_category_prompt,\n  model=models.gpt35,\n  out_schema=SecondLevelCategoryIndex,\n  name=\"second_categorize\"\n)\n</pre> def second_level_category_prompt(row):     second_level_categories = list(lookup_table[row['predicted_first_category']])     i = len(second_level_categories)      return f\"\"\"     You are given a product name, description, first level category      and {i} options for the product's second level category.     Pick the index of the most accurate category.     The index must be between 1 and {i}.     Product description: {row['description']}     Product name: {row['name']}     First level category: {row['predicted_first_category']}     Categories:     {second_level_categories}     \"\"\"      class SecondLevelCategoryIndex(BaseModel):     second_category_index: int = Field(description=\"The index of the most accurate second level category\")      second_level_category_step = steps.LLMStructuredStep(   prompt=second_level_category_prompt,   model=models.gpt35,   out_schema=SecondLevelCategoryIndex,   name=\"second_categorize\" ) In\u00a0[19]: Copied! <pre>select_second_category_step = steps.CustomStep(\n  transform=lambda row: list(lookup_table[row['predicted_first_category']])[row[\"second_category_index\"] - 1],\n  name=\"predicted_second_category\"\n)\n</pre> select_second_category_step = steps.CustomStep(   transform=lambda row: list(lookup_table[row['predicted_first_category']])[row[\"second_category_index\"] - 1],   name=\"predicted_second_category\" ) <p>Let's combine our results so we can properly compare to our ground truth column.</p> In\u00a0[20]: Copied! <pre>combine_taxonomy_step = steps.CustomStep(\n    transform=lambda row: f\"{row['predicted_first_category']} &gt; {row['predicted_second_category']}\",\n    name='combine_taxonomy'\n)\n</pre> combine_taxonomy_step = steps.CustomStep(     transform=lambda row: f\"{row['predicted_first_category']} &gt; {row['predicted_second_category']}\",     name='combine_taxonomy' ) In\u00a0[22]: Copied! <pre>test_df2 = df.head(5).copy()\n\nevaluate2 = lambda row: row['combine_taxonomy'].lower() == row['category_new'].lower()\n\ncategorizer_llm = pipeline.Pipeline([\n  first_level_category_step, \n  select_first_category_step,\n  second_level_category_step,\n  select_second_category_step,\n  combine_taxonomy_step\n], evaluation_fn=evaluate2)\n\ncategorizer_llm.run(test_df2)\n</pre> test_df2 = df.head(5).copy()  evaluate2 = lambda row: row['combine_taxonomy'].lower() == row['category_new'].lower()  categorizer_llm = pipeline.Pipeline([   first_level_category_step,    select_first_category_step,   second_level_category_step,   select_second_category_step,   combine_taxonomy_step ], evaluation_fn=evaluate2)  categorizer_llm.run(test_df2) <pre>Applying step first_categorize: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:02&lt;00:00,  2.11it/s]\nApplying step predicted_first_category: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00&lt;00:00, 6659.74it/s]\nApplying step second_categorize: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:02&lt;00:00,  2.03it/s]\nApplying step predicted_second_category: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00&lt;00:00, 6288.31it/s]\nApplying step combine_taxonomy: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00&lt;00:00, 5734.62it/s]\n</pre> Out[22]: name description category brand.name category_new __first_categorize__ first_category_index predicted_first_category __second_categorize__ second_category_index predicted_second_category combine_taxonomy 0 EnGauge Deluxe Bedframe Introducing the Engauge Deluxe Bedframe - the ... Furniture &gt; Beds &amp; Headboards &gt; Bedframes NaN Beds &amp; Headboards &gt; Bedframes {'input_tokens': 419, 'output_tokens': 11, 'in... 1 Beds &amp; Headboards {'input_tokens': 372, 'output_tokens': 11, 'in... 2 Bedframes Beds &amp; Headboards &gt; Bedframes 1 Sparrow &amp; Wren Sullivan King Channel-Stitched ... 85\"L x 83\"W x 56\"H | Total weight: 150 lbs. | ... Furniture &gt; Beds &amp; Headboards &gt; Beds Sparrow &amp; Wren Beds &amp; Headboards &gt; Beds {'input_tokens': 275, 'output_tokens': 11, 'in... 1 Beds &amp; Headboards {'input_tokens': 228, 'output_tokens': 11, 'in... 3 Headboards Beds &amp; Headboards &gt; Headboards 2 Queen Bed With Frame Dimensions:Head Board -49H x 63.75W x 1.5DFoot... Furniture &gt; Beds &amp; Headboards &gt; Beds Hillsdale Beds &amp; Headboards &gt; Beds {'input_tokens': 230, 'output_tokens': 11, 'in... 1 Beds &amp; Headboards {'input_tokens': 183, 'output_tokens': 11, 'in... 1 Beds Beds &amp; Headboards &gt; Beds 3 Dylan Queen Bed Add a touch of a modern farmhouse to your bedr... Furniture &gt; Beds &amp; Headboards &gt; Beds NaN Beds &amp; Headboards &gt; Beds {'input_tokens': 246, 'output_tokens': 11, 'in... 1 Beds &amp; Headboards {'input_tokens': 199, 'output_tokens': 11, 'in... 1 Beds Beds &amp; Headboards &gt; Beds 4 Sparrow &amp; Wren Mara Full Diamond-Tufted Bed 78\"L x 56\"W x 51\"H | Total weight: 130 lbs. | ... Furniture &gt; Beds &amp; Headboards &gt; Beds Sparrow &amp; Wren Beds &amp; Headboards &gt; Beds {'input_tokens': 274, 'output_tokens': 11, 'in... 1 Beds &amp; Headboards {'input_tokens': 227, 'output_tokens': 11, 'in... 1 Beds Beds &amp; Headboards &gt; Beds In\u00a0[23]: Copied! <pre>print(categorizer_llm.statistics)\n</pre> print(categorizer_llm.statistics) <pre>+---------------+------------------------------+\n|     score     |             0.8              |\n+---------------+------------------------------+\n|  input_tokens | {'gpt-3.5-turbo-0125': 5306} |\n+---------------+------------------------------+\n| output_tokens | {'gpt-3.5-turbo-0125': 221}  |\n+---------------+------------------------------+\n|   input_cost  |          $0.002653           |\n+---------------+------------------------------+\n|  output_cost  |          $0.0003315          |\n+---------------+------------------------------+\n|  num_success  |              5               |\n+---------------+------------------------------+\n|  num_failure  |              0               |\n+---------------+------------------------------+\n| total_latency |      10.390514832979534      |\n+---------------+------------------------------+\n</pre> <p>It works, let's run it on some more data like we did before.</p> In\u00a0[24]: Copied! <pre>test_df2_100 = df.head(100).copy()\ncategorizer_llm.run(test_df2_100)\n</pre> test_df2_100 = df.head(100).copy() categorizer_llm.run(test_df2_100) <pre>Applying step first_categorize: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:58&lt;00:00,  1.71it/s]\nApplying step predicted_first_category: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:00&lt;00:00, 22609.58it/s]\nApplying step second_categorize: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [03:06&lt;00:00,  1.86s/it]\nApplying step predicted_second_category: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:00&lt;00:00, 27186.31it/s]\nApplying step combine_taxonomy: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:00&lt;00:00, 32531.64it/s]\n</pre> Out[24]: name description category brand.name category_new __first_categorize__ first_category_index predicted_first_category __second_categorize__ second_category_index predicted_second_category combine_taxonomy 0 EnGauge Deluxe Bedframe Introducing the Engauge Deluxe Bedframe - the ... Furniture &gt; Beds &amp; Headboards &gt; Bedframes NaN Beds &amp; Headboards &gt; Bedframes {'input_tokens': 419, 'output_tokens': 11, 'in... 1 Beds &amp; Headboards {'input_tokens': 372, 'output_tokens': 11, 'in... 2 Bedframes Beds &amp; Headboards &gt; Bedframes 1 Sparrow &amp; Wren Sullivan King Channel-Stitched ... 85\"L x 83\"W x 56\"H | Total weight: 150 lbs. | ... Furniture &gt; Beds &amp; Headboards &gt; Beds Sparrow &amp; Wren Beds &amp; Headboards &gt; Beds {'input_tokens': 275, 'output_tokens': 11, 'in... 1 Beds &amp; Headboards {'input_tokens': 228, 'output_tokens': 11, 'in... 3 Headboards Beds &amp; Headboards &gt; Headboards 2 Queen Bed With Frame Dimensions:Head Board -49H x 63.75W x 1.5DFoot... Furniture &gt; Beds &amp; Headboards &gt; Beds Hillsdale Beds &amp; Headboards &gt; Beds {'input_tokens': 230, 'output_tokens': 11, 'in... 1 Beds &amp; Headboards {'input_tokens': 183, 'output_tokens': 11, 'in... 3 Headboards Beds &amp; Headboards &gt; Headboards 3 Dylan Queen Bed Add a touch of a modern farmhouse to your bedr... Furniture &gt; Beds &amp; Headboards &gt; Beds NaN Beds &amp; Headboards &gt; Beds {'input_tokens': 246, 'output_tokens': 11, 'in... 1 Beds &amp; Headboards {'input_tokens': 199, 'output_tokens': 11, 'in... 1 Beds Beds &amp; Headboards &gt; Beds 4 Sparrow &amp; Wren Mara Full Diamond-Tufted Bed 78\"L x 56\"W x 51\"H | Total weight: 130 lbs. | ... Furniture &gt; Beds &amp; Headboards &gt; Beds Sparrow &amp; Wren Beds &amp; Headboards &gt; Beds {'input_tokens': 274, 'output_tokens': 11, 'in... 1 Beds &amp; Headboards {'input_tokens': 227, 'output_tokens': 11, 'in... 1 Beds Beds &amp; Headboards &gt; Beds ... ... ... ... ... ... ... ... ... ... ... ... ... 95 Modway Melanie Tufted Button Upholstered Fabri... Twin | Clean lines, a straightforward profile,... Furniture &gt; Beds &amp; Headboards &gt; Beds Modway Beds &amp; Headboards &gt; Beds {'input_tokens': 331, 'output_tokens': 11, 'in... 1 Beds &amp; Headboards {'input_tokens': 284, 'output_tokens': 11, 'in... 1 Beds Beds &amp; Headboards &gt; Beds 96 Concord Queen Panel Bed Looking for a new bed that has it all? Check o... Furniture &gt; Beds &amp; Headboards &gt; Beds Daniel's Amish Beds &amp; Headboards &gt; Beds {'input_tokens': 311, 'output_tokens': 11, 'in... 1 Beds &amp; Headboards {'input_tokens': 264, 'output_tokens': 11, 'in... 1 Beds Beds &amp; Headboards &gt; Beds 97 Sparrow &amp; Wren Myers King Bed Dimensions: 85\"L x 82\"W x 56\"H | Headboard hei... Furniture &gt; Beds &amp; Headboards &gt; Beds Sparrow &amp; Wren Beds &amp; Headboards &gt; Beds {'input_tokens': 377, 'output_tokens': 11, 'in... 1 Beds &amp; Headboards {'input_tokens': 330, 'output_tokens': 11, 'in... 3 Headboards Beds &amp; Headboards &gt; Headboards 98 Loden Beige 3 Pc Queen Upholstered Bed with 2 ... A classic design and sophisticated silhouette ... Furniture &gt; Beds &amp; Headboards &gt; Beds Rooms To Go Beds &amp; Headboards &gt; Beds {'input_tokens': 287, 'output_tokens': 11, 'in... 1 Beds &amp; Headboards {'input_tokens': 240, 'output_tokens': 11, 'in... 1 Beds Beds &amp; Headboards &gt; Beds 99 Hempstead Captain Bed in Graystone by A-America Hempstead Captain Bed Furniture &gt; Beds &amp; Headboards &gt; Beds A-America Beds &amp; Headboards &gt; Beds {'input_tokens': 203, 'output_tokens': 11, 'in... 1 Beds &amp; Headboards {'input_tokens': 156, 'output_tokens': 11, 'in... 1 Beds Beds &amp; Headboards &gt; Beds <p>100 rows \u00d7 12 columns</p> <p>Let's compare approach 1 to approach 2.</p> In\u00a0[25]: Copied! <pre>print(categorizer.statistics)\nprint(f\"Total cost: ${categorizer.statistics.input_cost + categorizer.statistics.output_cost}\")\nprint(categorizer_llm.statistics)\nprint(f\"Total cost: ${categorizer_llm.statistics.input_cost + categorizer_llm.statistics.output_cost}\")\n</pre> print(categorizer.statistics) print(f\"Total cost: ${categorizer.statistics.input_cost + categorizer.statistics.output_cost}\") print(categorizer_llm.statistics) print(f\"Total cost: ${categorizer_llm.statistics.input_cost + categorizer_llm.statistics.output_cost}\")   <pre>+---------------+-------------------------------+\n|     score     |              0.91             |\n+---------------+-------------------------------+\n|  input_tokens | {'gpt-3.5-turbo-0125': 39747} |\n+---------------+-------------------------------+\n| output_tokens |  {'gpt-3.5-turbo-0125': 6728} |\n+---------------+-------------------------------+\n|   input_cost  |     $0.019873500000000002     |\n+---------------+-------------------------------+\n|  output_cost  |     $0.010092000000000002     |\n+---------------+-------------------------------+\n|  num_success  |              100              |\n+---------------+-------------------------------+\n|  num_failure  |               0               |\n+---------------+-------------------------------+\n| total_latency |       191.24415755318478      |\n+---------------+-------------------------------+\nTotal cost: $0.029965500000000006\n+---------------+-------------------------------+\n|     score     |              0.76             |\n+---------------+-------------------------------+\n|  input_tokens | {'gpt-3.5-turbo-0125': 58359} |\n+---------------+-------------------------------+\n| output_tokens |  {'gpt-3.5-turbo-0125': 2651} |\n+---------------+-------------------------------+\n|   input_cost  |     $0.029179500000000004     |\n+---------------+-------------------------------+\n|  output_cost  |     $0.003976500000000004     |\n+---------------+-------------------------------+\n|  num_success  |              100              |\n+---------------+-------------------------------+\n|  num_failure  |               0               |\n+---------------+-------------------------------+\n| total_latency |       254.63680869879317      |\n+---------------+-------------------------------+\nTotal cost: $0.033156000000000005\n</pre> <p>Our hierarchical approach cost just a bit more at $0.032814 / 100 rows. It was much faster and seemed to perform better on accuracy as well. However, we're not done just yet. The power of <code>SuperPipe</code> is that we can easily try many different permuations of our pipeline using a grid search. There might be a better pipeline out there.</p> In\u00a0[26]: Copied! <pre>from superpipe import grid_search\n\nparams_grid = {\n    short_description_step.name: {\n        'model': [models.gpt35, models.gpt4], \n    },\n    embedding_search_step.name: {\n        'k': [3, 5, 7],  \n    },\n    categorize_step.name: {\n        'model': [models.gpt35, models.gpt4], \n    },\n}\n\nsmall_df = df.head(30).copy()\n\n\nsearch_embeddings = grid_search.GridSearch(categorizer, params_grid)\nsearch_embeddings.run(small_df)\n</pre> from superpipe import grid_search  params_grid = {     short_description_step.name: {         'model': [models.gpt35, models.gpt4],      },     embedding_search_step.name: {         'k': [3, 5, 7],       },     categorize_step.name: {         'model': [models.gpt35, models.gpt4],      }, }  small_df = df.head(30).copy()   search_embeddings = grid_search.GridSearch(categorizer, params_grid) search_embeddings.run(small_df) <pre>Iteration 1 of 12\nParams:  {'short_description': {'model': 'gpt-3.5-turbo-0125'}, 'embedding_search': {'k': 3}, 'categorize': {'model': 'gpt-3.5-turbo-0125'}}\n</pre> <pre>Applying step short_description: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:46&lt;00:00,  1.55s/it]\nApplying step embedding_search: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:05&lt;00:00,  5.70it/s]\nApplying step categorize: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:16&lt;00:00,  1.87it/s]\nApplying step predicted_category: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:00&lt;00:00, 27100.82it/s]\n</pre> <pre>Result:  {'short_description__model': 'gpt-3.5-turbo-0125', 'embedding_search__k': 3, 'categorize__model': 'gpt-3.5-turbo-0125', 'score': 0.8666666666666667, 'input_cost': 0.005675499999999998, 'output_cost': 0.0032055, 'total_latency': 62.25483133213129, 'input_tokens': defaultdict(&lt;class 'int'&gt;, {'gpt-3.5-turbo-0125': 11351}), 'output_tokens': defaultdict(&lt;class 'int'&gt;, {'gpt-3.5-turbo-0125': 2137}), 'num_success': 30, 'num_failure': 0, 'index': -6675265432874878197}\nIteration 2 of 12\nParams:  {'short_description': {'model': 'gpt-3.5-turbo-0125'}, 'embedding_search': {'k': 3}, 'categorize': {'model': 'gpt-4-turbo-preview'}}\n</pre> <pre>Applying step short_description: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:45&lt;00:00,  1.53s/it]\nApplying step embedding_search: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:04&lt;00:00,  6.68it/s]\nApplying step categorize: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [02:08&lt;00:00,  4.29s/it]\nApplying step predicted_category: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:00&lt;00:00, 28384.64it/s]\n</pre> <pre>Result:  {'short_description__model': 'gpt-3.5-turbo-0125', 'embedding_search__k': 3, 'categorize__model': 'gpt-4-turbo-preview', 'score': 0.9333333333333333, 'input_cost': 0.057895999999999996, 'output_cost': 0.0117495, 'total_latency': 174.37851832807064, 'input_tokens': defaultdict(&lt;class 'int'&gt;, {'gpt-3.5-turbo-0125': 5852, 'gpt-4-turbo-preview': 5497}), 'output_tokens': defaultdict(&lt;class 'int'&gt;, {'gpt-3.5-turbo-0125': 1833, 'gpt-4-turbo-preview': 300}), 'num_success': 30, 'num_failure': 0, 'index': 5054694111921705162}\nIteration 3 of 12\nParams:  {'short_description': {'model': 'gpt-3.5-turbo-0125'}, 'embedding_search': {'k': 5}, 'categorize': {'model': 'gpt-3.5-turbo-0125'}}\n</pre> <pre>Applying step short_description: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:46&lt;00:00,  1.53s/it]\nApplying step embedding_search: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:04&lt;00:00,  7.15it/s]\nApplying step categorize: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:15&lt;00:00,  1.89it/s]\nApplying step predicted_category: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:00&lt;00:00, 20226.51it/s]\n</pre> <pre>Result:  {'short_description__model': 'gpt-3.5-turbo-0125', 'embedding_search__k': 5, 'categorize__model': 'gpt-3.5-turbo-0125', 'score': 0.9, 'input_cost': 0.005970999999999999, 'output_cost': 0.003195, 'total_latency': 61.76637146304711, 'input_tokens': defaultdict(&lt;class 'int'&gt;, {'gpt-3.5-turbo-0125': 11942}), 'output_tokens': defaultdict(&lt;class 'int'&gt;, {'gpt-3.5-turbo-0125': 2130}), 'num_success': 30, 'num_failure': 0, 'index': -4607444568377834415}\nIteration 4 of 12\nParams:  {'short_description': {'model': 'gpt-3.5-turbo-0125'}, 'embedding_search': {'k': 5}, 'categorize': {'model': 'gpt-4-turbo-preview'}}\n</pre> <pre>Applying step short_description: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:48&lt;00:00,  1.63s/it]\nApplying step embedding_search: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:04&lt;00:00,  7.20it/s]\nApplying step categorize: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:47&lt;00:00,  1.58s/it]\nApplying step predicted_category: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:00&lt;00:00, 13654.81it/s]\n</pre> <pre>Result:  {'short_description__model': 'gpt-3.5-turbo-0125', 'embedding_search__k': 5, 'categorize__model': 'gpt-4-turbo-preview', 'score': 0.9666666666666667, 'input_cost': 0.062886, 'output_cost': 0.011592, 'total_latency': 96.02623478500755, 'input_tokens': defaultdict(&lt;class 'int'&gt;, {'gpt-3.5-turbo-0125': 5852, 'gpt-4-turbo-preview': 5996}), 'output_tokens': defaultdict(&lt;class 'int'&gt;, {'gpt-3.5-turbo-0125': 1728, 'gpt-4-turbo-preview': 300}), 'num_success': 30, 'num_failure': 0, 'index': -8503795277776717559}\nIteration 5 of 12\nParams:  {'short_description': {'model': 'gpt-3.5-turbo-0125'}, 'embedding_search': {'k': 7}, 'categorize': {'model': 'gpt-3.5-turbo-0125'}}\n</pre> <pre>Applying step short_description: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:43&lt;00:00,  1.45s/it]\nApplying step embedding_search: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:04&lt;00:00,  6.99it/s]\nApplying step categorize: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:18&lt;00:00,  1.65it/s]\nApplying step predicted_category: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:00&lt;00:00, 13476.40it/s]\n</pre> <pre>Result:  {'short_description__model': 'gpt-3.5-turbo-0125', 'embedding_search__k': 7, 'categorize__model': 'gpt-3.5-turbo-0125', 'score': 0.9, 'input_cost': 0.006242499999999998, 'output_cost': 0.003072, 'total_latency': 61.58326062496053, 'input_tokens': defaultdict(&lt;class 'int'&gt;, {'gpt-3.5-turbo-0125': 12485}), 'output_tokens': defaultdict(&lt;class 'int'&gt;, {'gpt-3.5-turbo-0125': 2048}), 'num_success': 30, 'num_failure': 0, 'index': 4015312520520374081}\nIteration 6 of 12\nParams:  {'short_description': {'model': 'gpt-3.5-turbo-0125'}, 'embedding_search': {'k': 7}, 'categorize': {'model': 'gpt-4-turbo-preview'}}\n</pre> <pre>Applying step short_description: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:41&lt;00:00,  1.39s/it]\nApplying step embedding_search: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:04&lt;00:00,  7.16it/s]\nApplying step categorize: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:51&lt;00:00,  1.70s/it]\nApplying step predicted_category: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:00&lt;00:00, 19828.10it/s]\n</pre> <pre>Result:  {'short_description__model': 'gpt-3.5-turbo-0125', 'embedding_search__k': 7, 'categorize__model': 'gpt-4-turbo-preview', 'score': 0.9333333333333333, 'input_cost': 0.06853599999999999, 'output_cost': 0.0115485, 'total_latency': 92.52336829315755, 'input_tokens': defaultdict(&lt;class 'int'&gt;, {'gpt-3.5-turbo-0125': 5852, 'gpt-4-turbo-preview': 6561}), 'output_tokens': defaultdict(&lt;class 'int'&gt;, {'gpt-3.5-turbo-0125': 1699, 'gpt-4-turbo-preview': 300}), 'num_success': 30, 'num_failure': 0, 'index': -6042391003316854449}\nIteration 7 of 12\nParams:  {'short_description': {'model': 'gpt-4-turbo-preview'}, 'embedding_search': {'k': 3}, 'categorize': {'model': 'gpt-3.5-turbo-0125'}}\n</pre> <pre>Applying step short_description: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [02:13&lt;00:00,  4.45s/it]\nApplying step embedding_search: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:04&lt;00:00,  6.62it/s]\nApplying step categorize: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:21&lt;00:00,  1.38it/s]\nApplying step predicted_category: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:00&lt;00:00, 14540.00it/s]\n</pre> <pre>Result:  {'short_description__model': 'gpt-4-turbo-preview', 'embedding_search__k': 3, 'categorize__model': 'gpt-3.5-turbo-0125', 'score': 0.6666666666666666, 'input_cost': 0.061239999999999996, 'output_cost': 0.05397149999999999, 'total_latency': 155.0610504578508, 'input_tokens': defaultdict(&lt;class 'int'&gt;, {'gpt-4-turbo-preview': 5852, 'gpt-3.5-turbo-0125': 5440}), 'output_tokens': defaultdict(&lt;class 'int'&gt;, {'gpt-4-turbo-preview': 1784, 'gpt-3.5-turbo-0125': 301}), 'num_success': 30, 'num_failure': 0, 'index': -3802806156793363307}\nIteration 8 of 12\nParams:  {'short_description': {'model': 'gpt-4-turbo-preview'}, 'embedding_search': {'k': 3}, 'categorize': {'model': 'gpt-4-turbo-preview'}}\n</pre> <pre>Applying step short_description: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [02:10&lt;00:00,  4.35s/it]\nApplying step embedding_search: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:04&lt;00:00,  7.09it/s]\nApplying step categorize: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:39&lt;00:00,  1.31s/it]\nApplying step predicted_category: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:00&lt;00:00, 13742.80it/s]\n</pre> <pre>Result:  {'short_description__model': 'gpt-4-turbo-preview', 'embedding_search__k': 3, 'categorize__model': 'gpt-4-turbo-preview', 'score': 0.9, 'input_cost': 0.11293, 'output_cost': 0.062310000000000004, 'total_latency': 169.59190933196805, 'input_tokens': defaultdict(&lt;class 'int'&gt;, {'gpt-4-turbo-preview': 11293}), 'output_tokens': defaultdict(&lt;class 'int'&gt;, {'gpt-4-turbo-preview': 2077}), 'num_success': 30, 'num_failure': 0, 'index': -3569261079577541644}\nIteration 9 of 12\nParams:  {'short_description': {'model': 'gpt-4-turbo-preview'}, 'embedding_search': {'k': 5}, 'categorize': {'model': 'gpt-3.5-turbo-0125'}}\n</pre> <pre>Applying step short_description: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [01:57&lt;00:00,  3.91s/it]\nApplying step embedding_search: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:04&lt;00:00,  7.20it/s]\nApplying step categorize: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:14&lt;00:00,  2.04it/s]\nApplying step predicted_category: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:00&lt;00:00, 18842.34it/s]\n</pre> <pre>Result:  {'short_description__model': 'gpt-4-turbo-preview', 'embedding_search__k': 5, 'categorize__model': 'gpt-3.5-turbo-0125', 'score': 0.8666666666666667, 'input_cost': 0.0615815, 'output_cost': 0.055741500000000006, 'total_latency': 131.90737874808838, 'input_tokens': defaultdict(&lt;class 'int'&gt;, {'gpt-4-turbo-preview': 5852, 'gpt-3.5-turbo-0125': 6123}), 'output_tokens': defaultdict(&lt;class 'int'&gt;, {'gpt-4-turbo-preview': 1843, 'gpt-3.5-turbo-0125': 301}), 'num_success': 30, 'num_failure': 0, 'index': 9106143806313371546}\nIteration 10 of 12\nParams:  {'short_description': {'model': 'gpt-4-turbo-preview'}, 'embedding_search': {'k': 5}, 'categorize': {'model': 'gpt-4-turbo-preview'}}\n</pre> <pre>Applying step short_description: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [02:04&lt;00:00,  4.14s/it]\nApplying step embedding_search: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:04&lt;00:00,  6.91it/s]\nApplying step categorize: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:41&lt;00:00,  1.38s/it]\nApplying step predicted_category: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:00&lt;00:00, 17329.45it/s]\n</pre> <pre>Result:  {'short_description__model': 'gpt-4-turbo-preview', 'embedding_search__k': 5, 'categorize__model': 'gpt-4-turbo-preview', 'score': 0.9333333333333333, 'input_cost': 0.11912999999999999, 'output_cost': 0.06251999999999999, 'total_latency': 165.1544967039954, 'input_tokens': defaultdict(&lt;class 'int'&gt;, {'gpt-4-turbo-preview': 11913}), 'output_tokens': defaultdict(&lt;class 'int'&gt;, {'gpt-4-turbo-preview': 2084}), 'num_success': 30, 'num_failure': 0, 'index': 2557837101084672621}\nIteration 11 of 12\nParams:  {'short_description': {'model': 'gpt-4-turbo-preview'}, 'embedding_search': {'k': 7}, 'categorize': {'model': 'gpt-3.5-turbo-0125'}}\n</pre> <pre>Applying step short_description: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [01:51&lt;00:00,  3.71s/it]\nApplying step embedding_search: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:08&lt;00:00,  3.53it/s]\nApplying step categorize: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:15&lt;00:00,  1.99it/s]\nApplying step predicted_category: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:00&lt;00:00, 19242.87it/s]\n</pre> <pre>Result:  {'short_description__model': 'gpt-4-turbo-preview', 'embedding_search__k': 7, 'categorize__model': 'gpt-3.5-turbo-0125', 'score': 0.8333333333333334, 'input_cost': 0.0618185, 'output_cost': 0.05157, 'total_latency': 126.11481383198407, 'input_tokens': defaultdict(&lt;class 'int'&gt;, {'gpt-4-turbo-preview': 5852, 'gpt-3.5-turbo-0125': 6597}), 'output_tokens': defaultdict(&lt;class 'int'&gt;, {'gpt-4-turbo-preview': 1704, 'gpt-3.5-turbo-0125': 300}), 'num_success': 30, 'num_failure': 0, 'index': -3503115138122502664}\nIteration 12 of 12\nParams:  {'short_description': {'model': 'gpt-4-turbo-preview'}, 'embedding_search': {'k': 7}, 'categorize': {'model': 'gpt-4-turbo-preview'}}\n</pre> <pre>Applying step short_description: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [01:53&lt;00:00,  3.78s/it]\nApplying step embedding_search: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:03&lt;00:00,  8.31it/s]\nApplying step categorize: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:35&lt;00:00,  1.18s/it]\nApplying step predicted_category: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:00&lt;00:00, 15283.51it/s]\n</pre> <pre>Result:  {'short_description__model': 'gpt-4-turbo-preview', 'embedding_search__k': 7, 'categorize__model': 'gpt-4-turbo-preview', 'score': 0.9333333333333333, 'input_cost': 0.12448999999999999, 'output_cost': 0.06029999999999999, 'total_latency': 148.47731783005293, 'input_tokens': defaultdict(&lt;class 'int'&gt;, {'gpt-4-turbo-preview': 12449}), 'output_tokens': defaultdict(&lt;class 'int'&gt;, {'gpt-4-turbo-preview': 2010}), 'num_success': 30, 'num_failure': 0, 'index': -3155834270503923271}\n</pre> <pre>/Users/bscharfstein/Projects/Stelo/code/superpipe/superpipe/grid_search.py:146: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.\n  styler = styler.applymap(lambda val, col=col: apply_style(val, col), subset=[col])\n</pre> Out[26]: short_description__model embedding_search__k categorize__model score input_cost output_cost total_latency input_tokens output_tokens num_success num_failure index 0 gpt-3.5-turbo-0125 3 gpt-3.5-turbo-0125 0.866667 0.005675 0.003206 62.254831 defaultdict(, {'gpt-3.5-turbo-0125': 11351}) defaultdict(, {'gpt-3.5-turbo-0125': 2137}) 30 0 -6675265432874878197 1 gpt-3.5-turbo-0125 3 gpt-4-turbo-preview 0.933333 0.057896 0.011749 174.378518 defaultdict(, {'gpt-3.5-turbo-0125': 5852, 'gpt-4-turbo-preview': 5497}) defaultdict(, {'gpt-3.5-turbo-0125': 1833, 'gpt-4-turbo-preview': 300}) 30 0 5054694111921705162 2 gpt-3.5-turbo-0125 5 gpt-3.5-turbo-0125 0.900000 0.005971 0.003195 61.766371 defaultdict(, {'gpt-3.5-turbo-0125': 11942}) defaultdict(, {'gpt-3.5-turbo-0125': 2130}) 30 0 -4607444568377834415 3 gpt-3.5-turbo-0125 5 gpt-4-turbo-preview 0.966667 0.062886 0.011592 96.026235 defaultdict(, {'gpt-3.5-turbo-0125': 5852, 'gpt-4-turbo-preview': 5996}) defaultdict(, {'gpt-3.5-turbo-0125': 1728, 'gpt-4-turbo-preview': 300}) 30 0 -8503795277776717559 4 gpt-3.5-turbo-0125 7 gpt-3.5-turbo-0125 0.900000 0.006242 0.003072 61.583261 defaultdict(, {'gpt-3.5-turbo-0125': 12485}) defaultdict(, {'gpt-3.5-turbo-0125': 2048}) 30 0 4015312520520374081 5 gpt-3.5-turbo-0125 7 gpt-4-turbo-preview 0.933333 0.068536 0.011548 92.523368 defaultdict(, {'gpt-3.5-turbo-0125': 5852, 'gpt-4-turbo-preview': 6561}) defaultdict(, {'gpt-3.5-turbo-0125': 1699, 'gpt-4-turbo-preview': 300}) 30 0 -6042391003316854449 6 gpt-4-turbo-preview 3 gpt-3.5-turbo-0125 0.666667 0.061240 0.053971 155.061050 defaultdict(, {'gpt-4-turbo-preview': 5852, 'gpt-3.5-turbo-0125': 5440}) defaultdict(, {'gpt-4-turbo-preview': 1784, 'gpt-3.5-turbo-0125': 301}) 30 0 -3802806156793363307 7 gpt-4-turbo-preview 3 gpt-4-turbo-preview 0.900000 0.112930 0.062310 169.591909 defaultdict(, {'gpt-4-turbo-preview': 11293}) defaultdict(, {'gpt-4-turbo-preview': 2077}) 30 0 -3569261079577541644 8 gpt-4-turbo-preview 5 gpt-3.5-turbo-0125 0.866667 0.061581 0.055742 131.907379 defaultdict(, {'gpt-4-turbo-preview': 5852, 'gpt-3.5-turbo-0125': 6123}) defaultdict(, {'gpt-4-turbo-preview': 1843, 'gpt-3.5-turbo-0125': 301}) 30 0 9106143806313371546 9 gpt-4-turbo-preview 5 gpt-4-turbo-preview 0.933333 0.119130 0.062520 165.154497 defaultdict(, {'gpt-4-turbo-preview': 11913}) defaultdict(, {'gpt-4-turbo-preview': 2084}) 30 0 2557837101084672621 10 gpt-4-turbo-preview 7 gpt-3.5-turbo-0125 0.833333 0.061818 0.051570 126.114814 defaultdict(, {'gpt-4-turbo-preview': 5852, 'gpt-3.5-turbo-0125': 6597}) defaultdict(, {'gpt-4-turbo-preview': 1704, 'gpt-3.5-turbo-0125': 300}) 30 0 -3503115138122502664 11 gpt-4-turbo-preview 7 gpt-4-turbo-preview 0.933333 0.124490 0.060300 148.477318 defaultdict(, {'gpt-4-turbo-preview': 12449}) defaultdict(, {'gpt-4-turbo-preview': 2010}) 30 0 -3155834270503923271 <p>The results of our grid search are conveniently put into a dataframe for us to review.</p> <p>Its seems that GPT-3.5 is more than sufficient for our description step and that 5 embeddings results is as well. For the last step, we have a cost/latency vs. accuracy tradeoff we need to make between the two models.</p> <p>This search was only run on 30 rows so we'd want to run it more extensively before making decisions for production but at least now we can reasonably confidently narrow down our search space.</p> <p>Let's do the same for our hierarchical prompting approach. This time we'll just vary the model selection for each step.</p> In\u00a0[28]: Copied! <pre>params_grid = {\n    first_level_category_step.name: {\n        'model': [models.gpt35, models.gpt4],  \n    },\n    second_level_category_step.name: {\n        'model': [models.gpt35, models.gpt4],  \n    },\n}\n\nsmall_df2 = df.head(30).copy()\n\nsearch_llm = grid_search.GridSearch(categorizer_llm, params_grid)\nsearch_llm.run(small_df2)\n</pre> params_grid = {     first_level_category_step.name: {         'model': [models.gpt35, models.gpt4],       },     second_level_category_step.name: {         'model': [models.gpt35, models.gpt4],       }, }  small_df2 = df.head(30).copy()  search_llm = grid_search.GridSearch(categorizer_llm, params_grid) search_llm.run(small_df2) <pre>Iteration 1 of 4\nParams:  {'first_categorize': {'model': 'gpt-3.5-turbo-0125'}, 'second_categorize': {'model': 'gpt-3.5-turbo-0125'}}\n</pre> <pre>Applying step first_categorize: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:20&lt;00:00,  1.45it/s]\nApplying step predicted_first_category: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:00&lt;00:00, 6613.54it/s]\nApplying step second_categorize: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:16&lt;00:00,  1.80it/s]\nApplying step predicted_second_category: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:00&lt;00:00, 6980.04it/s]\nApplying step combine_taxonomy: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:00&lt;00:00, 20631.11it/s]\n</pre> <pre>Result:  {'first_categorize__model': 'gpt-3.5-turbo-0125', 'second_categorize__model': 'gpt-3.5-turbo-0125', 'score': 0.7666666666666667, 'input_cost': 0.008323999999999998, 'output_cost': 0.0009915000000000006, 'total_latency': 37.14652425216627, 'input_tokens': defaultdict(&lt;class 'int'&gt;, {'gpt-3.5-turbo-0125': 16648}), 'output_tokens': defaultdict(&lt;class 'int'&gt;, {'gpt-3.5-turbo-0125': 661}), 'num_success': 30, 'num_failure': 0, 'index': 8291905896722117770}\nIteration 2 of 4\nParams:  {'first_categorize': {'model': 'gpt-3.5-turbo-0125'}, 'second_categorize': {'model': 'gpt-4-turbo-preview'}}\n</pre> <pre>Applying step first_categorize: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:15&lt;00:00,  1.96it/s]\nApplying step predicted_first_category: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:00&lt;00:00, 10063.11it/s]\nApplying step second_categorize: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:45&lt;00:00,  1.50s/it]\nApplying step predicted_second_category: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:00&lt;00:00, 6388.56it/s]\nApplying step combine_taxonomy: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:00&lt;00:00, 25450.87it/s]\n</pre> <pre>Result:  {'first_categorize__model': 'gpt-3.5-turbo-0125', 'second_categorize__model': 'gpt-4-turbo-preview', 'score': 0.9333333333333333, 'input_cost': 0.080676, 'output_cost': 0.010305000000000009, 'total_latency': 60.223217750986805, 'input_tokens': defaultdict(&lt;class 'int'&gt;, {'gpt-3.5-turbo-0125': 9032, 'gpt-4-turbo-preview': 7616}), 'output_tokens': defaultdict(&lt;class 'int'&gt;, {'gpt-3.5-turbo-0125': 330, 'gpt-4-turbo-preview': 327}), 'num_success': 30, 'num_failure': 0, 'index': 3072009371341041402}\nIteration 3 of 4\nParams:  {'first_categorize': {'model': 'gpt-4-turbo-preview'}, 'second_categorize': {'model': 'gpt-3.5-turbo-0125'}}\n</pre> <pre>Applying step first_categorize: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:59&lt;00:00,  1.97s/it]\nApplying step predicted_first_category: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:00&lt;00:00, 22137.42it/s]\nApplying step second_categorize: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:16&lt;00:00,  1.85it/s]\nApplying step predicted_second_category: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:00&lt;00:00, 8736.31it/s]\nApplying step combine_taxonomy: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:00&lt;00:00, 22832.36it/s]\n</pre> <pre>Result:  {'first_categorize__model': 'gpt-4-turbo-preview', 'second_categorize__model': 'gpt-3.5-turbo-0125', 'score': 0.7333333333333333, 'input_cost': 0.094135, 'output_cost': 0.010396500000000008, 'total_latency': 75.15457291598432, 'input_tokens': defaultdict(&lt;class 'int'&gt;, {'gpt-4-turbo-preview': 9032, 'gpt-3.5-turbo-0125': 7630}), 'output_tokens': defaultdict(&lt;class 'int'&gt;, {'gpt-4-turbo-preview': 330, 'gpt-3.5-turbo-0125': 331}), 'num_success': 30, 'num_failure': 0, 'index': -172566906004615760}\nIteration 4 of 4\nParams:  {'first_categorize': {'model': 'gpt-4-turbo-preview'}, 'second_categorize': {'model': 'gpt-4-turbo-preview'}}\n</pre> <pre>Applying step first_categorize: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:45&lt;00:00,  1.52s/it]\nApplying step predicted_first_category: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:00&lt;00:00, 12587.95it/s]\nApplying step second_categorize: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:40&lt;00:00,  1.34s/it]\nApplying step predicted_second_category: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:00&lt;00:00, 8019.19it/s]\nApplying step combine_taxonomy: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:00&lt;00:00, 22762.14it/s]</pre> <pre>Result:  {'first_categorize__model': 'gpt-4-turbo-preview', 'second_categorize__model': 'gpt-4-turbo-preview', 'score': 0.9, 'input_cost': 0.16662, 'output_cost': 0.019800000000000016, 'total_latency': 85.77000208501704, 'input_tokens': defaultdict(&lt;class 'int'&gt;, {'gpt-4-turbo-preview': 16662}), 'output_tokens': defaultdict(&lt;class 'int'&gt;, {'gpt-4-turbo-preview': 660}), 'num_success': 30, 'num_failure': 0, 'index': 7445854736442369664}\n</pre> <pre>\n/Users/bscharfstein/Projects/Stelo/code/superpipe/superpipe/grid_search.py:146: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.\n  styler = styler.applymap(lambda val, col=col: apply_style(val, col), subset=[col])\n</pre> Out[28]: first_categorize__model second_categorize__model score input_cost output_cost total_latency input_tokens output_tokens num_success num_failure index 0 gpt-3.5-turbo-0125 gpt-3.5-turbo-0125 0.766667 0.008324 0.000992 37.146524 defaultdict(, {'gpt-3.5-turbo-0125': 16648}) defaultdict(, {'gpt-3.5-turbo-0125': 661}) 30 0 8291905896722117770 1 gpt-3.5-turbo-0125 gpt-4-turbo-preview 0.933333 0.080676 0.010305 60.223218 defaultdict(, {'gpt-3.5-turbo-0125': 9032, 'gpt-4-turbo-preview': 7616}) defaultdict(, {'gpt-3.5-turbo-0125': 330, 'gpt-4-turbo-preview': 327}) 30 0 3072009371341041402 2 gpt-4-turbo-preview gpt-3.5-turbo-0125 0.733333 0.094135 0.010397 75.154573 defaultdict(, {'gpt-4-turbo-preview': 9032, 'gpt-3.5-turbo-0125': 7630}) defaultdict(, {'gpt-4-turbo-preview': 330, 'gpt-3.5-turbo-0125': 331}) 30 0 -172566906004615760 3 gpt-4-turbo-preview gpt-4-turbo-preview 0.900000 0.166620 0.019800 85.770002 defaultdict(, {'gpt-4-turbo-preview': 16662}) defaultdict(, {'gpt-4-turbo-preview': 660}) 30 0 7445854736442369664 <p>These results highlight the importance of experimentation and optimization. As we can see, the GPT-3.5 + GPT-4 hierarchical pipeline performs the best with relatively low latency with the GPT-3.5 only approach performing about as well as the GPT-3.5 only + 5 embedding approach.</p> <p>If we only care about accuracy, it looks like an embeddings based approach is our best bet. However, we may have other considerations. We're faced with a cost, accuracy, and latency tradeoff with no clear \"best\" option. Depending on what metric we care we'll choose a different approach. This is a decision we're now empowered to make with our Superpipe pipeline results.</p>"},{"location":"examples/comparing_pipelines/furniture/#comparing-different-approaches","title":"Comparing different approaches\u00b6","text":"<p>View on Github</p> <p>There are many ways to build a labeling pipeline that all will accomplish the same result. The goal of <code>Superpipe</code> is to empower rapid and robust experimentation so that you can understand the performance, accuracy, and cost tradeoffs between approaches.</p> <p>In this example, we'll experiment with a few different approaches to a categorization pipeline we want to build. <code>Superpipe</code> will make this experimentation quick and at the end we'll have a solid understanding of how different approaches perform.</p>"},{"location":"examples/comparing_pipelines/furniture/#task","title":"Task\u00b6","text":"<p>The task at hand is to categorize furniture items into a multi-level taxonomy based on their name and description.</p> <p>For example Name: <code>Blair Table by homestyles</code></p> <p>Description: <code>This Blair Table by homestyles is perfect for Sunday brunches or game night. The round pedestal table is available as shown, or as part of a five-piece set. Features solid hardwood construction in a black finish that can easily match a traditional or contemporary aesthetic. Measures: 30\"H x 42\" Diameter</code></p> <p>Correct classification: <code>Tables &amp; Desks &gt; Dining Tables</code></p>"},{"location":"examples/comparing_pipelines/furniture/#approaches","title":"Approaches\u00b6","text":"<p>There are two different approaches we want to try.</p> <ol> <li>LLMs + Embedding</li> <li>Heiarchical prompting</li> </ol>"},{"location":"examples/comparing_pipelines/furniture/#data-processing","title":"Data processing\u00b6","text":"<p>We'll start out with reading in our data and building our taxonomy. The process of building a taxonomy is a project in and of itself. There are also many taxonomies available online that you can use. In our case, we're building our taxonomy based on our ground truth dataset. Since we have such a large dataset we can be reasonably confident that all values are represented. As you'll see our approach does not use the ground truth data as training data so it will be easy for us to expand the taxonomy without needing additional data.</p>"},{"location":"examples/comparing_pipelines/furniture/#building-our-pipeline-using-superpipe","title":"Building our pipeline using Superpipe\u00b6","text":""},{"location":"examples/comparing_pipelines/furniture/#approach-1-embeddings","title":"Approach 1: Embeddings\u00b6","text":"<p>The first approach is similar to the approach we took in the <code>Product Categorization</code> example we gave in the project repo. We are omitting the Google Search step because we already have item descriptions.</p> <ol> <li>Write a simple description of the product given name and description</li> <li>Vector embedding search for top N categories</li> <li>LLM: pick the best category</li> </ol>"},{"location":"examples/comparing_pipelines/furniture/#approach-2-hierarchical-prompting","title":"Approach 2: hierarchical prompting\u00b6","text":"<p>Next we want to try forgoing embeddings all together and simply stuffing all of the categories into the prompt. There are too many categories to do this all in one go but we can use the fact that our categories are hierarchical and take a step by step approach.</p> <ol> <li>LLM: given product name, description, and first level categories, pick the best one.</li> <li>LLM: given product name, description, and second level categories, pick the best one.</li> </ol> <p>We may want to iterate a bit on this process. For example, we may want to use one model in step 1 and a different model in step 2. <code>Superpipe</code> makes this type of hyperparameter tuning easy and robust.</p>"},{"location":"examples/comparing_pipelines/furniture/#grid-search","title":"Grid search\u00b6","text":"<p>Our first pipeline has three steps we want to search over.</p> <ol> <li>Short description: vary the model</li> <li>Embedding search: vary the number of results</li> <li>Categorize: vary the model</li> </ol> <p>It's not clear which permutation will work the best so we'll try all of them.</p>"},{"location":"examples/custom_eval/jokes/","title":"Custom Evaluation functions","text":"In\u00a0[1]: Copied! <pre>from superpipe import *\nimport pandas as pd\nfrom pydantic import BaseModel, Field\n</pre> from superpipe import * import pandas as pd from pydantic import BaseModel, Field In\u00a0[2]: Copied! <pre>joke_prompt = lambda row: f\"\"\"\nTell me a joke about {row['topic']}\n\"\"\"\n\nJokesStep = steps.LLMStep(\n  prompt=joke_prompt,\n  model=models.gpt35,\n  name=\"joke\"\n)\n</pre> joke_prompt = lambda row: f\"\"\" Tell me a joke about {row['topic']} \"\"\"  JokesStep = steps.LLMStep(   prompt=joke_prompt,   model=models.gpt35,   name=\"joke\" ) In\u00a0[3]: Copied! <pre>topics = ['Beets', 'Bears', 'Battlestar Gallactica']\n</pre> topics = ['Beets', 'Bears', 'Battlestar Gallactica'] In\u00a0[4]: Copied! <pre>jokes_df = pd.DataFrame(topics)\njokes_df.columns = [\"topic\"]\n</pre> jokes_df = pd.DataFrame(topics) jokes_df.columns = [\"topic\"] In\u00a0[5]: Copied! <pre>jokes_df\n</pre> jokes_df Out[5]: topic 0 Beets 1 Bears 2 Battlestar Gallactica <p>Before we define our pipeline we need to create our evaluation function. We're pulling out the <code>get_structured_llm_response</code> function from <code>Superpipe</code> to make our lives easier. Our evaluation function just needs to return a boolean.</p> In\u00a0[57]: Copied! <pre>def evaluate_prompt(row):\n    return f\"\"\"\n    Is the following joke pretty funny? Your bar should be making a friend laugh out loud\n    {row['joke']}\n\n    Return a json object with a single boolean key called 'evaluation'\n    \"\"\"\n\ndef evaluate_joke(row):\n    return llm.get_structured_llm_response(evaluate_prompt(row), models.gpt4).content['evaluation']\n</pre> def evaluate_prompt(row):     return f\"\"\"     Is the following joke pretty funny? Your bar should be making a friend laugh out loud     {row['joke']}      Return a json object with a single boolean key called 'evaluation'     \"\"\"  def evaluate_joke(row):     return llm.get_structured_llm_response(evaluate_prompt(row), models.gpt4).content['evaluation'] <p>Now let's run our simple pipeline</p> In\u00a0[58]: Copied! <pre>comedian = pipeline.Pipeline([\n    JokesStep,\n], evaluation_fn=evaluate_joke)\n\ncomedian.run(jokes_df)\n</pre> comedian = pipeline.Pipeline([     JokesStep, ], evaluation_fn=evaluate_joke)  comedian.run(jokes_df) <pre>Running step joke...\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:01&lt;00:00,  1.91it/s]\n</pre> <pre>0     True\n1     True\n2    False\ndtype: bool\n</pre> Out[58]: topic __joke__ joke __joke4__ joke4 0 Beets {'input_tokens': 16, 'output_tokens': 17, 'inp... I tried to make a beet pun, but it just didn't... {'input_tokens': 16, 'output_tokens': 14, 'inp... Why did the beet turn red?\\n\\nBecause it saw t... 1 Bears {'input_tokens': 15, 'output_tokens': 13, 'inp... Why did the bear dissolve in water?\\n\\nBecause... {'input_tokens': 15, 'output_tokens': 15, 'inp... Why don't bears wear socks?\\n\\nBecause they li... 2 Battlestar Gallactica {'input_tokens': 19, 'output_tokens': 29, 'inp... Why did the Cylon break up with his girlfriend... {'input_tokens': 19, 'output_tokens': 22, 'inp... Why did the Cylon buy an iPhone?\\n\\nBecause he... In\u00a0[59]: Copied! <pre>comedian.statistics\n</pre> comedian.statistics Out[59]: <pre>PipelineStatistics(score=0.6666666666666666, input_tokens=defaultdict(&lt;class 'int'&gt;, {}), output_tokens=defaultdict(&lt;class 'int'&gt;, {}), input_cost=0.0, output_cost=0.0, num_success=0, num_failure=0, total_latency=0.0)</pre> <p>It looks like our eval is working -- it thought 2/3 jokes were funny.</p> <p>Our eval is extremely subjective right now and slight changes to the prompt will move our eval statistic dramatically.</p> In\u00a0[29]: Copied! <pre>JokesStep4 = steps.LLMStep(\n  prompt=joke_prompt,\n  model=models.gpt4,\n  name=\"joke4\"\n)\n</pre> JokesStep4 = steps.LLMStep(   prompt=joke_prompt,   model=models.gpt4,   name=\"joke4\" ) <p>Now we create an evaluation function that compares the jokes side-by-side</p> In\u00a0[61]: Copied! <pre>def evaluate_side_by_side_prompt(row):\n    return f\"\"\"\n    You are given two jokes. Rate which is funnier:\n    joke 1: {row['joke']}\n    joke 2: {row['joke4']}\n\n    If joke 1 is funnier or they are similar, return false. If joke 2 is funnier return true.\n\n    Return a json object with a single boolean key called 'evaluation'\n    \"\"\"\n\ndef evaluate_side_by_side(row):\n    return llm.get_structured_llm_response(evaluate_side_by_side_prompt(row), models.gpt4).content['evaluation']\n</pre> def evaluate_side_by_side_prompt(row):     return f\"\"\"     You are given two jokes. Rate which is funnier:     joke 1: {row['joke']}     joke 2: {row['joke4']}      If joke 1 is funnier or they are similar, return false. If joke 2 is funnier return true.      Return a json object with a single boolean key called 'evaluation'     \"\"\"  def evaluate_side_by_side(row):     return llm.get_structured_llm_response(evaluate_side_by_side_prompt(row), models.gpt4).content['evaluation'] In\u00a0[62]: Copied! <pre>jokes_df[['joke', 'joke4']].values\n</pre> jokes_df[['joke', 'joke4']].values Out[62]: <pre>array([[\"I tried to make a beet pun, but it just didn't turnip right.\",\n        'Why did the beet turn red?\\n\\nBecause it saw the salad dressing!'],\n       ['Why did the bear dissolve in water?\\n\\nBecause it was polar!',\n        \"Why don't bears wear socks?\\n\\nBecause they like to walk bear-foot!\"],\n       ['Why did the Cylon break up with his girlfriend? She kept telling him, \"You can\\'t Frak your way out of every problem!\"',\n        'Why did the Cylon buy an iPhone?\\n\\nBecause he heard it comes with Siri-usly good voice recognition!']],\n      dtype=object)</pre> In\u00a0[63]: Copied! <pre>comedian = pipeline.Pipeline([\n    JokesStep,\n    JokesStep4,\n], evaluation_fn=evaluate_side_by_side)\n\ncomedian.run(jokes_df)\n</pre> comedian = pipeline.Pipeline([     JokesStep,     JokesStep4, ], evaluation_fn=evaluate_side_by_side)  comedian.run(jokes_df) <pre>Running step joke...\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:01&lt;00:00,  1.82it/s]\n</pre> <pre>Running step joke4...\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:03&lt;00:00,  1.07s/it]\n</pre> <pre>0    True\n1    True\n2    True\ndtype: bool\n</pre> Out[63]: topic __joke__ joke __joke4__ joke4 0 Beets {'input_tokens': 16, 'output_tokens': 22, 'inp... Why did the beet break up with the turnip? Bec... {'input_tokens': 16, 'output_tokens': 14, 'inp... Why did the beet turn red?\\n\\nBecause it saw t... 1 Bears {'input_tokens': 15, 'output_tokens': 11, 'inp... Why do bears have hairy coats?\\n\\nFur protection! {'input_tokens': 15, 'output_tokens': 15, 'inp... Why don't bears wear socks?\\n\\nBecause they li... 2 Battlestar Gallactica {'input_tokens': 19, 'output_tokens': 18, 'inp... Why did the Cylon break up with the toaster?\\n... {'input_tokens': 19, 'output_tokens': 26, 'inp... Why did the Cylon go to Starbucks?\\n\\nBecause ... In\u00a0[64]: Copied! <pre>comedian.statistics\n</pre> comedian.statistics Out[64]: <pre>PipelineStatistics(score=1.0, input_tokens=defaultdict(&lt;class 'int'&gt;, {}), output_tokens=defaultdict(&lt;class 'int'&gt;, {}), input_cost=0.0, output_cost=0.0, num_success=0, num_failure=0, total_latency=0.0)</pre> <p>It seems like GPT-4 prefers its own jokes! Of course this isn't a rigourous evaluation but it should be enough to get you started evaluating generative outputs.</p>"},{"location":"examples/custom_eval/jokes/#custom-evaluation-functions","title":"Custom Evaluation functions\u00b6","text":"<p>View on Github</p> <p>Superpipe can also be used to evaluate generative output by definining custom eval functions. There are two primary ways to evaluate the quality of generative output:</p> <ol> <li>Assertions</li> <li>Side-by-side comparisons</li> </ol> <p>In this example, we'll ask GPT-4 to help us with both approaches.</p>"},{"location":"examples/custom_eval/jokes/#defining-our-pipeline","title":"Defining our pipeline\u00b6","text":"<p>The <code>LLMStep</code> takes in a prompt, model and name and outputs a column with that name. In this case, we just want the LLM to tell us a joke.</p>"},{"location":"examples/custom_eval/jokes/#side-by-side-evals","title":"Side-by-side evals\u00b6","text":"<p>Side-by-sides are still subjective but are often more aligned with the choice an AI engineer is making. The question is often not \"is this funny\" but rather \"which model is funnier\".</p> <p>Let's create a GPT-4 joke step to compare to.</p>"},{"location":"examples/product_categorization/product_categorization/","title":"Product Categorization","text":"In\u00a0[5]: Copied! <pre>from IPython.display import Image\nImage(\"image.png\")\n</pre> from IPython.display import Image Image(\"image.png\") Out[5]: <p>Install dependencies, import libraries, and load the data and the taxonomy</p> In\u00a0[1]: Copied! <pre># %pip install cohere\n\nimport json\nimport pandas as pd\nfrom superpipe import *\nfrom pydantic import BaseModel, Field\nimport cohere\nimport os\nimport numpy as np\nfrom typing import List\n</pre> # %pip install cohere  import json import pandas as pd from superpipe import * from pydantic import BaseModel, Field import cohere import os import numpy as np from typing import List <pre>/var/folders/bh/hwln9nhn6tb990j17wgs638r0000gn/T/ipykernel_89835/1194329160.py:4: DeprecationWarning: \nPyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\nbut was not found to be installed on your system.\nIf this would cause problems for you,\nplease provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n        \n  import pandas as pd\n</pre> In\u00a0[2]: Copied! <pre>taxonomy = pd.read_csv('./taxonomy.csv').fillna(\"\")\ntaxonomy = list(taxonomy['sector'] + \" &gt; \" + taxonomy['department'] + \" &gt; \" + taxonomy['major_category'])\ndf = pd.read_csv('./data.csv')\n</pre> taxonomy = pd.read_csv('./taxonomy.csv').fillna(\"\") taxonomy = list(taxonomy['sector'] + \" &gt; \" + taxonomy['department'] + \" &gt; \" + taxonomy['major_category']) df = pd.read_csv('./data.csv') <p>Now, let's define the first step of the pipeline which uses a Google SERP library to search for the <code>description</code> field on the input object.</p> <p>We do this by using Superpipe's built-in <code>SERPEnrichmentStep</code>. You could also easily build your own using a <code>CustomStep</code> instead.</p> In\u00a0[3]: Copied! <pre>def shorten(serp_result):\n  top_n = 5\n  short = []\n  y = json.loads(serp_result).get('organic')\n  if y is None:\n    return None\n  for o in y[:top_n]:\n    short.append({\n        'title': o['title'],\n        'snippet': o['snippet'],\n        'link': o['link']\n    })\n  return short\n\nserp_step = steps.SERPEnrichmentStep(\n  prompt=lambda row: row['description'], \n  postprocess=shorten,\n  name=\"serp\")\n</pre> def shorten(serp_result):   top_n = 5   short = []   y = json.loads(serp_result).get('organic')   if y is None:     return None   for o in y[:top_n]:     short.append({         'title': o['title'],         'snippet': o['snippet'],         'link': o['link']     })   return short  serp_step = steps.SERPEnrichmentStep(   prompt=lambda row: row['description'],    postprocess=shorten,   name=\"serp\") <p>The second step of the pipeline takes the product description and the search results and feeds them into an LLM to get a better description. We create this step using <code>LLMStructuredStep</code>.</p> <p>An <code>LLMStructuredStep</code> instance takes a Pydantic model and a prompt generator function as arguments. The pydantic model specifies the output structure (remember every <code>LLMStructuredStep</code> creates structured output). The prompt generator function defines how to generate a prompt from the input data.</p> In\u00a0[4]: Copied! <pre>short_description_prompt = lambda row: f\"\"\"\nYou are given a product description and a list of google search results about a product.\nReturn a single sentence decribing the product.\nProduct description: {row['description']}\nSearch results:\n{row['serp']}\n\"\"\"\n\nclass ShortDescription(BaseModel):\n  short_description: str = Field(description=\"A single sentence describing the product\")\n  \nshort_description_step = steps.LLMStructuredStep(\n  prompt=short_description_prompt,\n  model=models.gpt35,\n  out_schema=ShortDescription,\n  name=\"short_description\"\n)\n</pre> short_description_prompt = lambda row: f\"\"\" You are given a product description and a list of google search results about a product. Return a single sentence decribing the product. Product description: {row['description']} Search results: {row['serp']} \"\"\"  class ShortDescription(BaseModel):   short_description: str = Field(description=\"A single sentence describing the product\")    short_description_step = steps.LLMStructuredStep(   prompt=short_description_prompt,   model=models.gpt35,   out_schema=ShortDescription,   name=\"short_description\" ) <p>Next we define the embedding classification step which creates a vector embedding from the taxonomy, and then finds the top 5 nearest neighbors for each input data point based on the short description generated in the previous step.</p> <p>We can use the built-in <code>EmbeddingSearchStep</code> and provide it an <code>embed</code> function. You can use any embedding provider here. We use Cohere in this example.</p> In\u00a0[5]: Copied! <pre># set your cohere api key as an env var or set it directly here\nCOHERE_API_KEY = os.environ.get('COHERE_API_KEY')\nco = cohere.Client(COHERE_API_KEY)\n\ndef embed(texts: List[str]):\n  embeddings = co.embed(\n    model=\"embed-english-v3.0\",\n    texts=texts,\n    input_type='classification'\n  ).embeddings\n  return np.array(embeddings).astype('float32')\n\nembedding_search_prompt = lambda row: row[\"short_description\"]\n\nembedding_search_step = steps.EmbeddingSearchStep(\n  search_prompt= embedding_search_prompt,\n  embed_fn=embed,\n  k=5,    \n  candidates=taxonomy,\n  name=\"embedding_search\"\n)\n</pre> # set your cohere api key as an env var or set it directly here COHERE_API_KEY = os.environ.get('COHERE_API_KEY') co = cohere.Client(COHERE_API_KEY)  def embed(texts: List[str]):   embeddings = co.embed(     model=\"embed-english-v3.0\",     texts=texts,     input_type='classification'   ).embeddings   return np.array(embeddings).astype('float32')  embedding_search_prompt = lambda row: row[\"short_description\"]  embedding_search_step = steps.EmbeddingSearchStep(   search_prompt= embedding_search_prompt,   embed_fn=embed,   k=5,       candidates=taxonomy,   name=\"embedding_search\" ) <p>Finally we take the top 5 categories selected by the embedding step, feed them into an LLM query and ask it to pick the index of the best one. We use an <code>LLMStructuredStep</code> for this.</p> In\u00a0[6]: Copied! <pre>def categorize_prompt(row):\n    categories = \"\"\n    i = 1\n    while f\"candidate{i}\" in row[\"embedding_search\"]:\n        categories += f'{i}. {row[\"embedding_search\"][f\"candidate{i}\"]}\\n'\n        i += 1\n\n    return f\"\"\"\n    You are given a product description and {i-1} options for the product's category.\n    Pick the index of the most accurate category.\n    The index must be between 1 and {i-1}.\n    Product description: {row['short_description']}\n    Categories:\n    {categories}\n    \"\"\"\n    \nclass CategoryIndex(BaseModel):\n    category_index: int = Field(description=\"The index of the most accurate category\")\n    \ncategorize_step = steps.LLMStructuredStep(\n  prompt=categorize_prompt,\n  model=models.gpt35,\n  out_schema=CategoryIndex,\n  name=\"categorize\"\n)\n</pre>  def categorize_prompt(row):     categories = \"\"     i = 1     while f\"candidate{i}\" in row[\"embedding_search\"]:         categories += f'{i}. {row[\"embedding_search\"][f\"candidate{i}\"]}\\n'         i += 1      return f\"\"\"     You are given a product description and {i-1} options for the product's category.     Pick the index of the most accurate category.     The index must be between 1 and {i-1}.     Product description: {row['short_description']}     Categories:     {categories}     \"\"\"      class CategoryIndex(BaseModel):     category_index: int = Field(description=\"The index of the most accurate category\")      categorize_step = steps.LLMStructuredStep(   prompt=categorize_prompt,   model=models.gpt35,   out_schema=CategoryIndex,   name=\"categorize\" ) <p>The previous step output a category index but we want the actual category, so we need to map the index to the category. We'll create a simple <code>CustomStep</code> that simply grabs the <code>category{i}</code> field that was created in the embedding search step.</p> In\u00a0[7]: Copied! <pre>class Category(BaseModel):\n    category: str = Field(description=\"The most accurate category\")\n\nselect_category_step = steps.CustomStep(\n  transform=lambda row: row[\"embedding_search\"][f'candidate{row[\"category_index\"]}'],\n  out_schema=Category,\n  name=\"category\"\n)\n</pre> class Category(BaseModel):     category: str = Field(description=\"The most accurate category\")  select_category_step = steps.CustomStep(   transform=lambda row: row[\"embedding_search\"][f'candidate{row[\"category_index\"]}'],   out_schema=Category,   name=\"category\" ) <p>We're done defining the steps. Finally, we define an evaluation function - a simple string comparison against the ground truth column which was present in the dataset. Then we define a <code>Pipeline</code> and run it.</p> In\u00a0[8]: Copied! <pre>evaluate = lambda row: row['category'].lower() == row['gpt4_category'].lower()\n\ncategorizer = pipeline.Pipeline([\n  serp_step, \n  short_description_step, \n  embedding_search_step, \n  categorize_step,\n  select_category_step\n], evaluate)\n\ncategorizer.run(df)\n</pre> evaluate = lambda row: row['category'].lower() == row['gpt4_category'].lower()  categorizer = pipeline.Pipeline([   serp_step,    short_description_step,    embedding_search_step,    categorize_step,   select_category_step ], evaluate)  categorizer.run(df) <pre>Running step serp...\n</pre> <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [01:05&lt;00:00,  6.56s/it]\n</pre> <pre>Running step short_description...\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:35&lt;00:00,  3.58s/it]\n</pre> <pre>Running step embedding_search...\nRunning step categorize...\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:22&lt;00:00,  2.28s/it]\n</pre> <pre>Running step select_category...\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:00&lt;00:00, 11259.88it/s]\n</pre> Out[8]: description gpt4_category serp __short_description__ short_description category1 category2 category3 category4 category5 __categorize__ category_index category 0 Duracell Coppertop Alkaline Batteries AA - 48 pk Household Essentials &gt; Household Batteries &gt; A... [{'title': 'Duracell Coppertop Alkaline Batter... {'input_tokens': 646, 'output_tokens': 33, 'su... Duracell Coppertop Alkaline Batteries AA in a ... Household Essentials &gt; Household Batteries &gt; A... Household Essentials &gt; Household Batteries &gt; A... Household Essentials &gt; Household Batteries &gt; R... Household Essentials &gt; Household Batteries &gt; S... Automotive &gt; Auto Parts &gt; Car Batteries &amp; Acce... {'input_tokens': 193, 'output_tokens': 10, 'su... 1 Household Essentials &gt; Household Batteries &gt; A... 1 Kitsch Velvet Scrunchies for Hair, Hair Scrunc... Beauty &gt; Hair Care &gt; Hair Styling [{'title': 'Velvet Scrunchies - Blush/Mauve \u2013 ... {'input_tokens': 609, 'output_tokens': 32, 'su... A pack of 5 Velvet Scrunchies in blush and mau... Beauty &gt; Hair Care &gt; Hair Color Beauty &gt; Hair Care &gt; Hair Styling Beauty &gt; Hair Care &gt; Hair Brushes &amp; Combs Beauty &gt; Hair Care &gt; Beauty &gt; Hair Care &gt; Hair Styling Tools {'input_tokens': 179, 'output_tokens': 10, 'su... 2 Beauty &gt; Hair Care &gt; Hair Styling 2 Kellogg's Cold Breakfast Cereal, Bulk Pantry S... Grocery &gt; Breakfast &gt; Cold Cereal [{'title': 'Kellogg's Cold Breakfast Cereal, B... {'input_tokens': 568, 'output_tokens': 38, 'su... A variety pack of Kellogg's cold breakfast cer... Grocery &gt; Breakfast &gt; Cold Cereal Grocery &gt; Frozen &gt; Frozen Breakfast Grocery &gt; Breakfast &gt; Oatmeal &amp; Hot Cereal Grocery &gt; Breakfast &gt; Grocery &gt; Snacks &gt; {'input_tokens': 179, 'output_tokens': 10, 'su... 1 Grocery &gt; Breakfast &gt; Cold Cereal 3 Bar Keepers Friend Powdered Cleanser 12-Ounces... Household Essentials &gt; Household Cleaners &gt; Me... [{'title': 'Bar Keepers Friend Powdered Cleans... {'input_tokens': 595, 'output_tokens': 51, 'su... Bar Keepers Friend Powdered Cleanser in a 12-O... Household Essentials &gt; Household Cleaners &gt; Me... Beauty &gt; Bath &amp; Body &gt; Bar Soap Household Essentials &gt; Household Cleaners &gt; Al... Industrial &gt; Janitorial &amp; Sanitation Supplies ... Beauty &gt; Skin Care &gt; Skin Cleansers {'input_tokens': 212, 'output_tokens': 10, 'su... 1 Household Essentials &gt; Household Cleaners &gt; Me... 4 Orgain Organic Kids Nutritional Protein Shake,... Baby &gt; Baby Food &amp; Nutrition &gt; Baby Drinks [{'title': 'Orgain Organic Kids Nutritional Pr... {'input_tokens': 676, 'output_tokens': 64, 'su... Orgain Organic Kids Nutritional Protein Shake ... Health &amp; Medicine &gt; Vitamins &amp; Supplements &gt; C... Health &amp; Medicine &gt; Fitness &amp; Weight Loss &gt; Pr... Baby &gt; Baby Food &amp; Nutrition &gt; Health &amp; Medicine &gt; Fitness &amp; Weight Loss &gt; Pr... Baby &gt; Baby Food &amp; Nutrition &gt; Baby Food {'input_tokens': 226, 'output_tokens': 10, 'su... 1 Health &amp; Medicine &gt; Vitamins &amp; Supplements &gt; C... 5 Crosley Furniture Everett Mid-Century Modern M... Home &gt; Furniture &gt; TV Stands &amp; Entertainment C... [{'title': 'Crosley Furniture Everett Mid-Cent... {'input_tokens': 531, 'output_tokens': 48, 'su... The Crosley Furniture Everett Mid-Century Mode... Home &gt; Furniture &gt; Accent, Coffee, &amp; Console T... Home &gt; Furniture &gt; Home &gt; Furniture &gt; TV Stands &amp; Entertainment C... Electronics &gt; Home Audio &gt; Stereo Shelf Systems Home &gt; Furniture &gt; Entertaining Furniture {'input_tokens': 197, 'output_tokens': 10, 'su... 3 Home &gt; Furniture &gt; TV Stands &amp; Entertainment C... 6 Dorco Tinkle Eyebrow Razors for Women, Dermapl... Personal Care &gt; Shaving &amp; Grooming &gt; Women's S... [{'title': 'Dorco Tinkle Eyebrow Razors for Wo... {'input_tokens': 711, 'output_tokens': 54, 'su... Dorco Tinkle Eyebrow Razors for Women is a der... Personal Care &gt; Shaving &amp; Grooming &gt; Women's S... Personal Care &gt; Shaving &amp; Grooming &gt; Beauty &gt; Skin Care &gt; Skin Care Tools Beauty &gt; Cosmetics &gt; Makeup Tools &amp; Brushes Beauty &gt; Hair Care &gt; {'input_tokens': 208, 'output_tokens': 10, 'su... 3 Beauty &gt; Skin Care &gt; Skin Care Tools 7 BareOrganics 13313 Cardio Care USDA Organic Co... Grocery &gt; Beverages &gt; Coffee [{'title': 'BareOrganics 13313 Cardio Care USD... {'input_tokens': 649, 'output_tokens': 51, 'su... BareOrganics 13313 Cardio Care USDA Organic Co... Grocery &gt; Beverages &gt; Coffee Health &amp; Medicine &gt; Fitness &amp; Weight Loss &gt; Su... QSR &gt; Beverages &gt; Coffee Grocery &gt; Beverages &gt; Drink Mixes &amp; Flavor Enh... Home &gt; Kitchen &gt; Coffee &amp; Tea Makers {'input_tokens': 206, 'output_tokens': 10, 'su... 2 Health &amp; Medicine &gt; Fitness &amp; Weight Loss &gt; Su... 8 Amazon.com Gift Card in a Holiday Twig Box Party, Gifts &amp; Cards &gt; Gift Cards &gt; [{'title': 'Customer reviews: Amazon.com Gift ... {'input_tokens': 548, 'output_tokens': 47, 'su... Amazon.com Gift Card in a Holiday Twig Box is ... Party, Gifts &amp; Cards &gt; Gift Cards &gt; Party, Gifts &amp; Cards &gt; Gift Wrap, Bags &amp; Acces... Party, Gifts &amp; Cards &gt; Gift Cards &gt; Airline Gi... Party, Gifts &amp; Cards &gt; Gift Wrap, Bags &amp; Acces... Party, Gifts &amp; Cards &gt; Gift Wrap, Bags &amp; Acces... {'input_tokens': 220, 'output_tokens': 10, 'su... 1 Party, Gifts &amp; Cards &gt; Gift Cards &gt; 9 Amazon Brand - Happy Belly 1% Milk, Half Gallo... Grocery &gt; Dairy &gt; Milk [{'title': 'Happy Belly: Milk - Amazon.com', '... {'input_tokens': 610, 'output_tokens': 38, 'su... Amazon Brand - Happy Belly 1% Milk is a 64-oun... Grocery &gt; Dairy &gt; Milk Baby &gt; Baby Food &amp; Nutrition &gt; Baby Formula Grocery &gt; Dairy &gt; Grocery &gt; Beverages &gt; Nutritional Beverages Grocery &gt; Dairy &gt; Milk Substitutes {'input_tokens': 180, 'output_tokens': 10, 'su... 1 Grocery &gt; Dairy &gt; Milk In\u00a0[9]: Copied! <pre>print(f\"Accuracy: {categorizer.score}\")\nprint(f\"Statistics: {categorizer.statistics}\")\n</pre> print(f\"Accuracy: {categorizer.score}\") print(f\"Statistics: {categorizer.statistics}\") <pre>Accuracy: 0.7\nStatistics: {\n    \"input_tokens\": {\n        \"gpt-3.5-turbo-0125\": 8143\n    },\n    \"output_tokens\": {\n        \"gpt-3.5-turbo-0125\": 556\n    },\n    \"num_success\": 10,\n    \"num_failure\": 0,\n    \"total_latency\": 58.50821637362242\n}\n</pre> In\u00a0[10]: Copied! <pre>categorizer.update_params({\n  \"categorize\": {\n    \"model\": models.gpt4\n  }\n})\ncategorizer.run(df)\nprint(f\"Accuracy: {categorizer.score}\")\nprint(f\"Statistics: {categorizer.statistics}\")\n</pre> categorizer.update_params({   \"categorize\": {     \"model\": models.gpt4   } }) categorizer.run(df) print(f\"Accuracy: {categorizer.score}\") print(f\"Statistics: {categorizer.statistics}\") <pre>Running step serp...\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:48&lt;00:00,  4.87s/it]\n</pre> <pre>Running step short_description...\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:25&lt;00:00,  2.52s/it]\n</pre> <pre>Running step embedding_search...\nRunning step categorize...\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:31&lt;00:00,  3.18s/it]\n</pre> <pre>Running step select_category...\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:00&lt;00:00, 12394.52it/s]</pre> <pre>Accuracy: 0.9\nStatistics: {\n    \"input_tokens\": {\n        \"gpt-3.5-turbo-0125\": 6073,\n        \"gpt-4-turbo-preview\": 2011\n    },\n    \"output_tokens\": {\n        \"gpt-3.5-turbo-0125\": 460,\n        \"gpt-4-turbo-preview\": 100\n    },\n    \"num_success\": 10,\n    \"num_failure\": 0,\n    \"total_latency\": 56.871944959741086\n}\n</pre> <pre>\n</pre> In\u00a0[11]: Copied! <pre>params_grid = {\n  \"global\": {\n    \"model\": [models.gpt35, models.gpt4]\n  },\n  serp_step.name: {\n    \"top_n\": [3, 5]\n  },\n  short_description_step.name: {\n    \"model\": [models.gpt35]\n  },\n  embedding_search_step.name: {\n    \"k\": [5, 10],\n  }\n}\n</pre> params_grid = {   \"global\": {     \"model\": [models.gpt35, models.gpt4]   },   serp_step.name: {     \"top_n\": [3, 5]   },   short_description_step.name: {     \"model\": [models.gpt35]   },   embedding_search_step.name: {     \"k\": [5, 10],   } } In\u00a0[12]: Copied! <pre>from superpipe import grid_search\n\nsearch = grid_search.GridSearch(categorizer, params_grid)\nsearch.run(df)\n</pre> from superpipe import grid_search  search = grid_search.GridSearch(categorizer, params_grid) search.run(df) <pre>Iteration 1 of 8\nParams:  {'global': {'model': 'gpt-3.5-turbo-0125'}, 'serp': {'top_n': 3}, 'short_description': {'model': 'gpt-3.5-turbo-0125'}, 'embedding_search': {'k': 5}}\nResult:  {'global__model': 'gpt-3.5-turbo-0125', 'serp__top_n': 3, 'short_description__model': 'gpt-3.5-turbo-0125', 'embedding_search__k': 5, 'score': 0.6, 'input_tokens': defaultdict(&lt;class 'int'&gt;, {'gpt-3.5-turbo-0125': 8118}), 'output_tokens': defaultdict(&lt;class 'int'&gt;, {'gpt-3.5-turbo-0125': 541}), 'num_success': 10, 'num_failure': 0, 'total_latency': 49.89761724695563, 'index': -140950531981795240}\nIteration 2 of 8\nParams:  {'global': {'model': 'gpt-3.5-turbo-0125'}, 'serp': {'top_n': 3}, 'short_description': {'model': 'gpt-3.5-turbo-0125'}, 'embedding_search': {'k': 10}}\nResult:  {'global__model': 'gpt-3.5-turbo-0125', 'serp__top_n': 3, 'short_description__model': 'gpt-3.5-turbo-0125', 'embedding_search__k': 10, 'score': 0.6, 'input_tokens': defaultdict(&lt;class 'int'&gt;, {'gpt-3.5-turbo-0125': 8751}), 'output_tokens': defaultdict(&lt;class 'int'&gt;, {'gpt-3.5-turbo-0125': 572}), 'num_success': 10, 'num_failure': 0, 'total_latency': 37.86992995790206, 'index': -5214020317066003945}\nIteration 3 of 8\nParams:  {'global': {'model': 'gpt-3.5-turbo-0125'}, 'serp': {'top_n': 5}, 'short_description': {'model': 'gpt-3.5-turbo-0125'}, 'embedding_search': {'k': 5}}\nResult:  {'global__model': 'gpt-3.5-turbo-0125', 'serp__top_n': 5, 'short_description__model': 'gpt-3.5-turbo-0125', 'embedding_search__k': 5, 'score': 0.8, 'input_tokens': defaultdict(&lt;class 'int'&gt;, {'gpt-3.5-turbo-0125': 8179}), 'output_tokens': defaultdict(&lt;class 'int'&gt;, {'gpt-3.5-turbo-0125': 589}), 'num_success': 10, 'num_failure': 0, 'total_latency': 88.11189049854875, 'index': 408154170496236326}\nIteration 4 of 8\nParams:  {'global': {'model': 'gpt-3.5-turbo-0125'}, 'serp': {'top_n': 5}, 'short_description': {'model': 'gpt-3.5-turbo-0125'}, 'embedding_search': {'k': 10}}\nResult:  {'global__model': 'gpt-3.5-turbo-0125', 'serp__top_n': 5, 'short_description__model': 'gpt-3.5-turbo-0125', 'embedding_search__k': 10, 'score': 0.5, 'input_tokens': defaultdict(&lt;class 'int'&gt;, {'gpt-3.5-turbo-0125': 8730}), 'output_tokens': defaultdict(&lt;class 'int'&gt;, {'gpt-3.5-turbo-0125': 563}), 'num_success': 10, 'num_failure': 0, 'total_latency': 172.64198220823891, 'index': 5329708189983242092}\nIteration 5 of 8\nParams:  {'global': {'model': 'gpt-4-turbo-preview'}, 'serp': {'top_n': 3}, 'short_description': {'model': 'gpt-3.5-turbo-0125'}, 'embedding_search': {'k': 5}}\nResult:  {'global__model': 'gpt-4-turbo-preview', 'serp__top_n': 3, 'short_description__model': 'gpt-3.5-turbo-0125', 'embedding_search__k': 5, 'score': 0.9, 'input_tokens': defaultdict(&lt;class 'int'&gt;, {'gpt-3.5-turbo-0125': 6097, 'gpt-4-turbo-preview': 2011}), 'output_tokens': defaultdict(&lt;class 'int'&gt;, {'gpt-3.5-turbo-0125': 458, 'gpt-4-turbo-preview': 100}), 'num_success': 10, 'num_failure': 0, 'total_latency': 45.84698062716052, 'index': 7300372654060448865}\nIteration 6 of 8\nParams:  {'global': {'model': 'gpt-4-turbo-preview'}, 'serp': {'top_n': 3}, 'short_description': {'model': 'gpt-3.5-turbo-0125'}, 'embedding_search': {'k': 10}}\nResult:  {'global__model': 'gpt-4-turbo-preview', 'serp__top_n': 3, 'short_description__model': 'gpt-3.5-turbo-0125', 'embedding_search__k': 10, 'score': 0.8, 'input_tokens': defaultdict(&lt;class 'int'&gt;, {'gpt-3.5-turbo-0125': 6099, 'gpt-4-turbo-preview': 2670}), 'output_tokens': defaultdict(&lt;class 'int'&gt;, {'gpt-3.5-turbo-0125': 473, 'gpt-4-turbo-preview': 100}), 'num_success': 10, 'num_failure': 0, 'total_latency': 50.5230847497005, 'index': 2027969551187248201}\nIteration 7 of 8\nParams:  {'global': {'model': 'gpt-4-turbo-preview'}, 'serp': {'top_n': 5}, 'short_description': {'model': 'gpt-3.5-turbo-0125'}, 'embedding_search': {'k': 5}}\nResult:  {'global__model': 'gpt-4-turbo-preview', 'serp__top_n': 5, 'short_description__model': 'gpt-3.5-turbo-0125', 'embedding_search__k': 5, 'score': 0.6, 'input_tokens': defaultdict(&lt;class 'int'&gt;, {'gpt-3.5-turbo-0125': 6201, 'gpt-4-turbo-preview': 2020}), 'output_tokens': defaultdict(&lt;class 'int'&gt;, {'gpt-3.5-turbo-0125': 455, 'gpt-4-turbo-preview': 100}), 'num_success': 10, 'num_failure': 0, 'total_latency': 48.812740583438426, 'index': 3547485497719024256}\nIteration 8 of 8\nParams:  {'global': {'model': 'gpt-4-turbo-preview'}, 'serp': {'top_n': 5}, 'short_description': {'model': 'gpt-3.5-turbo-0125'}, 'embedding_search': {'k': 10}}\nResult:  {'global__model': 'gpt-4-turbo-preview', 'serp__top_n': 5, 'short_description__model': 'gpt-3.5-turbo-0125', 'embedding_search__k': 10, 'score': 0.8, 'input_tokens': defaultdict(&lt;class 'int'&gt;, {'gpt-3.5-turbo-0125': 6108, 'gpt-4-turbo-preview': 2679}), 'output_tokens': defaultdict(&lt;class 'int'&gt;, {'gpt-3.5-turbo-0125': 474, 'gpt-4-turbo-preview': 100}), 'num_success': 10, 'num_failure': 0, 'total_latency': 54.17387087456882, 'index': -5348831425406814354}\n</pre> Out[12]: global__model serp__top_n short_description__model embedding_search__k score input_tokens output_tokens num_success num_failure total_latency index 0 gpt-3.5-turbo-0125 3 gpt-3.5-turbo-0125 5 0.6 {'gpt-3.5-turbo-0125': 8118} {'gpt-3.5-turbo-0125': 541} 10 0 49.897617 -140950531981795240 1 gpt-3.5-turbo-0125 3 gpt-3.5-turbo-0125 10 0.6 {'gpt-3.5-turbo-0125': 8751} {'gpt-3.5-turbo-0125': 572} 10 0 37.869930 -5214020317066003945 2 gpt-3.5-turbo-0125 5 gpt-3.5-turbo-0125 5 0.8 {'gpt-3.5-turbo-0125': 8179} {'gpt-3.5-turbo-0125': 589} 10 0 88.111890 408154170496236326 3 gpt-3.5-turbo-0125 5 gpt-3.5-turbo-0125 10 0.5 {'gpt-3.5-turbo-0125': 8730} {'gpt-3.5-turbo-0125': 563} 10 0 172.641982 5329708189983242092 4 gpt-4-turbo-preview 3 gpt-3.5-turbo-0125 5 0.9 {'gpt-3.5-turbo-0125': 6097, 'gpt-4-turbo-prev... {'gpt-3.5-turbo-0125': 458, 'gpt-4-turbo-previ... 10 0 45.846981 7300372654060448865 5 gpt-4-turbo-preview 3 gpt-3.5-turbo-0125 10 0.8 {'gpt-3.5-turbo-0125': 6099, 'gpt-4-turbo-prev... {'gpt-3.5-turbo-0125': 473, 'gpt-4-turbo-previ... 10 0 50.523085 2027969551187248201 6 gpt-4-turbo-preview 5 gpt-3.5-turbo-0125 5 0.6 {'gpt-3.5-turbo-0125': 6201, 'gpt-4-turbo-prev... {'gpt-3.5-turbo-0125': 455, 'gpt-4-turbo-previ... 10 0 48.812741 3547485497719024256 7 gpt-4-turbo-preview 5 gpt-3.5-turbo-0125 10 0.8 {'gpt-3.5-turbo-0125': 6108, 'gpt-4-turbo-prev... {'gpt-3.5-turbo-0125': 474, 'gpt-4-turbo-previ... 10 0 54.173871 -5348831425406814354 In\u00a0[\u00a0]: Copied! <pre># Print the best score and params\nprint(f\"Best score: {search.best_score}\")\nprint(f\"Best params: {search.best_params}\")\n\n# Pick the best params and re-run the pipeline\ncategorizer.update_params(search.best_params)\ncategorizer.run(df)\n</pre> # Print the best score and params print(f\"Best score: {search.best_score}\") print(f\"Best params: {search.best_params}\")  # Pick the best params and re-run the pipeline categorizer.update_params(search.best_params) categorizer.run(df)"},{"location":"examples/product_categorization/product_categorization/#product-categorization","title":"Product Categorization\u00b6","text":"<p>View on Github</p> <p>This notebook shows how to use SupePipe to categorize a list of products into a product taxonomy.</p> <p>We are given a list of product names from an e-commerce marketplace, like</p> <p><code>Kitsch Velvet Scrunchies for Hair, Hair Scrunchies for Women, Scrunchy Hair Bands, 5 Pack (Blush/Mauve)</code></p> <p>And some product categories in the form of a taxonomy, for example the category for the above product might be</p> <p><code>Beauty &gt; Hair Care &gt; Hair Styling</code></p> <p>The goal is to accurately map each product into the best category, given a taxonomy containing 1000+ categories.</p>"},{"location":"examples/product_categorization/product_categorization/#approach","title":"Approach\u00b6","text":"<p>We'll implement the following multi-step approach:</p> <ol> <li><p>Do a google search on the product name</p> </li> <li><p>Feed the name and the search results from Step 1 into an LLM to get a short product description</p> </li> <li><p>Create embeddings of the product categories and store them in a vector store. Then do a nearest neighbor search with the product description created in Step 3.</p> </li> <li><p>Feed the top N nearest neighbor categories along with the product description into an LLM and ask it to pick the best one</p> </li> </ol>"},{"location":"examples/product_categorization/product_categorization/#defining-the-pipeline-using-superpipe","title":"Defining the Pipeline using Superpipe\u00b6","text":""},{"location":"examples/product_categorization/product_categorization/#evaluating-accuracy-token-usage-and-latency","title":"Evaluating accuracy, token usage and latency\u00b6","text":"<p>Superpipe makes it easy to:</p> <ul> <li>Evaluate the accuracy of your pipeline if your dataset has a ground truth column. If you passed in an <code>evaluate</code> function you can call <code>Pipeline.score</code> to get the accuracy score</li> <li>Track the token usage and latency for each row or in aggregate over the entire dataset. To get the aggregate statistics, call <code>Pipeline.statistics</code></li> </ul>"},{"location":"examples/product_categorization/product_categorization/#tuning-the-pipeline","title":"Tuning the pipeline\u00b6","text":""},{"location":"examples/product_categorization/product_categorization/#running-a-grid-search-over-parameters","title":"Running a grid search over parameters\u00b6","text":"<p>First we define the \"grid\", ie. a list of values for each parameter that we want to try out. The grid must be defined as a dictionary of dictionaries where the top-level key is the name of the step and the second-level key is the name of the parameter</p> <p>The <code>\"global\"</code> top-level key is special: parameters defined under <code>\"global\"</code> will be applied to all steps. If a step doesn't use that param, it will simply be ignored. If the same parameter is defined under global as well as under the step name, the value under the step name will take precedence.</p> <p>In the below example, <code>params_grid['global']['model']</code> will be used by the <code>categorize</code> step since no overriding model is specified, but ignored by the <code>short_description</code> step because the <code>model</code> key is overriden.</p> <p>The grid defined below will run 2 * 2 * 2 = 8 configurations.</p>"},{"location":"examples/web_scraping/web_scraping/","title":"Scraping Wikipedia","text":"In\u00a0[1]: Copied! <pre>from superpipe.steps import LLMStructuredStep, CustomStep, SERPEnrichmentStep\nfrom superpipe import models\nfrom pydantic import BaseModel, Field\n\n# Step 1: use Superpipe's built-in SERP enrichment step to search for the persons wikipedia page\n# Include a unique \"name\" for the step that will used to reference this step's output in future steps\n\nsearch_step = SERPEnrichmentStep(\n  prompt= lambda row: f\"{row['name']} wikipedia\",\n  name=\"search\"\n)\n\n# Step 2: Use an LLM to extract the wikipedia URL from the search results\n# First, define a Pydantic model that specifies the structured output we want from the LLM\n\nclass ParseSearchResult(BaseModel):\n  wikipedia_url: str = Field(description=\"The URL of the Wikipedia page for the person\")\n\n# Then we use the built-in LLMStructuredStep and specify a model and a prompt\n# The prompt is a function that has access to all the fields in the input as well as the outputs of previous steps\n\nparse_search_step = LLMStructuredStep(\n  model=models.gpt35,\n  prompt= lambda row: f\"Extract the Wikipedia URL for {row['name']} from the following search results: \\n\\n {row['search']}\",\n  out_schema=ParseSearchResult,\n  name=\"parse_search\"\n)\n</pre> from superpipe.steps import LLMStructuredStep, CustomStep, SERPEnrichmentStep from superpipe import models from pydantic import BaseModel, Field  # Step 1: use Superpipe's built-in SERP enrichment step to search for the persons wikipedia page # Include a unique \"name\" for the step that will used to reference this step's output in future steps  search_step = SERPEnrichmentStep(   prompt= lambda row: f\"{row['name']} wikipedia\",   name=\"search\" )  # Step 2: Use an LLM to extract the wikipedia URL from the search results # First, define a Pydantic model that specifies the structured output we want from the LLM  class ParseSearchResult(BaseModel):   wikipedia_url: str = Field(description=\"The URL of the Wikipedia page for the person\")  # Then we use the built-in LLMStructuredStep and specify a model and a prompt # The prompt is a function that has access to all the fields in the input as well as the outputs of previous steps  parse_search_step = LLMStructuredStep(   model=models.gpt35,   prompt= lambda row: f\"Extract the Wikipedia URL for {row['name']} from the following search results: \\n\\n {row['search']}\",   out_schema=ParseSearchResult,   name=\"parse_search\" ) In\u00a0[\u00a0]: Copied! <pre>from superpipe.pipeline import Pipeline\nimport requests\nimport html2text\nimport json\n\nh = html2text.HTML2Text()\nh.ignore_links = True\n\n# Step 3: we create a CustomStep that can execute any arbitrary function (transform)\n# The function fetches the contents of the wikipedia url and converts them to markdown\n\nfetch_wikipedia_step = CustomStep(\n  transform=lambda row: h.handle(requests.get(row['wikipedia_url']).text),\n  name=\"wikipedia\"\n)\n\n# Step 4: we extract the date of birth, living/dead status and cause of death from the wikipedia contents\n\nclass ExtractedData(BaseModel):\n    date_of_birth: str = Field(description=\"The date of birth of the person in the format YYYY-MM-DD\")\n    alive: bool = Field(description=\"Whether the person is still alive\")\n    cause_of_death: str = Field(description=\"The cause of death of the person. If the person is alive, return 'N/A'\")\n\nextract_step = LLMStructuredStep(\n  model=models.gpt4,\n  prompt= lambda row: f\"\"\"Extract the date of birth for {row['name']}, whether they're still alive \\\n  and if not, their cause of death from the following Wikipedia content: \\n\\n {row['wikipedia']}\"\"\",\n  out_schema=ExtractedData,\n  name=\"extract_data\"\n)\n\n# Finally we define and run the pipeline\n\npipeline = Pipeline([\n  search_step,\n  parse_search_step,\n  fetch_wikipedia_step,\n  extract_step\n])\n\noutput = pipeline.run({\"name\": \"Jean-Paul Sartre\"})\nprint(json.dumps(output, indent=2))\n</pre> from superpipe.pipeline import Pipeline import requests import html2text import json  h = html2text.HTML2Text() h.ignore_links = True  # Step 3: we create a CustomStep that can execute any arbitrary function (transform) # The function fetches the contents of the wikipedia url and converts them to markdown  fetch_wikipedia_step = CustomStep(   transform=lambda row: h.handle(requests.get(row['wikipedia_url']).text),   name=\"wikipedia\" )  # Step 4: we extract the date of birth, living/dead status and cause of death from the wikipedia contents  class ExtractedData(BaseModel):     date_of_birth: str = Field(description=\"The date of birth of the person in the format YYYY-MM-DD\")     alive: bool = Field(description=\"Whether the person is still alive\")     cause_of_death: str = Field(description=\"The cause of death of the person. If the person is alive, return 'N/A'\")  extract_step = LLMStructuredStep(   model=models.gpt4,   prompt= lambda row: f\"\"\"Extract the date of birth for {row['name']}, whether they're still alive \\   and if not, their cause of death from the following Wikipedia content: \\n\\n {row['wikipedia']}\"\"\",   out_schema=ExtractedData,   name=\"extract_data\" )  # Finally we define and run the pipeline  pipeline = Pipeline([   search_step,   parse_search_step,   fetch_wikipedia_step,   extract_step ])  output = pipeline.run({\"name\": \"Jean-Paul Sartre\"}) print(json.dumps(output, indent=2)) In\u00a0[8]: Copied! <pre>import pandas as pd\n\ndata = [\n  (\"Ruth Bader Ginsburg\", \"1933-03-15\", False, \"Pancreatic cancer\"),\n  (\"Bill Gates\", \"1955-10-28\", True, \"N/A\"),\n  (\"Steph Curry\", \"1988-03-14\", True, \"N/A\"),\n  (\"Scott Belsky\", \"1980-04-18\", True, \"N/A\"),\n  (\"Steve Jobs\", \"1955-02-24\", False, \"Pancreatic tumor/cancer\"),\n  (\"Paris Hilton\", \"1981-02-17\", True, \"N/A\"),\n  (\"Kurt Vonnegut\", \"1922-11-11\", False, \"Brain injuries\"),\n  (\"Snoop Dogg\", \"1971-10-20\", True, \"N/A\"),\n  (\"Kobe Bryant\", \"1978-08-23\", False, \"Helicopter crash\"),\n  (\"Aaron Swartz\", \"1986-11-08\", False, \"Suicide\")\n]\ndf = pd.DataFrame([{\"name\": d[0], \"dob_label\": d[1], \"alive_label\": d[2], \"cause_label\": d[3]} for d in data])\n\nclass EvalResult(BaseModel):\n  result: bool = Field(description=\"Is the answer correct or not?\")\n\ncause_evaluator = LLMStructuredStep(\n  model=models.gpt4,\n  prompt=lambda row: f\"This is the correct cause of death: {row['cause_label']}. Is this provided cause of death accurate? The phrasing might be slightly different. Use your judgement: \\n{row['cause_of_death']}\",\n  out_schema=EvalResult,\n  name=\"cause_evaluator\")\n\ndef eval_fn(row):\n  score = 0\n  if row['date_of_birth'] == row['dob_label']:\n    score += 0.25\n  if row['alive'] == row['alive_label']:\n    score += 0.25\n  if row['cause_label'] == \"N/A\":\n    if row['cause_of_death'] == \"N/A\":\n      score += 0.5\n  elif cause_evaluator.run(row)['result']:\n    score += 0.5  \n  return score\n\npipeline.run(df)\nprint(\"Score: \", pipeline.evaluate(eval_fn))\ndf\n</pre> import pandas as pd  data = [   (\"Ruth Bader Ginsburg\", \"1933-03-15\", False, \"Pancreatic cancer\"),   (\"Bill Gates\", \"1955-10-28\", True, \"N/A\"),   (\"Steph Curry\", \"1988-03-14\", True, \"N/A\"),   (\"Scott Belsky\", \"1980-04-18\", True, \"N/A\"),   (\"Steve Jobs\", \"1955-02-24\", False, \"Pancreatic tumor/cancer\"),   (\"Paris Hilton\", \"1981-02-17\", True, \"N/A\"),   (\"Kurt Vonnegut\", \"1922-11-11\", False, \"Brain injuries\"),   (\"Snoop Dogg\", \"1971-10-20\", True, \"N/A\"),   (\"Kobe Bryant\", \"1978-08-23\", False, \"Helicopter crash\"),   (\"Aaron Swartz\", \"1986-11-08\", False, \"Suicide\") ] df = pd.DataFrame([{\"name\": d[0], \"dob_label\": d[1], \"alive_label\": d[2], \"cause_label\": d[3]} for d in data])  class EvalResult(BaseModel):   result: bool = Field(description=\"Is the answer correct or not?\")  cause_evaluator = LLMStructuredStep(   model=models.gpt4,   prompt=lambda row: f\"This is the correct cause of death: {row['cause_label']}. Is this provided cause of death accurate? The phrasing might be slightly different. Use your judgement: \\n{row['cause_of_death']}\",   out_schema=EvalResult,   name=\"cause_evaluator\")  def eval_fn(row):   score = 0   if row['date_of_birth'] == row['dob_label']:     score += 0.25   if row['alive'] == row['alive_label']:     score += 0.25   if row['cause_label'] == \"N/A\":     if row['cause_of_death'] == \"N/A\":       score += 0.5   elif cause_evaluator.run(row)['result']:     score += 0.5     return score  pipeline.run(df) print(\"Score: \", pipeline.evaluate(eval_fn)) df <pre>Applying step search: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:08&lt;00:00,  1.16it/s]\nApplying step parse_search: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:10&lt;00:00,  1.02s/it]\nApplying step wikipedia: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:04&lt;00:00,  2.27it/s]\nApplying step extract_data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [01:26&lt;00:00,  8.66s/it]\n</pre> <pre>Score:  1.0\n</pre> Out[8]: name dob_label alive_label cause_label search __parse_search__ wikipedia_url wikipedia __extract_data__ date_of_birth alive cause_of_death __eval_fn__ 0 Ruth Bader Ginsburg 1933-03-15 False Pancreatic cancer {\"searchParameters\":{\"q\":\"Ruth Bader Ginsburg ... {'input_tokens': 1922, 'output_tokens': 23, 'i... https://en.wikipedia.org/wiki/Ruth_Bader_Ginsburg Jump to content\\n\\nMain menu\\n\\nMain menu\\n\\nm... {'input_tokens': 46522, 'output_tokens': 37, '... 1933-03-15 False complications of metastatic pancreatic cancer 1.0 1 Bill Gates 1955-10-28 True N/A {\"searchParameters\":{\"q\":\"Bill Gates wikipedia... {'input_tokens': 1809, 'output_tokens': 20, 'i... https://en.wikipedia.org/wiki/Bill_Gates Jump to content\\n\\nMain menu\\n\\nMain menu\\n\\nm... {'input_tokens': 46613, 'output_tokens': 32, '... 1955-10-28 True N/A 1.0 2 Steph Curry 1988-03-14 True N/A {\"searchParameters\":{\"q\":\"Steph Curry wikipedi... {'input_tokens': 1339, 'output_tokens': 20, 'i... https://en.wikipedia.org/wiki/Stephen_Curry Jump to content\\n\\nMain menu\\n\\nMain menu\\n\\nm... {'input_tokens': 64861, 'output_tokens': 32, '... 1988-03-14 True N/A 1.0 3 Scott Belsky 1980-04-18 True N/A {\"searchParameters\":{\"q\":\"Scott Belsky wikiped... {'input_tokens': 1566, 'output_tokens': 21, 'i... https://en.wikipedia.org/wiki/Scott_Belsky Jump to content\\n\\nMain menu\\n\\nMain menu\\n\\nm... {'input_tokens': 2227, 'output_tokens': 32, 'i... 1980-04-18 True N/A 1.0 4 Steve Jobs 1955-02-24 False Pancreatic tumor/cancer {\"searchParameters\":{\"q\":\"Steve Jobs wikipedia... {'input_tokens': 1625, 'output_tokens': 20, 'i... https://en.wikipedia.org/wiki/Steve_Jobs Jump to content\\n\\nMain menu\\n\\nMain menu\\n\\nm... {'input_tokens': 47086, 'output_tokens': 42, '... 1955-02-24 False respiratory arrest related to a pancreatic neu... 1.0 5 Paris Hilton 1981-02-17 True N/A {\"searchParameters\":{\"q\":\"Paris Hilton wikiped... {'input_tokens': 1322, 'output_tokens': 20, 'i... https://en.wikipedia.org/wiki/Paris_Hilton Jump to content\\n\\nMain menu\\n\\nMain menu\\n\\nm... {'input_tokens': 49288, 'output_tokens': 32, '... 1981-02-17 True N/A 1.0 6 Kurt Vonnegut 1922-11-11 False Brain injuries {\"searchParameters\":{\"q\":\"Kurt Vonnegut wikipe... {'input_tokens': 1369, 'output_tokens': 22, 'i... https://en.wikipedia.org/wiki/Kurt_Vonnegut Jump to content\\n\\nMain menu\\n\\nMain menu\\n\\nm... {'input_tokens': 29700, 'output_tokens': 45, '... 1922-11-11 False brain injuries incurred several weeks prior, f... 1.0 7 Snoop Dogg 1971-10-20 True N/A {\"searchParameters\":{\"q\":\"Snoop Dogg wikipedia... {'input_tokens': 1702, 'output_tokens': 20, 'i... https://en.wikipedia.org/wiki/Snoop_Dogg Jump to content\\n\\nMain menu\\n\\nMain menu\\n\\nm... {'input_tokens': 40901, 'output_tokens': 32, '... 1971-10-20 True N/A 1.0 8 Kobe Bryant 1978-08-23 False Helicopter crash {\"searchParameters\":{\"q\":\"Kobe Bryant wikipedi... {'input_tokens': 1355, 'output_tokens': 21, 'i... https://en.wikipedia.org/wiki/Kobe_Bryant Jump to content\\n\\nMain menu\\n\\nMain menu\\n\\nm... {'input_tokens': 74108, 'output_tokens': 33, '... 1978-08-23 False helicopter crash 1.0 9 Aaron Swartz 1986-11-08 False Suicide {\"searchParameters\":{\"q\":\"Aaron Swartz wikiped... {'input_tokens': 1329, 'output_tokens': 21, 'i... https://en.wikipedia.org/wiki/Aaron_Swartz Jump to content\\n\\nMain menu\\n\\nMain menu\\n\\nm... {'input_tokens': 37532, 'output_tokens': 34, '... 1986-11-08 False Suicide by hanging 1.0 In\u00a0[4]: Copied! <pre>for step in pipeline.steps:\n  print(f\"Step {step.name}:\")\n  print(f\"- Latency: {step.statistics.total_latency}\")\n  print(f\"- Cost: {step.statistics.input_cost + step.statistics.output_cost}\")\n</pre> for step in pipeline.steps:   print(f\"Step {step.name}:\")   print(f\"- Latency: {step.statistics.total_latency}\")   print(f\"- Cost: {step.statistics.input_cost + step.statistics.output_cost}\") <pre>Step search:\n- Latency: 12.000389575958252\n- Cost: 0.0\nStep parse_search:\n- Latency: 10.51110366685316\n- Cost: 0.008334\nStep wikipedia:\n- Latency: 4.235257387161255\n- Cost: 0.0\nStep extract_data:\n- Latency: 90.95815300196409\n- Cost: 4.7203800000000005\n</pre> <p>Clearly the final step (<code>extract_data</code>) is the one responsible for the bulk of the cost and latency. This makes sense, because we're feeding in the entire wikipedia article to GPT-4, one of the most expensive models.</p> <p>Let's find out if we can get away with a cheaper/faster model. Most models cannot handle the number of tokens needed to ingest a whole wikipedia article, so we'll turn to the two that can that are also cheaper than GPT4: Claude 3 Sonnet and Claude 3 Haiku.</p> In\u00a0[5]: Copied! <pre>from superpipe.grid_search import GridSearch\nfrom superpipe.models import claude3_haiku, claude3_sonnet\nfrom superpipe.steps import LLMStructuredCompositeStep\n\n# we need to use LLMStructuredCompositeStep which uses GPT3.5 for structured JSON extraction\n# because Claude does not support JSON mode or function calling out of the box\nnew_extract_step = LLMStructuredCompositeStep(\n  model=models.claude3_haiku,\n  prompt=extract_step.prompt,\n  out_schema=ExtractedData,\n  name=\"extract_data_new\"\n)\n\nnew_pipeline = Pipeline([\n  search_step,\n  parse_search_step,\n  fetch_wikipedia_step,\n  new_extract_step\n], evaluation_fn=eval_fn)\n\nparam_grid = {\n  new_extract_step.name:{\n    \"model\": [claude3_haiku, claude3_sonnet]}\n}\ngrid_search = GridSearch(new_pipeline, param_grid)\ngrid_search.run(df)\n</pre> from superpipe.grid_search import GridSearch from superpipe.models import claude3_haiku, claude3_sonnet from superpipe.steps import LLMStructuredCompositeStep  # we need to use LLMStructuredCompositeStep which uses GPT3.5 for structured JSON extraction # because Claude does not support JSON mode or function calling out of the box new_extract_step = LLMStructuredCompositeStep(   model=models.claude3_haiku,   prompt=extract_step.prompt,   out_schema=ExtractedData,   name=\"extract_data_new\" )  new_pipeline = Pipeline([   search_step,   parse_search_step,   fetch_wikipedia_step,   new_extract_step ], evaluation_fn=eval_fn)  param_grid = {   new_extract_step.name:{     \"model\": [claude3_haiku, claude3_sonnet]} } grid_search = GridSearch(new_pipeline, param_grid) grid_search.run(df) <pre>Applying step search: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:08&lt;00:00,  1.20it/s]\nApplying step parse_search: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:10&lt;00:00,  1.06s/it]\nApplying step wikipedia: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:03&lt;00:00,  2.56it/s]\nApplying step extract_data_new: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [01:26&lt;00:00,  8.63s/it]\nApplying step search: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:08&lt;00:00,  1.18it/s]\nApplying step parse_search: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:10&lt;00:00,  1.03s/it]\nApplying step wikipedia: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:03&lt;00:00,  2.57it/s]\nApplying step extract_data_new: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [05:17&lt;00:00, 31.73s/it]\n/Users/amandhesi/llm/superpipe/superpipe/util.py:44: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.\n  styler = styler.applymap(\n</pre> Out[5]: extract_data_new__model score input_cost output_cost total_latency input_tokens output_tokens num_success num_failure index 0 claude-3-haiku-20240307 1.000000 0.129856 0.001945 109.038948 defaultdict(, {'gpt-3.5-turbo-0125': 15056, 'claude-3-haiku-20240307': 487402}) defaultdict(, {'gpt-3.5-turbo-0125': 208, 'claude-3-haiku-20240307': 1218}) 10 0 4643861466949536679 1 claude-3-sonnet-20240229 0.450000 1.465117 0.022944 339.825781 defaultdict(, {'gpt-3.5-turbo-0125': 14733, 'claude-3-sonnet-20240229': 488036}) defaultdict(, {'gpt-3.5-turbo-0125': 208, 'claude-3-sonnet-20240229': 1786}) 10 0 3722756468172814577 <p>Strangely, Claude 3 Haiku is both more accurate (100% v/s 45%) as well as cheaper and faster. This is suprising, but useful information that we wouldn't have found out unless we built and evaluated pipelines on our specific data rather than benchmark data.</p> In\u00a0[6]: Copied! <pre>best_params = grid_search.best_params\nnew_pipeline.update_params(best_params)\nnew_pipeline.run(df)\nprint(\"Score: \", new_pipeline.score)\nfor step in new_pipeline.steps:\n  print(f\"Step {step.name}:\")\n  print(f\"- Latency: {step.statistics.total_latency}\")\n  print(f\"- Cost: {step.statistics.input_cost + step.statistics.output_cost}\")\n</pre> best_params = grid_search.best_params new_pipeline.update_params(best_params) new_pipeline.run(df) print(\"Score: \", new_pipeline.score) for step in new_pipeline.steps:   print(f\"Step {step.name}:\")   print(f\"- Latency: {step.statistics.total_latency}\")   print(f\"- Cost: {step.statistics.input_cost + step.statistics.output_cost}\") <pre>Applying step search: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:08&lt;00:00,  1.14it/s]\nApplying step parse_search: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:11&lt;00:00,  1.15s/it]\nApplying step wikipedia: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:03&lt;00:00,  2.52it/s]\nApplying step extract_data_new: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [01:27&lt;00:00,  8.76s/it]\n</pre> <pre>Score:  1.0\nStep search:\n- Latency: 8.75270938873291\n- Cost: 0.0\nStep parse_search:\n- Latency: 11.506851500831544\n- Cost: 0.007930999999999999\nStep wikipedia:\n- Latency: 3.9602952003479004\n- Cost: 0.0\nStep extract_data_new:\n- Latency: 87.57113150181249\n- Cost: 0.12396325000000001\n</pre>"},{"location":"examples/web_scraping/web_scraping/#scraping-wikipedia","title":"Scraping Wikipedia\u00b6","text":"<p>View on Github</p> <p>We'll use Superpipe to build a pipeline that receives a famous person's name and figures out their birthday, whether they're still alive and if not, their cause of death.</p> <p>This pipeline will work in 4 steps -</p> <ol> <li>Do a google search with the person's name</li> <li>Use an LLM to fetch the URL of their wikipedia page from the search results</li> <li>Fetch the contents of the wikipedia page and convert them to markdown</li> <li>Use an LLM to extract the birthdate and living or dead from the wikipedia contents</li> </ol> <p>We'll build the pipeline, evaluate it on some data, and optimize it to maximize accuracy while reducing cost and latency.</p>"},{"location":"examples/web_scraping/web_scraping/#step-1-building-the-pipeline","title":"Step 1: Building the pipeline\u00b6","text":""},{"location":"examples/web_scraping/web_scraping/#step-2-evaluating-the-pipeline","title":"Step 2: Evaluating the pipeline\u00b6","text":"<p>Now, we'll evaluate the pipeline on a dataset. Think of this as unit tests for your code. You wouldn't ship code to production without testing it, you shouldn't ship LLM pipelines to production without evaluating them.</p> <p>To do this, we need:</p> <ol> <li>A dataset with labels - In this case we need a list of famous people and the true date of birth, living status and cause of death of each person</li> <li>Evaluation function - a function that defines what \"correct\" is. We'll use simple comparison for date of birth and living status, and an LLM call to evaluate the correctness of cause of death.</li> </ol>"},{"location":"examples/web_scraping/web_scraping/#step-3-optimizing-the-pipeline","title":"Step 3: Optimizing the pipeline\u00b6","text":"<p>This pipeline has an accuracy score of 100%, but perhaps there's room for improvement on cost and speed. First let's view the cost and latency of each step to figure out which one is the bottleneck.</p>"},{"location":"studio/","title":"Superpipe Studio","text":"<p>Superpipe Studio is a free and open-source observability and experimentation app for the Superpipe SDK. It can help you:</p> <ul> <li>Log and monitor results of your Superpipe pipelines in dev or production</li> <li>Manage datasets and build golden sets for ground truth labeling and evaluation</li> <li>Track experiments/grid searches and compare them on accuracy, latency and cost</li> </ul> <p>Superpipe Studio is a Next JS app that can be run locally or self-hosted with Vercel.</p> <p>Demo</p>"},{"location":"studio/#running-superpipe-studio","title":"Running Superpipe Studio","text":"<p>To run Superpipe Studio locally follow instructions in Running Studio locally or to self-host with Vercel follow instructions in Deplying Studio with Vercel.</p>"},{"location":"studio/#usage-with-superpipe","title":"Usage with Superpipe","text":"<ol> <li>Install the superpipe-studio python library with <code>pip install superpipe-studio</code>. Also make sure you're on the latest version of superpipe (<code>pip install superpipe-py -U</code>).</li> <li>Set the following environment variables:</li> <li><code>SUPERPIPE_STUDIO_URL</code> = the url where your studio instance is hosted</li> <li><code>SUPERPIPE_API_KEY</code> = your Superpipe API key if running Studio with authentication enabled (see the Authenticating Superpipe Studio requests section of the Studio readme)</li> </ol>"},{"location":"studio/#logging","title":"Logging","text":"<p>To log a pipeline to Studio, simply pass in <code>enable_logging=True</code> when calling <code>pipeline.run</code>.</p> <pre><code>input = {\n  ...\n}\npipeline.run(data=input, enable_logging=True)\n</code></pre> <p>It\u2019s helpful to set the pipeline\u2019s <code>name</code> field when initializing it to identify the pipeline logs in Studio.</p>"},{"location":"studio/#datasets","title":"Datasets","text":"<p>Creating a Studio dataset uploads the data to Studio where you can visualize it in a convenient interface. It also allows you to use the same dataset across experiments.</p> <p>Datasets are created by calling the constructor of the <code>Dataset</code> class and passing in a pandas dataframe.</p> <pre><code>from studio import Dataset\nimport pandas as pd\n\ndf = pd.DataFrame(...)\ndataset = Dataset(data=df, name=\"furniture\")\n</code></pre> <p>Ground truth columns</p> <p>You can optionally specify a list of ground truth columns which will be shown separately in the Studio UI and can be edited. This lets you create datasets with accurate ground truth labels that can be used to evaluate your pipelines.</p> <pre><code>dataset = Dataset(data=df, name=\"furniture\", ground_truths=[\"brand_name\"])\n</code></pre> <p>You can also download a dataset that already exists in Studio by passing in its <code>id</code>.</p> <pre><code>id = dataset.id\ndataset_copy = Dataset(id=id)\n</code></pre> <p>To add data to an existing dataset, call the <code>add_data</code> function on a dataset and pass in a pandas dataframe.</p> <pre><code>df = pd.DataFrame(...)\ndataset.add_data(data=df)\n</code></pre>"},{"location":"studio/#experiments","title":"Experiments","text":"<p>Experiments in Studio help you log the results of running a pipeline or a grid search on a dataset, so you can evaluate their accuracy, cost and speed and compare pipelines objectively.</p> <p>To run a pipeline experiment, define your pipeline as usual, call <code>pipeline.run_experiment</code> and pass in a pandas dataframe, a Studio dataset object or a dataset id string. If you pass in a dataframe, a Studio dataset object will be created and can be reused for future experiments. If you pass in a dataset id, a Studio dataset will be downloaded.</p> <pre><code>import pandas as pd\nfrom studio import Dataset\n\n# running an experiment on a dataframe implicitly creates a new dataset\ndf = pd.DataFrame(...)\npipeline.run_experiment(data=df)\n\n# running an experiment on an existing dataset\ndataset = Dataset(data=df, name=\"furniture\")\npipeline.run_experiment(data=dataset)\n</code></pre> <p>To run a grid search experiment, define your grid search as usual, call <code>grid_search.run_experiment</code>. Everything else is the same as a pipeline experiment, but you will see one experiment created for each set of parameters in the grid search.</p> <pre><code>grid_search.run_experiment(data=df)\n</code></pre> <p>Experiment groups</p> <p>Studio intelligently groups pipeline experiments so you can compare them more easily. Two experiments will be added to the same group if:</p> <ol> <li>they have the same name, and</li> <li>they have the same steps (but the steps can have different params)</li> </ol> <p>Grid searches are automatically grouped into the same experiment group.</p>"},{"location":"studio/#usage-without-superpipe","title":"Usage without Superpipe","text":"<p>Studio can be used even if you're not using Superpipe to build your LLM pipelines.</p> <p>You can interact with Studio directly via REST API - see API docs here.</p>"},{"location":"blog/archive/2024/","title":"2024","text":""}]}